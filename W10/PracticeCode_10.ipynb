{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22c0024",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "    Import Libraries and datasets from PyTorch Geometric\n",
    "    1. Extracting subgraphs from set of nodes\n",
    "    2. Extracting k_hop_subgraph from nodes\n",
    "    3. Dropping a random walk with a probability\n",
    "    4. MixHop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423b58b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce94718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (3.10.5)\n",
      "Requirement already satisfied: fsspec in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (1.24.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from requests->torch_geometric) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntucat/Documents/Graph-Neural-Networks-Fall-2023/.conda/lib/python3.9/site-packages (from requests->torch_geometric) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torchversion = torch.__version__\n",
    "\n",
    "# Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric\n",
    "!pip install torch_geometric\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Numpy for matrices\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67549f-b192-4222-814e-e67171abcae4",
   "metadata": {},
   "source": [
    "# 1. Extracting subgraphs from set of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d08ce0e5-52ce-4220-bf7b-7587568512b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Graph:\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "\n",
      "Subgraph:\n",
      "(tensor([[2, 1],\n",
      "        [1, 2]]), None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='.', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "edge_index  = data.edge_index\n",
    "\n",
    "# Example: Extract subgraph for nodes 0, 1, 2, 3\n",
    "sampled_nodes = torch.tensor([0, 1, 2, 3])\n",
    "subgraph_data = subgraph(sampled_nodes, edge_index, edge_attr=None)\n",
    "\n",
    "print(\"Original Graph:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nSubgraph:\")\n",
    "print(subgraph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127747d-4fd4-44e9-9eaa-2eadc7f29a5a",
   "metadata": {},
   "source": [
    "# 2. Extracting k_hop_subgraph from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83c6b10d-d324-4d93-8daf-9ca6c57e152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "\n",
    "# Function to extract the k-hop subgraph around a given node or set of nodes\n",
    "def k_hop_subgraph(\n",
    "    node_idx: Union[int, List[int], Tensor],  # The target node(s)\n",
    "    num_hops: int,  # The number of hops k\n",
    "    edge_index: Tensor,  # The edge indices\n",
    "    relabel_nodes: bool = False,  # Whether to relabel nodes to a contiguous range\n",
    "    num_nodes: Optional[int] = None,  # The number of nodes in the graph\n",
    "    flow: str = 'source_to_target',  # The flow direction ('source_to_target' or 'target_to_source')\n",
    "    directed: bool = False,  # Whether the graph is directed\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:  # Returns the subgraph, edge indices, inverse mapping, and edge mask\n",
    "\n",
    "    # Determine the number of nodes if not provided\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    # Ensure the flow direction is valid\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    if flow == 'target_to_source':\n",
    "        row, col = edge_index\n",
    "    else:\n",
    "        col, row = edge_index\n",
    "\n",
    "    # Initialize masks for nodes and edges\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    # Convert node_idx to a tensor if it is not already\n",
    "    if isinstance(node_idx, (int, list, tuple)):\n",
    "        node_idx = torch.tensor([node_idx], device=row.device).flatten()\n",
    "    else:\n",
    "        node_idx = node_idx.to(row.device)\n",
    "\n",
    "    # List to store the subsets of nodes at each hop\n",
    "    subsets = [node_idx]\n",
    "\n",
    "    # Perform k-hop expansion\n",
    "    for _ in range(num_hops):\n",
    "        node_mask.fill_(False)\n",
    "        node_mask[subsets[-1]] = True\n",
    "        torch.index_select(node_mask, 0, row, out=edge_mask)\n",
    "        subsets.append(col[edge_mask])\n",
    "\n",
    "    # Concatenate all subsets and get unique nodes\n",
    "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "    inv = inv[:node_idx.numel()]\n",
    "\n",
    "    # Create a mask for the subset of nodes\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "\n",
    "    # If the graph is undirected, update the edge mask\n",
    "    if not directed:\n",
    "        edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    # Filter the edge index to include only the edges in the subgraph\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    # Relabel nodes to a contiguous range if specified\n",
    "    if relabel_nodes:\n",
    "        node_idx = row.new_full((num_nodes, ), -1)\n",
    "        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "        edge_index = node_idx[edge_index]\n",
    "\n",
    "    # Return the subset of nodes, the filtered edge index, the inverse mapping, and the edge mask\n",
    "    return subset, edge_index, inv, edge_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc483a-f31f-440f-ad96-bd2cdc000988",
   "metadata": {},
   "source": [
    "# 3. Get subgraphs from target node 6 with 2 hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19799a3b-9220-4093-8f28-5ef27db3abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='.', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "edge_index  = data.edge_index\n",
    "\n",
    "subset, edge_index, mapping, edge_mask = k_hop_subgraph(6, 2, edge_index, relabel_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "920ea9de-6ae4-4aba-a8a5-38589d70faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9, 17, 23, 27, 23, 23, 17, 19, 23, 26, 36, 27, 28, 37, 41, 27, 23, 23,\n",
      "         27, 29,  0, 16, 17, 17, 23, 17, 23, 17, 40, 23, 30,  9, 17,  0,  3,  9,\n",
      "         10, 12, 14, 16, 18, 19, 20, 21, 25, 26, 28, 33, 34, 35, 36, 37, 38, 40,\n",
      "         42, 43, 17, 43,  3, 17, 23, 17, 17, 27,  0,  1,  2,  3,  6,  7, 11, 13,\n",
      "         15, 19, 24, 27, 29, 30, 31, 32, 33, 34, 23, 17,  3, 17,  0,  4,  5,  8,\n",
      "         22, 23, 37, 39, 40, 41,  4, 17, 37, 38,  8, 23, 15, 23, 23, 23, 17, 23,\n",
      "         17, 23, 35, 36, 17, 34,  3, 17, 34,  4, 17, 27, 28, 17, 28, 42, 27, 14,\n",
      "         17, 27,  4, 27, 17, 38, 17, 18],\n",
      "        [ 0,  0,  0,  0,  1,  2,  3,  3,  3,  3,  3,  4,  4,  4,  4,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9, 10, 11, 12, 13, 14, 14, 15, 15, 16, 16, 17, 17, 17,\n",
      "         17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "         17, 17, 18, 18, 19, 19, 19, 20, 21, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "         23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 25, 26, 26, 27, 27, 27, 27,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 30, 30, 31, 32, 33, 33,\n",
      "         34, 34, 34, 34, 35, 35, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 39, 40,\n",
      "         40, 40, 41, 41, 42, 42, 43, 43]])\n"
     ]
    }
   ],
   "source": [
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9889b8-fdc9-4db3-bce3-f3a791e63bc1",
   "metadata": {},
   "source": [
    "# 4. Drop a random walk path\n",
    "\n",
    "Drops edges from the adjacency matrix based on random walks.\n",
    "\n",
    "The source nodes to start random walks from are sampled from the edge index with probability p, following a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780c38b-7ab6-4cf7-856b-c36cf73ef4e3",
   "metadata": {},
   "source": [
    "## 4.1. Construct GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b89afd27-86b3-4e41-996b-75ef376ecf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN_dropout_path(torch.nn.Module):\n",
    "  \"\"\"Graph Convolutional Network\"\"\"\n",
    "  def __init__(self, dim_in, dim_h, dim_out):\n",
    "    super().__init__()\n",
    "    self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "    self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h = F.dropout(x, p=0.5, training=self.training)\n",
    "    h = self.gcn1(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5, training=self.training)\n",
    "    h = self.gcn2(h, edge_index)\n",
    "    return h, F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b2b6e",
   "metadata": {},
   "source": [
    "## 4.2. Dropout_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ceab199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import cumsum, degree, sort_edge_index, subgraph\n",
    "from torch_geometric import is_compiling\n",
    "import torch_geometric.typing\n",
    "\n",
    "def dropout_path(edge_index: Tensor, p: float = 0.2, walks_per_node: int = 1,\n",
    "                 walk_length: int = 3, num_nodes: Optional[int] = None,\n",
    "                 is_sorted: bool = False, training: bool = True) -> Tuple[Tensor, Tensor]:\n",
    "    # Ensure probability is within range\n",
    "    if not (0.0 <= p <= 1.0):\n",
    "        raise ValueError(f'Sample probability must be between 0 and 1 (got {p})')\n",
    "    \n",
    "    # Return unchanged edge_index if not in training mode or p=0\n",
    "    if not training or p == 0.0:\n",
    "        return edge_index, torch.ones(edge_index.size(1), dtype=torch.bool, device=edge_index.device)\n",
    "    \n",
    "    # Ensure required torch-cluster support is available\n",
    "    if not torch_geometric.typing.WITH_TORCH_CLUSTER or is_compiling():\n",
    "        raise ImportError('`dropout_path` requires `torch-cluster`.')\n",
    "    \n",
    "    # Sort edges if necessary\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "    edge_orders = None\n",
    "    if not is_sorted:\n",
    "        edge_orders = torch.arange(edge_index.size(1), device=edge_index.device)\n",
    "        edge_index, edge_orders = sort_edge_index(edge_index, edge_orders, num_nodes=num_nodes)\n",
    "    \n",
    "    # Randomly mask edges\n",
    "    row, col = edge_index\n",
    "    sample_mask = torch.rand(row.size(0), device=edge_index.device) <= p\n",
    "    start = row[sample_mask].repeat(walks_per_node)\n",
    "    \n",
    "    # Perform random walk to determine paths\n",
    "    rowptr = cumsum(degree(row, num_nodes=num_nodes, dtype=torch.long))\n",
    "    n_id, e_id = torch.ops.torch_cluster.random_walk(rowptr, col, start, walk_length, 1.0, 1.0)\n",
    "    e_id = e_id[e_id != -1].view(-1)  # Filter out illegal edges\n",
    "    \n",
    "    # Adjust for sorted edges if applicable\n",
    "    if edge_orders is not None:\n",
    "        e_id = edge_orders[e_id]\n",
    "    \n",
    "    # Apply mask to edges and return\n",
    "    edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool, device=edge_index.device)\n",
    "    edge_mask[e_id] = False\n",
    "    return edge_index[:, edge_mask], edge_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52505c0",
   "metadata": {},
   "source": [
    "## 4.3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b7b3cce-5dc4-44f9-b7fb-c0ea5ab9ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def train_dropout_path(model, data):\n",
    "    \"\"\"Train a GNN model and return the trained model.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = model.optimizer\n",
    "    epochs = 5\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        edge_index1, _ = dropout_path(data.edge_index, p= 0.2,walks_per_node = 1, walk_length = 3)\n",
    "        _, out = model(data.x, edge_index1 )\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "\n",
    "        # Print metrics every 10 epochs\n",
    "        if(epoch % 1 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n",
    "                  f'{acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cdc39",
   "metadata": {},
   "source": [
    "## 4.4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4844f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data):\n",
    "    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n",
    "    model.eval()\n",
    "    _, out = model(data.x, data.edge_index)\n",
    "    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef64126b-4707-4c8f-9815-64e89330edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_dropout_path(\n",
      "  (gcn1): GCNConv(1433, 16)\n",
      "  (gcn2): GCNConv(16, 7)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.945 | Train Acc:  18.57% | Val Loss: 1.95 | Val Acc: 14.00%\n",
      "Epoch   1 | Train Loss: 1.941 | Train Acc:  25.71% | Val Loss: 1.94 | Val Acc: 17.40%\n",
      "Epoch   2 | Train Loss: 1.934 | Train Acc:  27.14% | Val Loss: 1.94 | Val Acc: 19.40%\n",
      "Epoch   3 | Train Loss: 1.927 | Train Acc:  35.00% | Val Loss: 1.94 | Val Acc: 21.80%\n",
      "Epoch   4 | Train Loss: 1.918 | Train Acc:  37.14% | Val Loss: 1.93 | Val Acc: 25.80%\n",
      "Epoch   5 | Train Loss: 1.908 | Train Acc:  42.86% | Val Loss: 1.93 | Val Acc: 29.00%\n",
      "\n",
      "GCN test accuracy: 60.40%\n",
      "\n",
      "CPU times: user 185 ms, sys: 38.4 ms, total: 223 ms\n",
      "Wall time: 32.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create GCN model\n",
    "gcn_dropout_path = GCN_dropout_path(dataset.num_features, 16, dataset.num_classes).to(device)\n",
    "print(gcn_dropout_path)\n",
    "\n",
    "# Train\n",
    "train_dropout_path(gcn_dropout_path, data.to(device))\n",
    "\n",
    "# Test\n",
    "acc = test(gcn_dropout_path, data.to(device))\n",
    "print(f'\\nGCN test accuracy: {acc*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a4732-99c8-41a1-a942-dcaa3e278633",
   "metadata": {},
   "source": [
    "# 5. MixHop Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c84aa6",
   "metadata": {},
   "source": [
    "## 5.1. MixHopConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f85c3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import zeros\n",
    "from torch_geometric.utils import spmm\n",
    "from typing import List, Optional\n",
    "\n",
    "class MixHopConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,  # Number of input features\n",
    "        out_channels: int,  # Number of output features\n",
    "        powers: Optional[List[int]] = None,  # List of powers for MixHop\n",
    "        add_self_loops: bool = True,  # Whether to add self-loops\n",
    "        bias: bool = True,  # Whether to add a bias term\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr='add', **kwargs)  # Initialize the MessagePassing class with 'add' aggregation\n",
    "        \n",
    "        self.powers = powers or [0, 1, 2]  # Default powers are [0, 1, 2]\n",
    "        self.add_self_loops = add_self_loops  # Store the add_self_loops flag\n",
    "        \n",
    "        # Create a list of linear transformations for each power\n",
    "        self.lins = nn.ModuleList([\n",
    "            Linear(in_channels, out_channels, bias=False) if p in self.powers else nn.Identity()\n",
    "            for p in range(max(self.powers) + 1)\n",
    "        ])\n",
    "        \n",
    "        # Initialize the bias parameter if bias is True\n",
    "        self.bias = Parameter(torch.empty(len(self.powers) * out_channels)) if bias else None\n",
    "        self.reset_parameters()  # Reset parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Reset parameters of each linear transformation\n",
    "        for lin in self.lins:\n",
    "            if hasattr(lin, 'reset_parameters'):\n",
    "                lin.reset_parameters()\n",
    "        zeros(self.bias)  # Initialize the bias to zeros\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index, edge_weight=None) -> Tensor:\n",
    "        # Normalize the edge index and edge weight using GCN normalization\n",
    "        edge_index, edge_weight = gcn_norm(\n",
    "            edge_index, edge_weight, x.size(0), False, self.add_self_loops, self.flow, x.dtype\n",
    "        )\n",
    "        \n",
    "        # Initialize the output list with the transformed input features\n",
    "        outs = [self.lins[0](x)]\n",
    "        \n",
    "        # Propagate the features through the graph for each power\n",
    "        for lin in self.lins[1:]:\n",
    "            x = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "            outs.append(lin(x))\n",
    "\n",
    "        # Concatenate the outputs for each power along the feature dimension\n",
    "        out = torch.cat([outs[p] for p in self.powers], dim=-1)\n",
    "        \n",
    "        # Add the bias term if it exists\n",
    "        return out + self.bias if self.bias is not None else out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight=None) -> Tensor:\n",
    "        # Compute the message to be passed to the target nodes\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x: Tensor) -> Tensor:\n",
    "        # Perform sparse matrix multiplication to aggregate messages\n",
    "        return spmm(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Return a string representation of the MixHopConv layer\n",
    "        return f'{self.__class__.__name__}({self.in_channels}, {self.out_channels}, powers={self.powers})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84509d-462b-4417-9310-d70433239b03",
   "metadata": {},
   "source": [
    "## 5.2. Mixhop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9bd17412-4f7e-4ed0-ac77-582b9866872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import BatchNorm, Linear\n",
    "\n",
    "class MixHop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First MixHopConv layer with powers [0, 1, 2] and 60 output features\n",
    "        self.conv1 = MixHopConv(dataset.num_features, 60, powers=[0, 1, 2])\n",
    "        # Batch normalization for the first layer's output\n",
    "        self.norm1 = BatchNorm(3 * 60)\n",
    "\n",
    "        # Second MixHopConv layer with powers [0, 1, 2] and 60 output features\n",
    "        self.conv2 = MixHopConv(3 * 60, 60, powers=[0, 1, 2])\n",
    "        # Batch normalization for the second layer's output\n",
    "        self.norm2 = BatchNorm(3 * 60)\n",
    "\n",
    "        # Third MixHopConv layer with powers [0, 1, 2] and 60 output features\n",
    "        self.conv3 = MixHopConv(3 * 60, 60, powers=[0, 1, 2])\n",
    "        # Batch normalization for the third layer's output\n",
    "        self.norm3 = BatchNorm(3 * 60)\n",
    "\n",
    "        # Linear layer to map the final output to the number of classes\n",
    "        self.lin = Linear(3 * 60, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply dropout to the input features\n",
    "        x = F.dropout(x, p=0.7, training=self.training)\n",
    "\n",
    "        # First MixHopConv layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        # Apply batch normalization\n",
    "        x = self.norm1(x)\n",
    "        # Apply dropout\n",
    "        x = F.dropout(x, p=0.9, training=self.training)\n",
    "\n",
    "        # Second MixHopConv layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # Apply batch normalization\n",
    "        x = self.norm2(x)\n",
    "        # Apply dropout\n",
    "        x = F.dropout(x, p=0.9, training=self.training)\n",
    "\n",
    "        # Third MixHopConv layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        # Apply batch normalization\n",
    "        x = self.norm3(x)\n",
    "        # Apply dropout\n",
    "        x = F.dropout(x, p=0.9, training=self.training)\n",
    "\n",
    "        # Final linear layer to get the class scores\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35c4ea-757b-43b4-9387-731e23a8d9b9",
   "metadata": {},
   "source": [
    "## 5.3. Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "370c2de8-2f5c-4c19-9eb4-76c9b5a1cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model, data = MixHop().to(device), data.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5, weight_decay=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfa7fb-41dc-40ef-8f39-3b97aae49b30",
   "metadata": {},
   "source": [
    "## 5.4. Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c0e6294-8183-41b0-af93-6a2f2f173bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829929e5-d533-4ed3-a976-f9c39fd15f48",
   "metadata": {},
   "source": [
    "## 5.5. Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b9a0026-9f4a-45e3-93ac-74cedce8923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1fcc481-c5f8-410b-86a3-4ab4e8fea6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3.0828, Train: 0.1500, Val: 0.1600, Test: 0.1460\n",
      "Epoch: 002, Loss: 4.4179, Train: 0.1714, Val: 0.2100, Test: 0.1860\n",
      "Epoch: 003, Loss: 4.1751, Train: 0.1643, Val: 0.2100, Test: 0.1860\n",
      "Epoch: 004, Loss: 4.9424, Train: 0.2714, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 005, Loss: 3.8162, Train: 0.1429, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 006, Loss: 6.1398, Train: 0.2143, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 007, Loss: 5.1311, Train: 0.1571, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 008, Loss: 5.9651, Train: 0.1500, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 009, Loss: 5.3913, Train: 0.1500, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 010, Loss: 5.8275, Train: 0.1500, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 011, Loss: 5.2305, Train: 0.2786, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 012, Loss: 4.4176, Train: 0.1500, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 013, Loss: 6.4840, Train: 0.2000, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 014, Loss: 4.3410, Train: 0.1714, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 015, Loss: 5.0022, Train: 0.3000, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 016, Loss: 4.3941, Train: 0.2071, Val: 0.2160, Test: 0.2020\n",
      "Epoch: 017, Loss: 3.6330, Train: 0.3500, Val: 0.2500, Test: 0.2800\n",
      "Epoch: 018, Loss: 4.5336, Train: 0.1786, Val: 0.2500, Test: 0.2800\n",
      "Epoch: 019, Loss: 4.1530, Train: 0.2929, Val: 0.2500, Test: 0.2800\n",
      "Epoch: 020, Loss: 3.7307, Train: 0.2714, Val: 0.2500, Test: 0.2800\n",
      "Epoch: 021, Loss: 3.9647, Train: 0.4214, Val: 0.2900, Test: 0.2820\n",
      "Epoch: 022, Loss: 2.9331, Train: 0.4857, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 023, Loss: 3.4457, Train: 0.2857, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 024, Loss: 3.6661, Train: 0.2643, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 025, Loss: 3.3236, Train: 0.3000, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 026, Loss: 4.0805, Train: 0.2786, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 027, Loss: 4.5577, Train: 0.2786, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 028, Loss: 3.6674, Train: 0.2214, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 029, Loss: 4.1127, Train: 0.2000, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 030, Loss: 5.0089, Train: 0.2857, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 031, Loss: 4.0616, Train: 0.3214, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 032, Loss: 2.7341, Train: 0.3643, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 033, Loss: 3.2377, Train: 0.2929, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 034, Loss: 3.6483, Train: 0.3071, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 035, Loss: 2.7383, Train: 0.4143, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 036, Loss: 2.7498, Train: 0.4500, Val: 0.3020, Test: 0.3180\n",
      "Epoch: 037, Loss: 2.5186, Train: 0.4071, Val: 0.3060, Test: 0.3040\n",
      "Epoch: 038, Loss: 3.6814, Train: 0.4857, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 039, Loss: 2.5465, Train: 0.4786, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 040, Loss: 2.6176, Train: 0.4429, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 041, Loss: 2.4983, Train: 0.4429, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 042, Loss: 1.9191, Train: 0.4500, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 043, Loss: 2.5059, Train: 0.4714, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 044, Loss: 1.9413, Train: 0.4786, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 045, Loss: 3.0870, Train: 0.4929, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 046, Loss: 2.2753, Train: 0.4929, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 047, Loss: 3.7694, Train: 0.5071, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 048, Loss: 2.5518, Train: 0.5214, Val: 0.3600, Test: 0.3510\n",
      "Epoch: 049, Loss: 2.4836, Train: 0.5286, Val: 0.3760, Test: 0.3560\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43b100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
