{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3dcbfb-8bae-48de-94b2-288f1c0b132d",
   "metadata": {},
   "source": [
    "### Assignment 1. Download \"Citeseer\" graph.\n",
    "Questions: Construct GCN model by using dropping_path technique for node classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564f8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='.', name='Citeseer', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "edge_index  = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3135d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "\n",
    "# Function to extract the k-hop subgraph around a given node or set of nodes\n",
    "def k_hop_subgraph(\n",
    "    node_idx: Union[int, List[int], Tensor],  # The target node(s)\n",
    "    num_hops: int,  # The number of hops k\n",
    "    edge_index: Tensor,  # The edge indices\n",
    "    relabel_nodes: bool = False,  # Whether to relabel nodes to a contiguous range\n",
    "    num_nodes: Optional[int] = None,  # The number of nodes in the graph\n",
    "    flow: str = 'source_to_target',  # The flow direction ('source_to_target' or 'target_to_source')\n",
    "    directed: bool = False,  # Whether the graph is directed\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:  # Returns the subgraph, edge indices, inverse mapping, and edge mask\n",
    "\n",
    "    # Determine the number of nodes if not provided\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    # Ensure the flow direction is valid\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    if flow == 'target_to_source':\n",
    "        row, col = edge_index\n",
    "    else:\n",
    "        col, row = edge_index\n",
    "\n",
    "    # Initialize masks for nodes and edges\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    # Convert node_idx to a tensor if it is not already\n",
    "    if isinstance(node_idx, (int, list, tuple)):\n",
    "        node_idx = torch.tensor([node_idx], device=row.device).flatten()\n",
    "    else:\n",
    "        node_idx = node_idx.to(row.device)\n",
    "\n",
    "    # List to store the subsets of nodes at each hop\n",
    "    subsets = [node_idx]\n",
    "\n",
    "    # Perform k-hop expansion\n",
    "    for _ in range(num_hops):\n",
    "        node_mask.fill_(False)\n",
    "        node_mask[subsets[-1]] = True\n",
    "        torch.index_select(node_mask, 0, row, out=edge_mask)\n",
    "        subsets.append(col[edge_mask])\n",
    "\n",
    "    # Concatenate all subsets and get unique nodes\n",
    "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "    inv = inv[:node_idx.numel()]\n",
    "\n",
    "    # Create a mask for the subset of nodes\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "\n",
    "    # If the graph is undirected, update the edge mask\n",
    "    if not directed:\n",
    "        edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    # Filter the edge index to include only the edges in the subgraph\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    # Relabel nodes to a contiguous range if specified\n",
    "    if relabel_nodes:\n",
    "        node_idx = row.new_full((num_nodes, ), -1)\n",
    "        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "        edge_index = node_idx[edge_index]\n",
    "\n",
    "    # Return the subset of nodes, the filtered edge index, the inverse mapping, and the edge mask\n",
    "    return subset, edge_index, inv, edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be72b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN_dropout_path(torch.nn.Module):\n",
    "    \"\"\"Graph Convolutional Network\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.5, training=self.training)\n",
    "        h = self.gcn1(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        return h, F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac7be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_dropout_path(\n",
      "  (gcn1): GCNConv(3703, 16)\n",
      "  (gcn2): GCNConv(16, 6)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.792 | Train Acc:  17.50% | Val Loss: 1.79 | Val Acc: 13.00%\n",
      "Epoch   1 | Train Loss: 1.789 | Train Acc:  18.33% | Val Loss: 1.79 | Val Acc: 6.60%\n",
      "Epoch   2 | Train Loss: 1.784 | Train Acc:  25.83% | Val Loss: 1.79 | Val Acc: 7.40%\n",
      "Epoch   3 | Train Loss: 1.778 | Train Acc:  36.67% | Val Loss: 1.79 | Val Acc: 16.60%\n",
      "Epoch   4 | Train Loss: 1.773 | Train Acc:  38.33% | Val Loss: 1.79 | Val Acc: 22.00%\n",
      "Epoch   5 | Train Loss: 1.772 | Train Acc:  41.67% | Val Loss: 1.78 | Val Acc: 24.60%\n",
      "\n",
      "GCN test accuracy: 36.00%\n",
      "\n",
      "Execution time: 0.8125090599060059 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import cumsum, degree, sort_edge_index, subgraph\n",
    "from torch_geometric import is_compiling\n",
    "import torch_geometric.typing\n",
    "\n",
    "def dropout_path(edge_index: Tensor, p: float = 0.2, walks_per_node: int = 1,\n",
    "                 walk_length: int = 3, num_nodes: Optional[int] = None,\n",
    "                 is_sorted: bool = False, training: bool = True) -> Tuple[Tensor, Tensor]:\n",
    "    # Ensure probability is within range\n",
    "    if not (0.0 <= p <= 1.0):\n",
    "        raise ValueError(f'Sample probability must be between 0 and 1 (got {p})')\n",
    "\n",
    "    # Return unchanged edge_index if not in training mode or p=0\n",
    "    if not training or p == 0.0:\n",
    "        return edge_index, torch.ones(edge_index.size(1), dtype=torch.bool, device=edge_index.device)\n",
    "\n",
    "    # Ensure required torch-cluster support is available\n",
    "    if not torch_geometric.typing.WITH_TORCH_CLUSTER or is_compiling():\n",
    "        raise ImportError('`dropout_path` requires `torch-cluster`.')\n",
    "\n",
    "    # Sort edges if necessary\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "    edge_orders = None\n",
    "    if not is_sorted:\n",
    "        edge_orders = torch.arange(edge_index.size(1), device=edge_index.device)\n",
    "        edge_index, edge_orders = sort_edge_index(edge_index, edge_orders, num_nodes=num_nodes)\n",
    "\n",
    "    # Randomly mask edges\n",
    "    row, col = edge_index\n",
    "    sample_mask = torch.rand(row.size(0), device=edge_index.device) <= p\n",
    "    start = row[sample_mask].repeat(walks_per_node)\n",
    "\n",
    "    # Perform random walk to determine paths\n",
    "    rowptr = cumsum(degree(row, num_nodes=num_nodes, dtype=torch.long))\n",
    "    n_id, e_id = torch.ops.torch_cluster.random_walk(rowptr, col, start, walk_length, 1.0, 1.0)\n",
    "    e_id = e_id[e_id != -1].view(-1)  # Filter out illegal edges\n",
    "\n",
    "    # Adjust for sorted edges if applicable\n",
    "    if edge_orders is not None:\n",
    "        e_id = edge_orders[e_id]\n",
    "\n",
    "    # Apply mask to edges and return\n",
    "    edge_mask = torch.ones(edge_index.size(1), dtype=torch.bool, device=edge_index.device)\n",
    "    edge_mask[e_id] = False\n",
    "    return edge_index[:, edge_mask], edge_mask\n",
    "\n",
    "\n",
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "\n",
    "def train_dropout_path(model, data):\n",
    "    \"\"\"Train a GNN model and return the trained model.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = model.optimizer\n",
    "    epochs = 5\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs + 1):\n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        edge_index1, _ = dropout_path(data.edge_index, p=0.2, walks_per_node=1, walk_length=3)\n",
    "        _, out = model(data.x, edge_index1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "\n",
    "        # Print metrics every 10 epochs\n",
    "        if (epoch % 1 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n",
    "                  f'{acc * 100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc * 100:.2f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n",
    "    model.eval()\n",
    "    _, out = model(data.x, data.edge_index)\n",
    "    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "    return acc\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create GCN model\n",
    "gcn_dropout_path = GCN_dropout_path(dataset.num_features, 16, dataset.num_classes).to(device)\n",
    "print(gcn_dropout_path)\n",
    "\n",
    "# Train\n",
    "train_dropout_path(gcn_dropout_path, data.to(device))\n",
    "\n",
    "# Test\n",
    "acc = test(gcn_dropout_path, data.to(device))\n",
    "print(f'\\nGCN test accuracy: {acc*100:.2f}%\\n')\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65cba6-fcad-41a1-a6ef-cc209e6f77c9",
   "metadata": {},
   "source": [
    "### Assignment 2. Load the Cora dataset from Torch Geometric.\n",
    "\n",
    "Questions: Train the Mixhop model on node classification task with 2-step transition and 3-step transition steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee2216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='.', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "edge_index  = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bcce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import zeros\n",
    "from torch_geometric.utils import spmm\n",
    "from typing import List, Optional\n",
    "\n",
    "class MixHopConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,  # Number of input features\n",
    "        out_channels: int,  # Number of output features\n",
    "        powers: Optional[List[int]] = None,  # List of powers for MixHop\n",
    "        add_self_loops: bool = True,  # Whether to add self-loops\n",
    "        bias: bool = True,  # Whether to add a bias term\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr='add', **kwargs)  # Initialize the MessagePassing class with 'add' aggregation\n",
    "        \n",
    "        self.powers = powers or [0, 1, 2]  # Default powers are [0, 1, 2]\n",
    "        self.add_self_loops = add_self_loops  # Store the add_self_loops flag\n",
    "        \n",
    "        # Create a list of linear transformations for each power\n",
    "        self.lins = nn.ModuleList([\n",
    "            Linear(in_channels, out_channels, bias=False) if p in self.powers else nn.Identity()\n",
    "            for p in range(max(self.powers) + 1)\n",
    "        ])\n",
    "        \n",
    "        # Initialize the bias parameter if bias is True\n",
    "        self.bias = Parameter(torch.empty(len(self.powers) * out_channels)) if bias else None\n",
    "        self.reset_parameters()  # Reset parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Reset parameters of each linear transformation\n",
    "        for lin in self.lins:\n",
    "            if hasattr(lin, 'reset_parameters'):\n",
    "                lin.reset_parameters()\n",
    "        zeros(self.bias)  # Initialize the bias to zeros\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index, edge_weight=None) -> Tensor:\n",
    "        # Normalize the edge index and edge weight using GCN normalization\n",
    "        edge_index, edge_weight = gcn_norm(\n",
    "            edge_index, edge_weight, x.size(0), False, self.add_self_loops, self.flow, x.dtype\n",
    "        )\n",
    "        \n",
    "        # Initialize the output list with the transformed input features\n",
    "        outs = [self.lins[0](x)]\n",
    "        \n",
    "        # Propagate the features through the graph for each power\n",
    "        for lin in self.lins[1:]:\n",
    "            x = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "            outs.append(lin(x))\n",
    "\n",
    "        # Concatenate the outputs for each power along the feature dimension\n",
    "        out = torch.cat([outs[p] for p in self.powers], dim=-1)\n",
    "        \n",
    "        # Add the bias term if it exists\n",
    "        return out + self.bias if self.bias is not None else out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight=None) -> Tensor:\n",
    "        # Compute the message to be passed to the target nodes\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x: Tensor) -> Tensor:\n",
    "        # Perform sparse matrix multiplication to aggregate messages\n",
    "        return spmm(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Return a string representation of the MixHopConv layer\n",
    "        return f'{self.__class__.__name__}({self.in_channels}, {self.out_channels}, powers={self.powers})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a60d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import BatchNorm, Linear\n",
    "\n",
    "class MixHop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First MixHopConv layer with powers [0, 1, 2] and 60 output features\n",
    "        self.conv1 = MixHopConv(dataset.num_features, 60, powers=[0, 1, 2])\n",
    "        # Batch normalization for the first layer's output\n",
    "        self.norm1 = BatchNorm(3 * 60)\n",
    "\n",
    "        # Second MixHopConv layer with powers [0, 1, 2] and 60 output features\n",
    "        self.conv2 = MixHopConv(3 * 60, 60, powers=[0, 1, 2])\n",
    "        # Batch normalization for the second layer's output\n",
    "        self.norm2 = BatchNorm(3 * 60)\n",
    "\n",
    "        # Third MixHopConv layer with powers [0, 1, 2] and 60 output features\n",
    "        self.conv3 = MixHopConv(3 * 60, 60, powers=[0, 1, 2])\n",
    "        # Batch normalization for the third layer's output\n",
    "        self.norm3 = BatchNorm(3 * 60)\n",
    "\n",
    "        # Linear layer to map the final output to the number of classes\n",
    "        self.lin = Linear(3 * 60, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply dropout to the input features\n",
    "        x = F.dropout(x, p=0.7, training=self.training)\n",
    "\n",
    "        # First MixHopConv layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        # Apply batch normalization\n",
    "        x = self.norm1(x)\n",
    "        # Apply dropout\n",
    "        x = F.dropout(x, p=0.9, training=self.training)\n",
    "\n",
    "        # Second MixHopConv layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # Apply batch normalization\n",
    "        x = self.norm2(x)\n",
    "        # Apply dropout\n",
    "        x = F.dropout(x, p=0.9, training=self.training)\n",
    "\n",
    "        # Third MixHopConv layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        # Apply batch normalization\n",
    "        x = self.norm3(x)\n",
    "        # Apply dropout\n",
    "        x = F.dropout(x, p=0.9, training=self.training)\n",
    "\n",
    "        # Final linear layer to get the class scores\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb35d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model, data = MixHop().to(device), data.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5, weight_decay=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ff2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c22797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05053969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3.0005, Train: 0.1429, Val: 0.0580, Test: 0.0640\n",
      "Epoch: 002, Loss: 4.2431, Train: 0.1643, Val: 0.1360, Test: 0.1730\n",
      "Epoch: 003, Loss: 3.7366, Train: 0.1429, Val: 0.3200, Test: 0.3200\n",
      "Epoch: 004, Loss: 5.0231, Train: 0.2286, Val: 0.1100, Test: 0.1340\n",
      "Epoch: 005, Loss: 5.0167, Train: 0.1786, Val: 0.3180, Test: 0.3170\n",
      "Epoch: 006, Loss: 7.6017, Train: 0.2000, Val: 0.1340, Test: 0.1330\n",
      "Epoch: 007, Loss: 7.5737, Train: 0.3714, Val: 0.2220, Test: 0.2400\n",
      "Epoch: 008, Loss: 6.2074, Train: 0.2571, Val: 0.1300, Test: 0.1390\n",
      "Epoch: 009, Loss: 7.0606, Train: 0.1929, Val: 0.0900, Test: 0.1060\n",
      "Epoch: 010, Loss: 5.2631, Train: 0.1714, Val: 0.0760, Test: 0.1060\n",
      "Epoch: 011, Loss: 8.0932, Train: 0.1643, Val: 0.3180, Test: 0.3230\n",
      "Epoch: 012, Loss: 9.0790, Train: 0.1714, Val: 0.1740, Test: 0.1490\n",
      "Epoch: 013, Loss: 5.7691, Train: 0.1643, Val: 0.0980, Test: 0.1070\n",
      "Epoch: 014, Loss: 5.7827, Train: 0.1500, Val: 0.1600, Test: 0.1420\n",
      "Epoch: 015, Loss: 4.7050, Train: 0.2143, Val: 0.1780, Test: 0.1610\n",
      "Epoch: 016, Loss: 4.7978, Train: 0.1857, Val: 0.1740, Test: 0.1540\n",
      "Epoch: 017, Loss: 5.4023, Train: 0.1571, Val: 0.1320, Test: 0.1100\n",
      "Epoch: 018, Loss: 5.0578, Train: 0.3143, Val: 0.1740, Test: 0.1710\n",
      "Epoch: 019, Loss: 7.5375, Train: 0.3286, Val: 0.2820, Test: 0.2640\n",
      "Epoch: 020, Loss: 5.0889, Train: 0.2000, Val: 0.1760, Test: 0.1540\n",
      "Epoch: 021, Loss: 6.1771, Train: 0.3286, Val: 0.2400, Test: 0.2190\n",
      "Epoch: 022, Loss: 4.1578, Train: 0.2857, Val: 0.2500, Test: 0.2220\n",
      "Epoch: 023, Loss: 3.3177, Train: 0.2000, Val: 0.1840, Test: 0.1680\n",
      "Epoch: 024, Loss: 4.0917, Train: 0.2786, Val: 0.2360, Test: 0.2420\n",
      "Epoch: 025, Loss: 3.6973, Train: 0.2857, Val: 0.1840, Test: 0.1890\n",
      "Epoch: 026, Loss: 4.2193, Train: 0.3357, Val: 0.2600, Test: 0.2360\n",
      "Epoch: 027, Loss: 3.7796, Train: 0.3500, Val: 0.2720, Test: 0.2490\n",
      "Epoch: 028, Loss: 4.6321, Train: 0.3071, Val: 0.2360, Test: 0.2130\n",
      "Epoch: 029, Loss: 3.0294, Train: 0.2000, Val: 0.1900, Test: 0.1690\n",
      "Epoch: 030, Loss: 3.4829, Train: 0.2714, Val: 0.2040, Test: 0.1930\n",
      "Epoch: 031, Loss: 4.0383, Train: 0.3143, Val: 0.2640, Test: 0.2510\n",
      "Epoch: 032, Loss: 3.0099, Train: 0.3143, Val: 0.2460, Test: 0.2300\n",
      "Epoch: 033, Loss: 3.1340, Train: 0.3286, Val: 0.2560, Test: 0.2380\n",
      "Epoch: 034, Loss: 3.2554, Train: 0.3500, Val: 0.2760, Test: 0.2580\n",
      "Epoch: 035, Loss: 2.7690, Train: 0.3286, Val: 0.2560, Test: 0.2450\n",
      "Epoch: 036, Loss: 2.8181, Train: 0.3143, Val: 0.2640, Test: 0.2470\n",
      "Epoch: 037, Loss: 2.9503, Train: 0.3071, Val: 0.2520, Test: 0.2360\n",
      "Epoch: 038, Loss: 2.2758, Train: 0.3143, Val: 0.2680, Test: 0.2550\n",
      "Epoch: 039, Loss: 2.5921, Train: 0.4000, Val: 0.3120, Test: 0.2930\n",
      "Epoch: 040, Loss: 2.3607, Train: 0.3214, Val: 0.3360, Test: 0.3530\n",
      "Epoch: 041, Loss: 2.7539, Train: 0.3429, Val: 0.3360, Test: 0.3520\n",
      "Epoch: 042, Loss: 2.6513, Train: 0.3429, Val: 0.3360, Test: 0.3640\n",
      "Epoch: 043, Loss: 3.5739, Train: 0.3786, Val: 0.3400, Test: 0.3740\n",
      "Epoch: 044, Loss: 2.7171, Train: 0.4143, Val: 0.3420, Test: 0.3780\n",
      "Epoch: 045, Loss: 4.3141, Train: 0.4214, Val: 0.3660, Test: 0.3910\n",
      "Epoch: 046, Loss: 2.3916, Train: 0.4643, Val: 0.3700, Test: 0.4000\n",
      "Epoch: 047, Loss: 2.3620, Train: 0.4929, Val: 0.3900, Test: 0.4140\n",
      "Epoch: 048, Loss: 2.5022, Train: 0.4929, Val: 0.4040, Test: 0.4280\n",
      "Epoch: 049, Loss: 3.8518, Train: 0.5214, Val: 0.4200, Test: 0.4450\n",
      "Epoch: 050, Loss: 1.8758, Train: 0.5143, Val: 0.4060, Test: 0.4390\n",
      "Epoch: 051, Loss: 2.3285, Train: 0.5357, Val: 0.4120, Test: 0.4470\n",
      "Epoch: 052, Loss: 3.1497, Train: 0.5714, Val: 0.4220, Test: 0.4560\n",
      "Epoch: 053, Loss: 2.7885, Train: 0.5714, Val: 0.4400, Test: 0.4720\n",
      "Epoch: 054, Loss: 2.1452, Train: 0.5786, Val: 0.4660, Test: 0.4870\n",
      "Epoch: 055, Loss: 2.2508, Train: 0.5929, Val: 0.4800, Test: 0.5010\n",
      "Epoch: 056, Loss: 2.6360, Train: 0.6286, Val: 0.5020, Test: 0.5140\n",
      "Epoch: 057, Loss: 3.0468, Train: 0.6357, Val: 0.5160, Test: 0.5210\n",
      "Epoch: 058, Loss: 2.8496, Train: 0.6571, Val: 0.5360, Test: 0.5380\n",
      "Epoch: 059, Loss: 2.7329, Train: 0.6571, Val: 0.5440, Test: 0.5500\n",
      "Epoch: 060, Loss: 2.6549, Train: 0.6786, Val: 0.5480, Test: 0.5590\n",
      "Epoch: 061, Loss: 2.1599, Train: 0.6929, Val: 0.5520, Test: 0.5700\n",
      "Epoch: 062, Loss: 2.1636, Train: 0.6857, Val: 0.5560, Test: 0.5710\n",
      "Epoch: 063, Loss: 2.5070, Train: 0.7071, Val: 0.5600, Test: 0.5790\n",
      "Epoch: 064, Loss: 3.9451, Train: 0.7071, Val: 0.5620, Test: 0.5880\n",
      "Epoch: 065, Loss: 3.5645, Train: 0.7214, Val: 0.5620, Test: 0.5980\n",
      "Epoch: 066, Loss: 2.4851, Train: 0.7286, Val: 0.5660, Test: 0.6000\n",
      "Epoch: 067, Loss: 2.5070, Train: 0.7357, Val: 0.5700, Test: 0.6090\n",
      "Epoch: 068, Loss: 3.2507, Train: 0.7429, Val: 0.5740, Test: 0.6120\n",
      "Epoch: 069, Loss: 3.9517, Train: 0.7571, Val: 0.5720, Test: 0.6140\n",
      "Epoch: 070, Loss: 2.7449, Train: 0.7714, Val: 0.5820, Test: 0.6190\n",
      "Epoch: 071, Loss: 2.4041, Train: 0.7786, Val: 0.5840, Test: 0.6210\n",
      "Epoch: 072, Loss: 2.7056, Train: 0.7786, Val: 0.5840, Test: 0.6210\n",
      "Epoch: 073, Loss: 2.2669, Train: 0.7857, Val: 0.5840, Test: 0.6190\n",
      "Epoch: 074, Loss: 2.7829, Train: 0.7857, Val: 0.5880, Test: 0.6230\n",
      "Epoch: 075, Loss: 2.5966, Train: 0.7857, Val: 0.5900, Test: 0.6280\n",
      "Epoch: 076, Loss: 3.9002, Train: 0.7857, Val: 0.5900, Test: 0.6280\n",
      "Epoch: 077, Loss: 4.1348, Train: 0.7929, Val: 0.5960, Test: 0.6310\n",
      "Epoch: 078, Loss: 2.4358, Train: 0.7929, Val: 0.5940, Test: 0.6300\n",
      "Epoch: 079, Loss: 2.4649, Train: 0.7929, Val: 0.6000, Test: 0.6380\n",
      "Epoch: 080, Loss: 3.9453, Train: 0.8000, Val: 0.6040, Test: 0.6380\n",
      "Epoch: 081, Loss: 3.2280, Train: 0.8071, Val: 0.6060, Test: 0.6390\n",
      "Epoch: 082, Loss: 2.9221, Train: 0.8071, Val: 0.6060, Test: 0.6390\n",
      "Epoch: 083, Loss: 2.8034, Train: 0.8071, Val: 0.6120, Test: 0.6410\n",
      "Epoch: 084, Loss: 3.0025, Train: 0.8000, Val: 0.6120, Test: 0.6390\n",
      "Epoch: 085, Loss: 2.4561, Train: 0.8000, Val: 0.6100, Test: 0.6420\n",
      "Epoch: 086, Loss: 2.9958, Train: 0.8000, Val: 0.6140, Test: 0.6470\n",
      "Epoch: 087, Loss: 2.8011, Train: 0.8000, Val: 0.6100, Test: 0.6450\n",
      "Epoch: 088, Loss: 3.1206, Train: 0.8000, Val: 0.6120, Test: 0.6460\n",
      "Epoch: 089, Loss: 2.3708, Train: 0.8000, Val: 0.6160, Test: 0.6510\n",
      "Epoch: 090, Loss: 2.9481, Train: 0.8071, Val: 0.6160, Test: 0.6540\n",
      "Epoch: 091, Loss: 2.5384, Train: 0.8143, Val: 0.6160, Test: 0.6540\n",
      "Epoch: 092, Loss: 3.3851, Train: 0.8143, Val: 0.6180, Test: 0.6550\n",
      "Epoch: 093, Loss: 3.0948, Train: 0.8143, Val: 0.6180, Test: 0.6550\n",
      "Epoch: 094, Loss: 2.4253, Train: 0.8143, Val: 0.6160, Test: 0.6520\n",
      "Epoch: 095, Loss: 2.0967, Train: 0.8143, Val: 0.6160, Test: 0.6550\n",
      "Epoch: 096, Loss: 2.4247, Train: 0.8214, Val: 0.6180, Test: 0.6570\n",
      "Epoch: 097, Loss: 2.4984, Train: 0.8143, Val: 0.6160, Test: 0.6570\n",
      "Epoch: 098, Loss: 2.4130, Train: 0.8143, Val: 0.6120, Test: 0.6450\n",
      "Epoch: 099, Loss: 3.6506, Train: 0.8071, Val: 0.6120, Test: 0.6490\n",
      "\n",
      "Best Validation Accuracy: 0.6180\n",
      "Corresponding Test Accuracy: 0.6550\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}')\n",
    "\n",
    "\n",
    "print(f'\\nBest Validation Accuracy: {best_val_acc:.4f}')\n",
    "print(f'Corresponding Test Accuracy: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
