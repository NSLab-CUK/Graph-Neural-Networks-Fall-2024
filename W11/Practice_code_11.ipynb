{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv-tHPvR-JKa"
   },
   "source": [
    "# 1. GT (Graph Transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8wIJZQqODy-7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /home/ubuntu/anaconda3/lib/python3.12/site-packages (4.0.0)\n",
      "installing ... \n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/conda/base/context.py:198: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/conda/base/context.py:198: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n",
      "Channels:\n",
      " - pytorch\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torchvision) (2.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch->torchvision) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchvision) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from jinja2->torch->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from sympy->torch->torchvision) (1.3.0)\n",
      "Requirement already satisfied: ogb in /home/ubuntu/anaconda3/lib/python3.12/site-packages (1.3.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (1.4.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (2.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (2.2.2)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from ogb) (0.2.2)\n",
      "Requirement already satisfied: setuptools>=44 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from outdated>=0.2.0->ogb) (69.5.1)\n",
      "Requirement already satisfied: littleutils in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from outdated>=0.2.0->ogb) (2.32.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->ogb) (2.2.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->outdated>=0.2.0->ogb) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->outdated>=0.2.0->ogb) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "installed!\n",
      "DGL installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "!pip install chardet\n",
    "print(\"installing ... \")\n",
    "!conda install -y pytorch torchvision torchaudio pytorch-cuda -c pytorch -c nvidia\n",
    "# print(\"installed! torch 2.0.0\")\n",
    "# !conda install -c dglteam/label/cu117 dgl\n",
    "\n",
    "\n",
    "\n",
    "!pip install torchvision \n",
    "# !pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
    "# !pip install dgl -f https://data.dgl.ai/wheels/cu124.html\n",
    "!pip install ogb\n",
    "\n",
    "print(\"installed!\")\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    installed = True\n",
    "except ImportError:\n",
    "    installed = False\n",
    "print(\"DGL installed!\" if installed else \"Failed to install DGL!\")\n",
    "\n",
    "#!pip install  torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
    "# pip install chardet\n",
    "# Uncomment below to install required packages. If the CUDA version is not 11.8,\n",
    "# check the https://www.dgl.ai/pages/start.html to find the supported CUDA\n",
    "# version and corresponding command to install DGL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOpFdtLI-JKb"
   },
   "source": [
    "## 1.1. Sparse Multi-head Attention\n",
    "\n",
    "Recall the all-pairs scaled-dot-product attention mechanism in vanillar Transformer:\n",
    "\n",
    "$$\\text{Attn}=\\text{softmax}(\\dfrac{QK^T} {\\sqrt{d}})V,$$\n",
    "\n",
    "The graph transformer (GT) model employs a Sparse Multi-head Attention block:\n",
    "\n",
    "$$\\text{SparseAttn}(Q, K, V, A) = \\text{softmax}(\\frac{(QK^T) \\circ A}{\\sqrt{d}})V,$$\n",
    "\n",
    "where $Q, K, V ∈\\mathbb{R}^{N\\times d}$ are query feature, key feature, and value feature, respectively. $A\\in[0,1]^{N\\times N}$ is the adjacency matrix of the input graph. $(QK^T)\\circ A$ means that the multiplication of query matrix and key matrix is followed by a Hadamard product (or element-wise multiplication) with the sparse adjacency matrix as illustrated in the figure below:\n",
    "![Sample Image](Fig1.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. SparseMHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SparseMHA(nn.Module):\n",
    "    \"\"\"Sparse Multi-head Attention Module using PyTorch sparse operations\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=80, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, A, h):\n",
    "        N = len(h)\n",
    "        # [N, head_dim, num_heads]\n",
    "        q = self.q_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "        q *= self.scaling\n",
    "        # [N, head_dim, num_heads]\n",
    "        k = self.k_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "        # [N, head_dim, num_heads]\n",
    "        v = self.v_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "\n",
    "        ######################################################################\n",
    "        # Compute the multi-head attention with PyTorch sparse matrix operations\n",
    "        ######################################################################\n",
    "        # Perform sparse-dense-dense matrix multiplication\n",
    "        attn = torch.bmm(q.transpose(1, 2), k)  # Shape: [num_heads, N, N]\n",
    "        attn = F.softmax(attn, dim=-1)  # Apply softmax on each head\n",
    "\n",
    "        # Perform the sparse attention computation\n",
    "        out = torch.bmm(attn, v.transpose(1, 2))  # Shape: [num_heads, N, head_dim]\n",
    "\n",
    "        # Reshape and project output\n",
    "        out = out.transpose(1, 2).reshape(N, -1)\n",
    "        return self.out_proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Fm6Lrx-JKc"
   },
   "source": [
    "## 1.3. Graph Transformer Layer\n",
    "\n",
    "The GT layer is composed of Multi-head Attention, Batch Norm, and Feed-forward Network, connected by residual links as in vanilla transformer.\n",
    "\n",
    "![Sample Image](Fig2.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "M6h7JVWT-JKd"
   },
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "    \"\"\"Graph Transformer Layer\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=80, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.MHA = SparseMHA(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.FFN1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.FFN2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, A, h):\n",
    "        h1 = h\n",
    "        h = self.MHA(A, h)\n",
    "        h = self.batchnorm1(h + h1)\n",
    "\n",
    "        h2 = h\n",
    "        h = self.FFN2(F.relu(self.FFN1(h)))\n",
    "        h = h2 + h\n",
    "\n",
    "        return self.batchnorm2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t40DhVjI-JKd"
   },
   "source": [
    "## 1.4. Graph Transformer Model\n",
    "\n",
    "The GT model is constructed by stacking GT layers. The input positional encoding of vanilla transformer is replaced with Laplacian positional encoding [(Dwivedi et al. 2020)](https://arxiv.org/abs/2003.00982). For the graph-level prediction task, an extra pooler is stacked on top of GT layers to aggregate node feature of the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UrjvEBrF-JKe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dgl.nn import SumPooling\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class GTModel(nn.Module):\n",
    "    def __init__(self, out_size, hidden_size=80, pos_enc_size=2, num_layers=8, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.atom_encoder = AtomEncoder(hidden_size)\n",
    "        self.pos_linear = nn.Linear(pos_enc_size, hidden_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GTLayer(hidden_size, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.pooler = SumPooling()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, out_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, g, X, pos_enc):\n",
    "        # Create adjacency matrix as a dense tensor\n",
    "        indices = torch.stack(g.edges())\n",
    "        N = g.num_nodes()\n",
    "        A = torch.zeros((N, N), device=X.device)\n",
    "        A[indices[0], indices[1]] = 1  # Undirected graph for symmetry\n",
    "\n",
    "        # Initial feature processing\n",
    "        h = self.atom_encoder(X) + self.pos_linear(pos_enc)\n",
    "        for layer in self.layers:\n",
    "            h = layer(A, h)  # Pass dense adjacency instead of sparse matrix\n",
    "        h = self.pooler(g, h)\n",
    "\n",
    "        return self.predictor(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdrPU18I-JKe"
   },
   "source": [
    "## 1.5. Training\n",
    "\n",
    "We train the GT model on [ogbg-molhiv](https://ogb.stanford.edu/docs/graphprop/#ogbg-mol) benchmark. \n",
    "The Laplacian positional encoding of each graph is pre-computed as part of the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V41i0w-9-JKe",
    "outputId": "15343d1a-a32d-4677-d053-d9da96910f43"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, evaluator, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batched_g, labels in dataloader:\n",
    "        batched_g, labels = batched_g.to(device), labels.to(device)\n",
    "        y_hat = model(batched_g, batched_g.ndata[\"feat\"], batched_g.ndata[\"PE\"])\n",
    "        y_true.append(labels.view(y_hat.shape).detach().cpu())\n",
    "        y_pred.append(y_hat.detach().cpu())\n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    return evaluator.eval(input_dict)[\"rocauc\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "from ogb.graphproppred import collate_dgl\n",
    "\n",
    "def train(model, dataset, evaluator, device):\n",
    "    train_dataloader = GraphDataLoader(\n",
    "        dataset[dataset.train_idx],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_dgl,\n",
    "    )\n",
    "    valid_dataloader = GraphDataLoader(\n",
    "        dataset[dataset.val_idx], batch_size=256, collate_fn=collate_dgl\n",
    "    )\n",
    "    test_dataloader = GraphDataLoader(\n",
    "        dataset[dataset.test_idx], batch_size=256, collate_fn=collate_dgl\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 5\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=num_epochs, gamma=0.5\n",
    "    )\n",
    "    loss_fcn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batched_g, labels in train_dataloader:\n",
    "            batched_g, labels = batched_g.to(device), labels.to(device)\n",
    "            logits = model(\n",
    "                batched_g, batched_g.ndata[\"feat\"], batched_g.ndata[\"PE\"]\n",
    "            )\n",
    "            loss = loss_fcn(logits, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        val_metric = evaluate(model, valid_dataloader, evaluator, device)\n",
    "        test_metric = evaluate(model, test_dataloader, evaluator, device)\n",
    "        print(\n",
    "            f\"Epoch: {epoch:03d}, Loss: {avg_loss:.4f}, \"\n",
    "            f\"Val: {val_metric:.4f}, Test: {test_metric:.4f}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Laplacian PE:   0%|          | 0/4000 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.12/site-packages/dgl/transforms/functional.py:3725: DGLWarning: dgl.laplacian_pe will be deprecated. Use dgl.lap_pe please.\n",
      "  dgl_warning(\"dgl.laplacian_pe will be deprecated. Use dgl.lap_pe please.\")\n",
      "Computing Laplacian PE: 100%|██████████| 4000/4000 [00:06<00:00, 613.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.3307, Val: 0.3266, Test: 0.3441\n",
      "Epoch: 001, Loss: 0.1917, Val: 0.4158, Test: 0.3426\n",
      "Epoch: 002, Loss: 0.1621, Val: 0.6177, Test: 0.4677\n",
      "Epoch: 003, Loss: 0.1402, Val: 0.6287, Test: 0.4744\n",
      "Epoch: 004, Loss: 0.1281, Val: 0.6714, Test: 0.5341\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import AsGraphPredDataset\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator\n",
    "from tqdm import tqdm\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "\n",
    "# Training device.\n",
    "# dev = torch.device(\"cpu\")\n",
    "#Be sure to install DGL with CUDA support.\n",
    "dev = torch.device(\"cuda:0\")\n",
    "\n",
    "# Load dataset.\n",
    "pos_enc_size = 8\n",
    "dataset = AsGraphPredDataset(\n",
    "    DglGraphPropPredDataset(\"ogbg-molhiv\", \"./data/OGB\")\n",
    ")\n",
    "evaluator = Evaluator(\"ogbg-molhiv\")\n",
    "\n",
    "# Down sample the dataset to make the tutorial run faster.\n",
    "import random\n",
    "random.seed(42)\n",
    "train_size = len(dataset.train_idx)\n",
    "val_size = len(dataset.val_idx)\n",
    "test_size = len(dataset.test_idx)\n",
    "dataset.train_idx = dataset.train_idx[\n",
    "    torch.LongTensor(random.sample(range(train_size), 2000))\n",
    "]\n",
    "dataset.val_idx = dataset.val_idx[\n",
    "    torch.LongTensor(random.sample(range(val_size), 1000))\n",
    "]\n",
    "dataset.test_idx = dataset.test_idx[\n",
    "    torch.LongTensor(random.sample(range(test_size), 1000))\n",
    "]\n",
    "\n",
    "# Laplacian positional encoding.\n",
    "indices = torch.cat([dataset.train_idx, dataset.val_idx, dataset.test_idx])\n",
    "for idx in tqdm(indices, desc=\"Computing Laplacian PE\"):\n",
    "    g, _ = dataset[idx]\n",
    "    g.ndata[\"PE\"] = dgl.laplacian_pe(g, k=pos_enc_size, padding=True)\n",
    "\n",
    "# Create model.\n",
    "out_size = dataset.num_tasks\n",
    "model = GTModel(out_size=out_size, pos_enc_size=pos_enc_size).to(dev)\n",
    "\n",
    "# Kick off training.\n",
    "train(model, dataset, evaluator, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GraphGPS: General Powerful Scalable Graph Transformers\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*QKN2j0vBNS8fF-W2EuW5NQ.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torchversion = torch.__version__\n",
    "print(torchversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric\n",
    "# !pip install torch_geometric\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+${11.7}.html\n",
    "# #!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{2.0.0}.html\n",
    "# #!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{2.0.0}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Embedding,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, GPSConv, global_add_pool\n",
    "from torch_geometric.nn.attention import PerformerAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ZINC-pe1'\n",
    "dev = torch.device(\"cuda:0\")\n",
    "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')\n",
    "train_dataset = ZINC(path, subset=True, split='train')\n",
    "val_dataset = ZINC(path, subset=True, split='val')\n",
    "test_dataset = ZINC(path, subset=True, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# #parser.add_argument( default='multihead')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "#*** Importance:\n",
    "# Copy file add_positional_encoding.py \n",
    "# to anaconda3\\envs\\your envs name\\lib\\site-packages\\torch_geometric\\transforms\\add_positional_encoding.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS(torch.nn.Module):\n",
    "    def __init__(self, channels: int, pe_dim: int, num_layers: int,\n",
    "                 attn_type: str, attn_kwargs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_emb = Embedding(28, channels )\n",
    "        self.pe_lin = Linear(20, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(20)\n",
    "        self.edge_emb = Embedding(4, channels)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(channels, channels),\n",
    "                ReLU(),\n",
    "                Linear(channels, channels),\n",
    "            )\n",
    "            conv = GPSConv(channels, GINEConv(nn), heads=4,\n",
    "                           attn_type=attn_type, attn_kwargs=attn_kwargs)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels // 2),\n",
    "            ReLU(),\n",
    "            Linear(channels // 2, channels // 4),\n",
    "            ReLU(),\n",
    "            Linear(channels // 4, 1),\n",
    "        )\n",
    "        self.redraw_projection = RedrawProjection(\n",
    "            self.convs,\n",
    "            redraw_interval=1000 if attn_type == 'performer' else None)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_emb(x.squeeze(-1))\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, batch, edge_attr=edge_attr)\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class RedrawProjection:\n",
    "    def __init__(self, model: torch.nn.Module,\n",
    "                 redraw_interval: Optional[int] = None):\n",
    "        self.model = model\n",
    "        self.redraw_interval = redraw_interval\n",
    "        self.num_last_redraw = 0\n",
    "\n",
    "    def redraw_projections(self):\n",
    "        if not self.model.training or self.redraw_interval is None:\n",
    "            return\n",
    "        if self.num_last_redraw >= self.redraw_interval:\n",
    "            fast_attentions = [\n",
    "                module for module in self.model.modules()\n",
    "                if isinstance(module, PerformerAttention)\n",
    "            ]\n",
    "            for fast_attention in fast_attentions:\n",
    "                fast_attention.redraw_projection_matrix()\n",
    "            self.num_last_redraw = 0\n",
    "            return\n",
    "        self.num_last_redraw += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "attn_kwargs = {'dropout': 0.5}\n",
    "model = GPS(channels=64, pe_dim=8, num_layers=10, attn_type='performer',\n",
    "            attn_kwargs=attn_kwargs).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.redraw_projection.redraw_projections()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x,  data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.7243, Val: 0.5609, Test: 0.5996\n",
      "Epoch: 02, Loss: 0.5899, Val: 0.5317, Test: 0.5483\n",
      "Epoch: 03, Loss: 0.5511, Val: 0.5443, Test: 0.5921\n",
      "Epoch: 04, Loss: 0.5264, Val: 0.4494, Test: 0.4924\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    loss = train()\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Graphormer: Do Transformers Really Perform Bad for Graph Representation\n",
    "<img src=\"image.png\" alt=\"Sample Image\" width=\"1000\" height=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Centrality Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "class CentralityEncoding(nn.Module):\n",
    "    def __init__(self, max_in_degree: int, max_out_degree: int, node_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the CentralityEncoding module.\n",
    "\n",
    "        :param max_in_degree: Maximum in-degree of nodes for encoding (limits the embedding size)\n",
    "        :param max_out_degree: Maximum out-degree of nodes for encoding (limits the embedding size)\n",
    "        :param node_dim: The dimensionality of node feature embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_in_degree = max_in_degree\n",
    "        self.max_out_degree = max_out_degree\n",
    "        self.node_dim = node_dim\n",
    "\n",
    "        # Learnable parameters for encoding based on in-degrees and out-degrees\n",
    "        # `z_in` encodes nodes based on in-degree; `z_out` encodes nodes based on out-degree\n",
    "        self.z_in = nn.Parameter(torch.randn((max_in_degree, node_dim)))\n",
    "        self.z_out = nn.Parameter(torch.randn((max_out_degree, node_dim)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to apply centrality encoding to node features.\n",
    "\n",
    "        :param x: Node feature matrix (num_nodes x node_dim)\n",
    "        :param edge_index: Edge index matrix (2 x num_edges) for adjacency representation\n",
    "        :return: torch.Tensor, node embeddings after centrality encoding\n",
    "        \"\"\"\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        # Calculate in-degrees and out-degrees of nodes using `edge_index`\n",
    "        # Limits the maximum in-degree and out-degree values to avoid exceeding embedding size\n",
    "\n",
    "        in_degree = self.decrease_to_max_value(degree(index=edge_index[1], num_nodes=num_nodes).long(),\n",
    "                                               self.max_in_degree - 1) \n",
    "        out_degree = self.decrease_to_max_value(degree(index=edge_index[0], num_nodes=num_nodes).long(),\n",
    "                                                self.max_out_degree - 1)\n",
    "\n",
    "        # Add in-degree and out-degree encodings to the node features\n",
    "        x += self.z_in[in_degree] + self.z_out[out_degree] \n",
    "\n",
    "        return x\n",
    "\n",
    "    def decrease_to_max_value(self, x, max_value):\n",
    "        \"\"\"\n",
    "        Limits the maximum value in tensor x to `max_value`.\n",
    "\n",
    "        :param x: Tensor with degree values (either in-degree or out-degree)\n",
    "        :param max_value: Maximum allowable value in x\n",
    "        :return: Modified tensor with values capped at max_value\n",
    "        \"\"\"\n",
    "        x[x > max_value] = max_value\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Spatial Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SpatialEncoding(nn.Module):\n",
    "    def __init__(self, max_path_distance: int):\n",
    "        \"\"\"\n",
    "        Initializes the SpatialEncoding module.\n",
    "\n",
    "        :param max_path_distance: Maximum pairwise distance between nodes to consider for encoding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_path_distance = max_path_distance\n",
    "\n",
    "        # Learnable parameter vector `b` for different path distances.\n",
    "        # It contains embeddings for each possible path length up to `max_path_distance`.\n",
    "        self.b = nn.Parameter(torch.randn(self.max_path_distance))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, paths) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the spatial encoding matrix based on pairwise node paths.\n",
    "\n",
    "        :param x: Node feature matrix of shape (num_nodes, node_dim).\n",
    "        :param paths: Dictionary containing pairwise node paths; paths[src][dst] gives the path from src to dst.\n",
    "        :return: Spatial encoding matrix of shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "        # Initialize the spatial encoding matrix with zeros.\n",
    "        # This matrix will store the spatial encoding value between each pair of nodes.\n",
    "        spatial_matrix = torch.zeros((x.shape[0], x.shape[0])).to(next(self.parameters()).device)  # (num_nodes, num_nodes)\n",
    "\n",
    "        # Iterate over all source nodes in the paths dictionary.\n",
    "        for src in paths:\n",
    "            # Iterate over all destination nodes reachable from the source node.\n",
    "            for dst in paths[src]:\n",
    "                # Calculate the length of the path from src to dst.\n",
    "                path_length = len(paths[src][dst])\n",
    "\n",
    "                # Cap the path length at `max_path_distance` to prevent indexing errors.\n",
    "                capped_length = min(path_length, self.max_path_distance)\n",
    "\n",
    "                # Subtract 1 because indexing starts at 0.\n",
    "                index = capped_length - 1\n",
    "\n",
    "                # Assign the corresponding embedding value from `b` to the spatial matrix entry.\n",
    "                spatial_matrix[src][dst] = self.b[index]\n",
    "\n",
    "        return spatial_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Edge Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class EdgeEncoding(nn.Module):\n",
    "    def __init__(self, edge_dim: int, max_path_distance: int):\n",
    "        \"\"\"\n",
    "        Initializes the EdgeEncoding module.\n",
    "\n",
    "        :param edge_dim: The dimensionality of edge feature embeddings.\n",
    "        :param max_path_distance: The maximum path distance to consider for encoding between nodes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.edge_dim = edge_dim\n",
    "        self.max_path_distance = max_path_distance\n",
    "        \n",
    "        # Learnable parameter `edge_vector` for encoding edges based on path distances.\n",
    "        # `edge_vector` has a shape of (max_path_distance, edge_dim) where each entry corresponds\n",
    "        # to an encoding for a specific path distance.\n",
    "        self.edge_vector = nn.Parameter(torch.randn(self.max_path_distance, self.edge_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_attr: torch.Tensor, edge_paths) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to apply edge encoding based on pairwise paths between nodes.\n",
    "\n",
    "        :param x: Node feature matrix (num_nodes x node_dim).\n",
    "        :param edge_attr: Edge feature matrix (num_edges x edge_dim).\n",
    "        :param edge_paths: Dictionary of paths between node pairs (based on edge indexes),\n",
    "                           where edge_paths[src][dst] contains the edges in the path from node `src` to `dst`.\n",
    "        :return: torch.Tensor, Edge Encoding matrix (num_nodes x num_nodes).\n",
    "        \"\"\"\n",
    "        # Initialize the edge encoding matrix `cij` with zeros.\n",
    "        cij = torch.zeros((x.shape[0], x.shape[0])).to(next(self.parameters()).device)\n",
    "\n",
    "        # Iterate over each source node in the edge_paths dictionary.\n",
    "        for src in edge_paths:\n",
    "            # For each destination node reachable from `src`, calculate an encoding based on path length.\n",
    "            for dst in edge_paths[src]:\n",
    "                # Retrieve the path between `src` and `dst` limited to `max_path_distance`.\n",
    "                path_ij = edge_paths[src][dst][:self.max_path_distance]\n",
    "                \n",
    "                # Generate indices based on the length of `path_ij`.\n",
    "                # `weight_inds` is a list of indices to select weights for each edge in `path_ij`.\n",
    "                weight_inds = [i for i in range(len(path_ij))]\n",
    "                \n",
    "                # Calculate the dot product between `edge_vector` and `edge_attr` for edges in `path_ij`.\n",
    "                # The mean is taken to create a single encoding value for the `src` to `dst` path.\n",
    "                cij[src][dst] = self.dot_product(self.edge_vector[weight_inds], edge_attr[path_ij]).mean()\n",
    "\n",
    "        # Replace any NaN values in the encoding matrix with zero.\n",
    "        cij = torch.nan_to_num(cij)\n",
    "\n",
    "        # Return the edge encoding matrix.\n",
    "        return cij\n",
    "\n",
    "    def dot_product(self, x1, x2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the dot product between two tensors along dimension 1.\n",
    "\n",
    "        :param x1: First tensor (subset of `edge_vector` for path distances).\n",
    "        :param x2: Second tensor (subset of `edge_attr` for edges in a path).\n",
    "        :return: Tensor of dot product results.\n",
    "        \"\"\"\n",
    "        return (x1 * x2).sum(dim=1)  # Element-wise multiplication followed by sum along `dim=1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Graphormer Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class GraphormerAttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int, edge_dim: int, max_path_distance: int):\n",
    "        \"\"\"\n",
    "        Initializes the GraphormerAttentionHead module.\n",
    "\n",
    "        :param dim_in: Input dimensionality of node features.\n",
    "        :param dim_q: Dimensionality for the query matrix.\n",
    "        :param dim_k: Dimensionality for the key and value matrices.\n",
    "        :param edge_dim: Dimensionality of edge features.\n",
    "        :param max_path_distance: Maximum path distance for encoding in EdgeEncoding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the EdgeEncoding module for edge-based encodings\n",
    "        self.edge_encoding = EdgeEncoding(edge_dim, max_path_distance)\n",
    "\n",
    "        # Linear layers to project node features into query, key, and value matrices\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                b: torch.Tensor,\n",
    "                edge_paths,\n",
    "                ptr=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for attention calculation.\n",
    "\n",
    "        :param x: Node feature matrix (num_nodes x dim_in).\n",
    "        :param edge_attr: Edge feature matrix (num_edges x edge_dim).\n",
    "        :param b: Spatial encoding matrix (num_nodes x num_nodes).\n",
    "        :param edge_paths: Dictionary of paths between node pairs, with paths based on edge indexes.\n",
    "        :param ptr: Optional batch pointer, specifying graph indices within batches of graphs.\n",
    "        :return: torch.Tensor, updated node embeddings after the attention operation.\n",
    "        \"\"\"\n",
    "        # Initialize batch masks:\n",
    "        # - batch_mask_neg_inf (with -inf values) for attention masking.\n",
    "        # - batch_mask_zeros (with zeros) for masking in softmax calculations.\n",
    "        batch_mask_neg_inf = torch.full(size=(x.shape[0], x.shape[0]), fill_value=-1e6).to(next(self.parameters()).device)\n",
    "        batch_mask_zeros = torch.zeros(size=(x.shape[0], x.shape[0])).to(next(self.parameters()).device)\n",
    "\n",
    "        # Check if `ptr` is None (single graph). If so, set all mask values to 1.\n",
    "        if type(ptr) == type(None):\n",
    "            batch_mask_neg_inf = torch.ones(size=(x.shape[0], x.shape[0])).to(next(self.parameters()).device)\n",
    "            batch_mask_zeros += 1\n",
    "        else:\n",
    "            # Otherwise, create batch masks based on the graph boundaries specified by `ptr`.\n",
    "            for i in range(len(ptr) - 1):\n",
    "                batch_mask_neg_inf[ptr[i]:ptr[i + 1], ptr[i]:ptr[i + 1]] = 1\n",
    "                batch_mask_zeros[ptr[i]:ptr[i + 1], ptr[i]:ptr[i + 1]] = 1\n",
    "\n",
    "        # Project node features `x` into query, key, and value matrices\n",
    "        query = self.q(x)\n",
    "        key = self.k(x)\n",
    "        value = self.v(x)\n",
    "\n",
    "        # Compute edge-based encoding using `EdgeEncoding`\n",
    "        c = self.edge_encoding(x, edge_attr, edge_paths)\n",
    "\n",
    "        # Calculate the attention scores by combining query and key\n",
    "        a = self.compute_a(key, query, ptr)\n",
    "\n",
    "        # Combine attention scores with spatial encoding `b` and edge encoding `c`, and apply masking\n",
    "        a = (a + b + c) * batch_mask_neg_inf\n",
    "\n",
    "        # Apply softmax to the attention scores along the last dimension and re-apply mask\n",
    "        softmax = torch.softmax(a, dim=-1) * batch_mask_zeros  # e^(-inf) results in 0\n",
    "\n",
    "        # Compute the new node embeddings by applying attention scores to the value matrix\n",
    "        x = softmax.mm(value)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_a(self, key, query, ptr=None):\n",
    "        \"\"\"\n",
    "        Computes the normalized dot-product attention between query and key.\n",
    "\n",
    "        :param key: Key matrix for nodes.\n",
    "        :param query: Query matrix for nodes.\n",
    "        :param ptr: Optional batch pointer specifying graph indices within batches of graphs.\n",
    "        :return: torch.Tensor, attention scores for the nodes.\n",
    "        \"\"\"\n",
    "        # If `ptr` is None (single graph), compute attention for all nodes in the graph\n",
    "        if type(ptr) == type(None):\n",
    "            a = query.mm(key.transpose(0, 1)) / query.size(-1) ** 0.5\n",
    "        else:\n",
    "            # Otherwise, compute attention scores for each graph separately in the batch\n",
    "            a = torch.zeros((query.shape[0], query.shape[0]), device=key.device)\n",
    "            for i in range(len(ptr) - 1):\n",
    "                a[ptr[i]:ptr[i + 1], ptr[i]:ptr[i + 1]] = query[ptr[i]:ptr[i + 1]].mm(\n",
    "                    key[ptr[i]:ptr[i + 1]].transpose(0, 1)) / query.size(-1) ** 0.5\n",
    "\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Graphormer Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphormerMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int, edge_dim: int, max_path_distance: int):\n",
    "        \"\"\"\n",
    "        :param num_heads: number of attention heads\n",
    "        :param dim_in: node feature matrix input number of dimension\n",
    "        :param dim_q: query node feature matrix input number dimension\n",
    "        :param dim_k: key node feature matrix input number of dimension\n",
    "        :param edge_dim: edge feature matrix number of dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [GraphormerAttentionHead(dim_in, dim_q, dim_k, edge_dim, max_path_distance) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                b: torch.Tensor,\n",
    "                edge_paths,\n",
    "                ptr) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param x: node feature matrix\n",
    "        :param edge_attr: edge feature matrix\n",
    "        :param b: spatial Encoding matrix\n",
    "        :param edge_paths: pairwise node paths in edge indexes\n",
    "        :param ptr: batch pointer that shows graph indexes in batch of graphs\n",
    "        :return: torch.Tensor, node embeddings after all attention heads\n",
    "        \"\"\"\n",
    "        return self.linear(\n",
    "            torch.cat([\n",
    "                attention_head(x, edge_attr, b, edge_paths, ptr) for attention_head in self.heads\n",
    "            ], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Graphormer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphormerEncoderLayer(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, num_heads, max_path_distance):\n",
    "        \"\"\"\n",
    "        :param node_dim: node feature matrix input number of dimension\n",
    "        :param edge_dim: edge feature matrix input number of dimension\n",
    "        :param num_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_dim = node_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = GraphormerMultiHeadAttention(\n",
    "            dim_in=node_dim,\n",
    "            dim_k=node_dim,\n",
    "            dim_q=node_dim,\n",
    "            num_heads=num_heads,\n",
    "            edge_dim=edge_dim,\n",
    "            max_path_distance=max_path_distance,\n",
    "        )\n",
    "        self.ln_1 = nn.LayerNorm(node_dim)\n",
    "        self.ln_2 = nn.LayerNorm(node_dim)\n",
    "        self.ff = nn.Linear(node_dim, node_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                edge_attr: torch.Tensor,\n",
    "                b: torch,\n",
    "                edge_paths,\n",
    "                ptr) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        h′(l) = MHA(LN(h(l−1))) + h(l−1)\n",
    "        h(l) = FFN(LN(h′(l))) + h′(l)\n",
    "\n",
    "        :param x: node feature matrix\n",
    "        :param edge_attr: edge feature matrix\n",
    "        :param b: spatial Encoding matrix\n",
    "        :param edge_paths: pairwise node paths in edge indexes\n",
    "        :param ptr: batch pointer that shows graph indexes in batch of graphs\n",
    "        :return: torch.Tensor, node embeddings after Graphormer layer operations\n",
    "        \"\"\"\n",
    "        x_prime = self.attention(self.ln_1(x), edge_attr, b, edge_paths, ptr) + x\n",
    "        x_new = self.ff(self.ln_2(x_prime)) + x_prime\n",
    "\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "def floyd_warshall_source_to_all(G, source, cutoff=None):\n",
    "    \"\"\"\n",
    "    Computes the shortest paths from a source node to all other nodes in graph `G`.\n",
    "    Uses a modified Floyd-Warshall algorithm for single-source shortest paths.\n",
    "\n",
    "    :param G: NetworkX graph.\n",
    "    :param source: Source node from which to calculate paths.\n",
    "    :param cutoff: Maximum path length to consider.\n",
    "    :return: Tuple of dictionaries, `node_paths` and `edge_paths`, where:\n",
    "             - `node_paths` contains the shortest path of nodes from source to each node.\n",
    "             - `edge_paths` contains the list of edges (by edge index) in each path.\n",
    "    \"\"\"\n",
    "    if source not in G:\n",
    "        raise nx.NodeNotFound(\"Source {} not in G\".format(source))\n",
    "\n",
    "    # Mapping each edge to a unique index for path tracking\n",
    "    edges = {edge: i for i, edge in enumerate(G.edges())}\n",
    "\n",
    "    level = 0  # The current level of traversal\n",
    "    nextlevel = {source: 1}  # Nodes to explore at the next level\n",
    "    node_paths = {source: [source]}  # Shortest paths (in nodes) from source to each reachable node\n",
    "    edge_paths = {source: []}  # Shortest paths (in edges) from source to each reachable node\n",
    "\n",
    "    # BFS to explore each level until no new nodes or cutoff level is reached\n",
    "    while nextlevel:\n",
    "        thislevel = nextlevel\n",
    "        nextlevel = {}\n",
    "        for v in thislevel:\n",
    "            # Iterate through each neighbor `w` of node `v`\n",
    "            for w in G[v]:\n",
    "                if w not in node_paths:  # Only consider unvisited nodes\n",
    "                    # Update node paths by appending `w` to path from source to `v`\n",
    "                    node_paths[w] = node_paths[v] + [w]\n",
    "                    # Append edge index to path from source to `w`\n",
    "                    edge_paths[w] = edge_paths[v] + [edges[tuple(node_paths[w][-2:])]]\n",
    "                    # Queue `w` for exploration in the next level\n",
    "                    nextlevel[w] = 1\n",
    "\n",
    "        level += 1\n",
    "\n",
    "        # Stop if the cutoff distance is reached\n",
    "        if (cutoff is not None and cutoff <= level):\n",
    "            break\n",
    "\n",
    "    return node_paths, edge_paths\n",
    "\n",
    "\n",
    "def all_pairs_shortest_path(G) -> Tuple[Dict[int, List[int]], Dict[int, List[int]]]:\n",
    "    \"\"\"\n",
    "    Computes shortest paths between all pairs of nodes in graph `G`.\n",
    "\n",
    "    :param G: NetworkX graph.\n",
    "    :return: Tuple of dictionaries `node_paths` and `edge_paths`:\n",
    "             - `node_paths` contains shortest paths of nodes between each pair.\n",
    "             - `edge_paths` contains shortest paths in terms of edge indices between each pair.\n",
    "    \"\"\"\n",
    "    # Compute shortest paths from each node to all other nodes\n",
    "    paths = {n: floyd_warshall_source_to_all(G, n) for n in G}\n",
    "    # Separate node and edge paths into separate dictionaries\n",
    "    node_paths = {n: paths[n][0] for n in paths}\n",
    "    edge_paths = {n: paths[n][1] for n in paths}\n",
    "    return node_paths, edge_paths\n",
    "\n",
    "\n",
    "def shortest_path_distance(data: Data) -> Tuple[Dict[int, List[int]], Dict[int, List[int]]]:\n",
    "    \"\"\"\n",
    "    Computes shortest paths between all pairs of nodes in a single PyTorch Geometric graph `data`.\n",
    "\n",
    "    :param data: PyTorch Geometric `Data` object representing the graph.\n",
    "    :return: Tuple of dictionaries `node_paths` and `edge_paths` containing shortest paths.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric data to a NetworkX graph\n",
    "    G = to_networkx(data)\n",
    "    # Compute all pairs shortest paths in terms of node and edge paths\n",
    "    node_paths, edge_paths = all_pairs_shortest_path(G)\n",
    "    return node_paths, edge_paths\n",
    "\n",
    "\n",
    "def batched_shortest_path_distance(data) -> Tuple[Dict[int, List[int]], Dict[int, List[int]]]:\n",
    "    \"\"\"\n",
    "    Computes shortest paths between all pairs of nodes for a batch of PyTorch Geometric graphs.\n",
    "\n",
    "    :param data: PyTorch Geometric batch object containing multiple graphs.\n",
    "    :return: Tuple of dictionaries `node_paths` and `edge_paths` for all graphs in the batch.\n",
    "    \"\"\"\n",
    "    # Convert each graph in the batch to a NetworkX graph and collect in a list\n",
    "    graphs = [to_networkx(sub_data) for sub_data in data.to_data_list()]\n",
    "    relabeled_graphs = []\n",
    "    shift = 0  # Track node ID shifts to ensure unique node IDs across graphs\n",
    "\n",
    "    # Relabel nodes in each graph to ensure unique node IDs across all graphs in the batch\n",
    "    for i in range(len(graphs)):\n",
    "        num_nodes = graphs[i].number_of_nodes()\n",
    "        # Shift the node IDs of the current graph by `shift`\n",
    "        relabeled_graphs.append(nx.relabel_nodes(graphs[i], {i: i + shift for i in range(num_nodes)}))\n",
    "        shift += num_nodes\n",
    "\n",
    "    # Compute all pairs shortest paths for each relabeled graph in the batch\n",
    "    paths = [all_pairs_shortest_path(G) for G in relabeled_graphs]\n",
    "    node_paths = {}\n",
    "    edge_paths = {}\n",
    "\n",
    "    # Aggregate node and edge paths for each graph into dictionaries\n",
    "    for path in paths:\n",
    "        # Update node_paths and edge_paths with results from each graph\n",
    "        for k, v in path[0].items():\n",
    "            node_paths[k] = v\n",
    "        for k, v in path[1].items():\n",
    "            edge_paths[k] = v\n",
    "\n",
    "    return node_paths, edge_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphormer(nn.Module):\n",
    "    def __init__(self, config, num_node_features, num_edge_features):\n",
    "        \"\"\"\n",
    "        :param config: dictionary of configuration parameters\n",
    "        :param num_node_features: number of node features\n",
    "        :param num_edge_features: number of edge features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.input_node_dim = num_node_features\n",
    "        self.node_dim = config['node_dim']\n",
    "        self.input_edge_dim = num_edge_features\n",
    "        self.edge_dim = config['edge_dim']\n",
    "        self.output_dim = config['output_dim']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.max_in_degree = config['max_in_degree']\n",
    "        self.max_out_degree = config['max_out_degree']\n",
    "        self.max_path_distance = config['max_path_distance']\n",
    "\n",
    "        self.node_in_lin = nn.Linear(self.input_node_dim, self.node_dim)\n",
    "        self.edge_in_lin = nn.Linear(self.input_edge_dim, self.edge_dim)\n",
    "\n",
    "        self.centrality_encoding = CentralityEncoding(\n",
    "            max_in_degree=self.max_in_degree,\n",
    "            max_out_degree=self.max_out_degree,\n",
    "            node_dim=self.node_dim\n",
    "        )\n",
    "\n",
    "        self.spatial_encoding = SpatialEncoding(\n",
    "            max_path_distance=self.max_path_distance,\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphormerEncoderLayer(\n",
    "                node_dim=self.node_dim,\n",
    "                edge_dim=self.edge_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                max_path_distance=self.max_path_distance) for _ in range(self.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.node_out_lin = nn.Linear(self.node_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x.float()\n",
    "        edge_index = data.edge_index.long()\n",
    "        edge_attr = data.edge_attr.float()\n",
    "\n",
    "        if isinstance(data, Data):\n",
    "            ptr = None\n",
    "            node_paths, edge_paths = shortest_path_distance(data)\n",
    "        else:\n",
    "            ptr = data.ptr\n",
    "            node_paths, edge_paths = batched_shortest_path_distance(data)\n",
    "\n",
    "        x = self.node_in_lin(x)\n",
    "        edge_attr = self.edge_in_lin(edge_attr)\n",
    "\n",
    "        x = self.centrality_encoding(x, edge_index)\n",
    "        b = self.spatial_encoding(x, node_paths)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_attr, b, edge_paths, ptr)\n",
    "\n",
    "        x = self.node_out_lin(x)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Load MoleculeNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_ESOL():\n",
    "    dataset = MoleculeNet(root='Data/MoleculeNet', name='ESOL')\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader, dataset.num_node_features, dataset.num_edge_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2024.3.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from rdkit) (10.3.0)\n",
      "Downloading rdkit-2024.3.5-cp312-cp312-manylinux_2_28_x86_64.whl (33.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2024.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, r2\n",
    "\n",
    "def train(model, train_loader, device, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        data = data.to(device)  # Move data batch to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, data.y.to(device))  # Ensure target is also on GPU\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        y_true.extend(data.y.cpu().numpy())\n",
    "        y_pred.extend(outputs.cpu().detach().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    mae, mse, r2 = calculate_metrics(y_true, y_pred)\n",
    "    print(f\"Train Loss: {avg_loss:.4f}, MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "def test(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc=\"Testing\"):\n",
    "            data = data.to(device)  # Move data batch to GPU\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y.to(device))  # Ensure target is also on GPU\n",
    "            total_loss += loss.item()\n",
    "            y_true.extend(data.y.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    mae, mse, r2 = calculate_metrics(y_true, y_pred)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Set hyperparameter and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Training: 100%|██████████| 15/15 [10:07<00:00, 40.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0286, MAE: 1.7572, MSE: 5.0952, R2: -0.1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 4/4 [00:42<00:00, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0305, MAE: 1.7575, MSE: 4.4930, R2: 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"num_layers\": 2,\n",
    "    \"node_dim\": 128,\n",
    "    \"edge_dim\": 128,\n",
    "    \"output_dim\": 1,\n",
    "    \"num_heads\": 4,\n",
    "    \"max_in_degree\": 5,\n",
    "    \"max_out_degree\": 5,\n",
    "    \"max_path_distance\": 5\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load data and initialize model\n",
    "train_loader, test_loader, num_node_features, num_edge_features = load_ESOL()\n",
    "model = Graphormer(config, num_node_features, num_edge_features).to(device)\n",
    "criterion = nn.L1Loss().to(device)  # Move criterion to GPU\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training and testing\n",
    "train(model, train_loader, device, criterion, optimizer)\n",
    "test(model, test_loader, device, criterion)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
