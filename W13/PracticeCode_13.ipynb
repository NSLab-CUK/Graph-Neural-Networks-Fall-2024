{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e04b823-c673-4699-b05a-77dce606c5e8",
   "metadata": {},
   "source": [
    "# 1. Modeling Relational Data with Graph Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb9ea5-6478-47a7-a117-2c4ac9bbe76e",
   "metadata": {},
   "source": [
    "### 1.1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764a7171-38d9-4ba7-825a-b46e5bdf327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /home/user/anaconda3/envs/GNN2/lib/python3.9/site-packages (7.1.1)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /home/user/anaconda3/envs/GNN2/lib/python3.9/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /home/user/anaconda3/envs/GNN2/lib/python3.9/site-packages (from rdflib) (3.1.2)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Entities\n",
    "# from torch_geometric.nn import FastRGCNConv, RGCNConv\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "!pip install rdflib\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=str, default='AIFB',choices=\n",
    "dataset = 'AIFB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b04aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for typing and PyTorch geometric functionality\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# Import backend utilities and components for geometric operations\n",
    "import torch_geometric.backend\n",
    "import torch_geometric.typing\n",
    "from torch_geometric import is_compiling\n",
    "from torch_geometric.index import index2ptr\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.typing import (\n",
    "    Adj,  # Type alias for adjacency representation\n",
    "    OptTensor,  # Optional tensor type alias\n",
    "    SparseTensor,  # Sparse tensor type alias\n",
    "    pyg_lib,  # Library for advanced geometric operations\n",
    "    torch_sparse,  # Support for sparse tensor operations\n",
    ")\n",
    "from torch_geometric.utils import index_sort, one_hot, scatter, spmm\n",
    "\n",
    "# Define a utility function to apply a mask to edge indices\n",
    "def masked_edge_index(edge_index: Adj, edge_mask: Tensor) -> Adj:\n",
    "    \"\"\"\n",
    "    Filters the edge index based on the provided edge mask.\n",
    "    Supports both dense and sparse adjacency formats.\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]  # Mask edges for dense adjacency\n",
    "    return torch_sparse.masked_select_nnz(edge_index, edge_mask, layout='coo')  # For sparse adjacency\n",
    "\n",
    "# Define a Relational Graph Convolutional Network (RGCN) convolution layer\n",
    "class RGCNConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],  # Input channels for source and target nodes\n",
    "        out_channels: int,  # Number of output channels\n",
    "        num_relations: int,  # Number of distinct edge types (relations)\n",
    "        num_bases: Optional[int] = None,  # Number of bases for weight decomposition (optional)\n",
    "        num_blocks: Optional[int] = None,  # Number of blocks for weight decomposition (optional)\n",
    "        aggr: str = 'mean',  # Aggregation strategy for message passing\n",
    "        root_weight: bool = True,  # Whether to use a root node weight\n",
    "        is_sorted: bool = False,  # Whether edge indices are sorted\n",
    "        bias: bool = True,  # Whether to use a bias parameter\n",
    "        **kwargs,  # Additional arguments for the parent class\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', aggr)  # Set default aggregation if not provided\n",
    "        super().__init__(node_dim=0, **kwargs)  # Initialize the base MessagePassing class\n",
    "\n",
    "        # Validate input parameters to ensure no conflicting decomposition strategies\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        # Save initialization parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "        self.is_sorted = is_sorted\n",
    "\n",
    "        # Support for tuple input format\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        # Attribute for segment matmul heuristic during runtime\n",
    "        self._use_segment_matmul_heuristic_output: torch.jit.Attribute(\n",
    "            None, Optional[float])\n",
    "\n",
    "        # Initialize weights based on decomposition strategy\n",
    "        if num_bases is not None:  # Basis-decomposition\n",
    "            self.weight = Parameter(\n",
    "                torch.empty(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.empty(num_relations, num_bases))\n",
    "\n",
    "        elif num_blocks is not None:  # Block-diagonal-decomposition\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.empty(num_relations, num_blocks,\n",
    "                            in_channels[0] // num_blocks,\n",
    "                            out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)  # No composition matrix\n",
    "\n",
    "        else:  # Default dense weight\n",
    "            self.weight = Parameter(\n",
    "                torch.empty(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        # Root node weight initialization\n",
    "        if root_weight:\n",
    "            self.root = Parameter(torch.empty(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        # Bias initialization\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()  # Reset weights and biases\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets all learnable parameters using standard initialization techniques.\n",
    "        \"\"\"\n",
    "        super().reset_parameters()\n",
    "        glorot(self.weight)  # Glorot initialization for weights\n",
    "        glorot(self.comp)  # Glorot initialization for composition matrix\n",
    "        glorot(self.root)  # Glorot initialization for root weights\n",
    "        zeros(self.bias)  # Zero initialization for biases\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None):\n",
    "        \"\"\"\n",
    "        Performs a forward pass on the input data using relational graph convolution.\n",
    "        \"\"\"\n",
    "        # Prepare input features for source and target nodes\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:  # Fallback to default indices\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l  # Target node features\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))  # Define input-output sizes\n",
    "\n",
    "        # For sparse edge indices, extract edge types from storage\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # Initialize output tensor\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        # Adjust weights based on decomposition strategy\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition\n",
    "            # Ensure compatibility with non-floating-point input\n",
    "            if not torch.is_floating_point(x_r):\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            # Process each relation individually\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, edge_type_ptr=None, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])  # Matrix multiplication\n",
    "                out = out + h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # Default decomposition\n",
    "            ...\n",
    "        # Other processing omitted for brevity\n",
    "        return out\n",
    "\n",
    "# Additional methods (`message`, `message_and_aggregate`, etc.) omitted for brevity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d750eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRGCNConv(RGCNConv):\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None):\n",
    "\n",
    "        self.fuse = False\n",
    "        assert self.aggr in ['add', 'sum', 'mean']\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_type: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x_l, edge_type=edge_type, size=size)\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            if not torch.is_floating_point(x_r):\n",
    "                out = out + root[x_r]\n",
    "            else:\n",
    "                out = out + x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_type: Tensor,\n",
    "                edge_index_j: Tensor) -> Tensor:\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =======\n",
    "            if not torch.is_floating_point(x_j):\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            weight = weight[edge_type].view(-1, weight.size(2), weight.size(3))\n",
    "            x_j = x_j.view(-1, 1, weight.size(1))\n",
    "            return torch.bmm(x_j, weight).view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            if not torch.is_floating_point(x_j):\n",
    "                weight_index = edge_type * weight.size(1) + edge_index_j\n",
    "                return weight.view(-1, self.out_channels)[weight_index]\n",
    "\n",
    "            return torch.bmm(x_j.unsqueeze(-2), weight[edge_type]).squeeze(-2)\n",
    "\n",
    "    def aggregate(self, inputs: Tensor, edge_type: Tensor, index: Tensor,\n",
    "                  dim_size: Optional[int] = None) -> Tensor:\n",
    "\n",
    "        # Compute normalization in separation for each `edge_type`.\n",
    "        if self.aggr == 'mean':\n",
    "            norm = one_hot(edge_type, self.num_relations, dtype=inputs.dtype)\n",
    "            norm = scatter(norm, index, dim=0, dim_size=dim_size)[index]\n",
    "            norm = torch.gather(norm, 1, edge_type.view(-1, 1))\n",
    "            norm = 1. / norm.clamp_(1.)\n",
    "            inputs = norm * inputs\n",
    "\n",
    "        return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9805b79-6d2d-437f-ac36-9e65350f59ee",
   "metadata": {},
   "source": [
    "### 1.2. Trade memory consumption for faster computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83f868f5-ec7f-498b-ac14-66dd0eb60150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "if dataset in ['AIFB', 'MUTAG']:\n",
    "    Conv = FastRGCNConv\n",
    "else:\n",
    "    Conv = RGCNConv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d03db-c00e-4db1-a351-8126466223b0",
   "metadata": {},
   "source": [
    "### 1.3. Dowload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d0c77b2-366c-4c96-81cd-8fc64d716f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join('data', 'Entities')\n",
    "dataset = Entities(path, dataset)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37778971-a22c-42bb-bdf3-6a734a94dfee",
   "metadata": {},
   "source": [
    "### 1.4. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e40967a-d093-4904-a0e1-3ac329c52773",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx = torch.cat([data.train_idx, data.test_idx], dim=0)\n",
    "node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "    node_idx, 2, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "data.num_nodes = node_idx.size(0)\n",
    "data.edge_index = edge_index\n",
    "data.edge_type = data.edge_type[edge_mask]\n",
    "data.train_idx = mapping[:data.train_idx.size(0)]\n",
    "data.test_idx = mapping[data.train_idx.size(0):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1ce78-9762-41c8-9185-5dc9e5fef026",
   "metadata": {},
   "source": [
    "### 1.5. Build rGCN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6a11765-e509-404c-b940-1cb519e21819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(data.num_nodes, 16, dataset.num_relations,\n",
    "                          num_bases=30)\n",
    "        self.conv2 = Conv(16, dataset.num_classes, dataset.num_relations,\n",
    "                          num_bases=30)\n",
    "\n",
    "    def forward(self, edge_index, edge_type):\n",
    "        x = F.relu(self.conv1(None, edge_index, edge_type))\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e59e96f8-fe23-4580-9785-ebbbb2866881",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5920bb04-26b9-45a0-b56c-c298a66eb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') if dataset == 'AM' else device\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c4133-8320-4cb6-8eea-afd042178ed3",
   "metadata": {},
   "source": [
    "### 1.6. Define the train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86bcc9dc-d4d8-4b52-a59e-3306fedc4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx], data.train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "    train_acc = float((pred[data.train_idx] == data.train_y).float().mean())\n",
    "    test_acc = float((pred[data.test_idx] == data.test_y).float().mean())\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0292cb94-3213-4c02-b4f4-19e4dd300f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.3997, Train: 0.9643 Test: 0.8333\n",
      "Epoch: 02, Loss: 0.7475, Train: 0.9643 Test: 0.8889\n",
      "Epoch: 03, Loss: 0.2929, Train: 0.9714 Test: 0.9167\n",
      "Epoch: 04, Loss: 0.1242, Train: 0.9714 Test: 0.9167\n",
      "Epoch: 05, Loss: 0.0789, Train: 0.9714 Test: 0.9167\n",
      "Epoch: 06, Loss: 0.0565, Train: 0.9786 Test: 0.9444\n",
      "Epoch: 07, Loss: 0.0342, Train: 0.9929 Test: 0.9444\n",
      "Epoch: 08, Loss: 0.0166, Train: 1.0000 Test: 0.9444\n",
      "Epoch: 09, Loss: 0.0074, Train: 1.0000 Test: 0.9444\n",
      "Median time per epoch: 0.0053s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for epoch in range(1, 10):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "          f'Test: {test_acc:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88613d-3455-41e3-9f66-3769a257be49",
   "metadata": {},
   "source": [
    "## 2. Translating Embeddings for Modeling Multi-relational Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07c28b",
   "metadata": {},
   "source": [
    "### 2.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac347427-208f-4a47-9326-ffc7036d3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "from torch_geometric.nn import RotatE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035518f",
   "metadata": {},
   "source": [
    "### 2.2. TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b4e69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.nn.kge import KGEModel\n",
    "\n",
    "\n",
    "class TransE(KGEModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        num_relations: int,\n",
    "        hidden_channels: int,\n",
    "        margin: float = 1.0,\n",
    "        p_norm: float = 1.0,\n",
    "        sparse: bool = False,\n",
    "    ):\n",
    "        super().__init__(num_nodes, num_relations, hidden_channels, sparse)\n",
    "\n",
    "        self.p_norm = p_norm\n",
    "        self.margin = margin\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 6. / math.sqrt(self.hidden_channels)\n",
    "        torch.nn.init.uniform_(self.node_emb.weight, -bound, bound)\n",
    "        torch.nn.init.uniform_(self.rel_emb.weight, -bound, bound)\n",
    "        F.normalize(self.rel_emb.weight.data, p=self.p_norm, dim=-1,\n",
    "                    out=self.rel_emb.weight.data)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        head_index: Tensor,\n",
    "        rel_type: Tensor,\n",
    "        tail_index: Tensor,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        head = self.node_emb(head_index)\n",
    "        rel = self.rel_emb(rel_type)\n",
    "        tail = self.node_emb(tail_index)\n",
    "\n",
    "        head = F.normalize(head, p=self.p_norm, dim=-1)\n",
    "        tail = F.normalize(tail, p=self.p_norm, dim=-1)\n",
    "\n",
    "        # Calculate *negative* TransE norm:\n",
    "        return -((head + rel) - tail).norm(p=self.p_norm, dim=-1)\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        head_index: Tensor,\n",
    "        rel_type: Tensor,\n",
    "        tail_index: Tensor,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        pos_score = self(head_index, rel_type, tail_index)\n",
    "        neg_score = self(*self.random_sample(head_index, rel_type, tail_index))\n",
    "\n",
    "        return F.margin_ranking_loss(\n",
    "            pos_score,\n",
    "            neg_score,\n",
    "            target=torch.ones_like(pos_score),\n",
    "            margin=self.margin,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da38fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    'transe': TransE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1a0eac1-f137-4024-a4d9-a24f24f9e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', choices=model_map.keys(), type=str.lower, required=False)\n",
    "# Add this to avoid issues with Jupyter Notebook's internal arguments\n",
    "parser.add_argument(\"-f\", required=False, help=\"Dummy argument to prevent Jupyter errors.\")\n",
    "args = parser.parse_args([])  # Passing an empty list ensures no CLI arguments are processed.\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = osp.join('data', 'FB15k')\n",
    "\n",
    "train_data = FB15k_237(path, split='train')[0].to(device)\n",
    "val_data = FB15k_237(path, split='val')[0].to(device)\n",
    "test_data = FB15k_237(path, split='test')[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3686fd-e5e7-481e-bd42-ea416b009842",
   "metadata": {},
   "source": [
    "### 2.3. TransE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16774cb3-d6ca-4b9f-9a7d-030ac69b4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg_map = {'rotate': {'margin': 9.0}}\n",
    "args.model = \"transe\"\n",
    "model = model_map[args.model](\n",
    "    num_nodes=train_data.num_nodes,\n",
    "    num_relations=train_data.num_edge_types,\n",
    "    hidden_channels=50,\n",
    "    **model_arg_map.get(args.model, {}),\n",
    ").to(device)\n",
    "\n",
    "loader = model.loader(\n",
    "    head_index=train_data.edge_index[0],\n",
    "    rel_type=train_data.edge_type,\n",
    "    tail_index=train_data.edge_index[1],\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b496a64",
   "metadata": {},
   "source": [
    "### 2.4. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9671f-da89-41cc-806e-f2c7ddc70a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_map = {\n",
    "    'transe': optim.Adam(model.parameters(), lr=0.01),\n",
    "    'rotate': optim.Adam(model.parameters(), lr=1e-3),\n",
    "}\n",
    "optimizer = optimizer_map[args.model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e1ee0",
   "metadata": {},
   "source": [
    "### 2.5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587e7f5-d23e-4eac-aef1-bc80a416cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for head_index, rel_type, tail_index in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head_index, rel_type, tail_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bab4e",
   "metadata": {},
   "source": [
    "### 2.6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df896099-bc62-4fd1-b2d0-2a314decf19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7630\n",
      "Epoch: 002, Loss: 0.5538\n",
      "Epoch: 003, Loss: 0.4318\n",
      "Epoch: 004, Loss: 0.3476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20466/20466 [00:05<00:00, 3490.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Rank: 812.74, Test MRR: 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    return model.test(\n",
    "        head_index=data.edge_index[0],\n",
    "        rel_type=data.edge_type,\n",
    "        tail_index=data.edge_index[1],\n",
    "        batch_size=20000,\n",
    "        k=10,\n",
    "    )\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    if epoch % 25 == 0:\n",
    "        rank, mrr, _ = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}, Val MRR: {mrr:.4f}')\n",
    "\n",
    "# Run\n",
    "rank, mrr, _ = test(test_data)\n",
    "print(f'Test Mean Rank: {rank:.2f}, Test MRR: {mrr:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79f81d-0c16-4691-beb2-d60740d60dd6",
   "metadata": {},
   "source": [
    "## 3. Learning Entity and Relation Embeddings for Knowledge Graph Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb26504",
   "metadata": {},
   "source": [
    "### 3.1. TransR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74f04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the TransR class, a relation-specific embedding model for knowledge graphs\n",
    "class TransR(nn.Module):\n",
    "    def __init__(self, num_nodes, num_rels, hidden_channels, rel_channels, p=1):\n",
    "        \"\"\"\n",
    "        Initializes the TransR model.\n",
    "        \n",
    "        Args:\n",
    "        - num_nodes: Number of entities in the knowledge graph.\n",
    "        - num_rels: Number of relation types in the knowledge graph.\n",
    "        - hidden_channels: Dimensionality of entity embeddings.\n",
    "        - rel_channels: Dimensionality of relation embeddings.\n",
    "        - p: Norm degree for computing the score (default is 1 for Manhattan norm).\n",
    "        \"\"\"\n",
    "        super(TransR, self).__init__()\n",
    "        \n",
    "        # Embedding layer for nodes (entities)\n",
    "        self.node_emb = nn.Embedding(num_nodes, hidden_channels)\n",
    "        \n",
    "        # Embedding layer for relations\n",
    "        self.rel_emb = nn.Embedding(num_rels, rel_channels)\n",
    "        \n",
    "        # Relation-specific projection matrix embeddings\n",
    "        self.rel_project = nn.Embedding(num_rels, hidden_channels * rel_channels)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.rel_channels = rel_channels\n",
    "        self.p = p\n",
    "\n",
    "        # Initialize model parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializes the embeddings using a uniform distribution with bounds\n",
    "        proportional to the size of the hidden channels.\n",
    "        \"\"\"\n",
    "        bound = 6. / math.sqrt(self.hidden_channels)  # Calculate bound\n",
    "        torch.nn.init.uniform_(self.node_emb.weight, -bound, bound)  # Initialize node embeddings\n",
    "        torch.nn.init.uniform_(self.rel_emb.weight, -bound, bound)  # Initialize relation embeddings\n",
    "        torch.nn.init.uniform_(self.rel_project.weight, -bound, bound)  # Initialize projection matrices\n",
    "\n",
    "    def forward(self, head_index, rel_type, tail_index):\n",
    "        \"\"\"\n",
    "        Computes the score for a triple (head, relation, tail).\n",
    "        \n",
    "        Args:\n",
    "        - head_index: Indices of head entities.\n",
    "        - rel_type: Indices of relations.\n",
    "        - tail_index: Indices of tail entities.\n",
    "        \n",
    "        Returns:\n",
    "        - Negative score for the triple, based on the TransR scoring function.\n",
    "        \"\"\"\n",
    "        # Look up embeddings for the head, relation, and tail\n",
    "        head = self.node_emb(head_index)\n",
    "        rel = self.rel_emb(rel_type)\n",
    "        proj_rel = self.rel_project(rel_type).view(-1, self.hidden_channels, self.rel_channels)  # Reshape relation projection matrix\n",
    "        tail = self.node_emb(tail_index)\n",
    "\n",
    "        # Project the head and tail embeddings to the relation-specific space\n",
    "        head_proj = (head.unsqueeze(1) @ proj_rel).squeeze(1)  # Relation-specific projection for head\n",
    "        tail_proj = (tail.unsqueeze(1) @ proj_rel).squeeze(1)  # Relation-specific projection for tail\n",
    "\n",
    "        # Compute the negative score using the p-norm\n",
    "        return -torch.norm(head_proj + rel - tail_proj, p=self.p, dim=-1)\n",
    "\n",
    "    def loss(self, head_index, rel_type, tail_index):\n",
    "        \"\"\"\n",
    "        Computes the margin ranking loss for a batch of triples.\n",
    "        \n",
    "        Args:\n",
    "        - head_index: Indices of head entities.\n",
    "        - rel_type: Indices of relations.\n",
    "        - tail_index: Indices of tail entities.\n",
    "        \n",
    "        Returns:\n",
    "        - Margin ranking loss for the batch.\n",
    "        \"\"\"\n",
    "        # Compute positive scores for given triples\n",
    "        pos_score = self(head_index, rel_type, tail_index)\n",
    "        \n",
    "        # Compute negative scores for randomly sampled negative triples\n",
    "        neg_score = self(*self.random_sample(head_index, rel_type, tail_index))\n",
    "\n",
    "        # Compute and return margin ranking loss\n",
    "        return F.margin_ranking_loss(\n",
    "            pos_score,  # Positive scores\n",
    "            neg_score,  # Negative scores\n",
    "            target=torch.ones_like(pos_score),  # Target indicating positive > negative\n",
    "            margin=1.0,  # Margin for the loss\n",
    "        )\n",
    "\n",
    "    def random_sample(self, head_index, rel_type, tail_index):\n",
    "        \"\"\"\n",
    "        Generates negative samples by corrupting the tail entity.\n",
    "        \n",
    "        Args:\n",
    "        - head_index: Indices of head entities.\n",
    "        - rel_type: Indices of relations.\n",
    "        - tail_index: Indices of tail entities.\n",
    "        \n",
    "        Returns:\n",
    "        - A corrupted triple (head, relation, negative tail).\n",
    "        \"\"\"\n",
    "        # Replace the tail index with random indices to create negative samples\n",
    "        neg_tail_index = torch.randint(0, self.node_emb.num_embeddings, tail_index.size(), device=tail_index.device)\n",
    "        return head_index, rel_type, neg_tail_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "836f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    'transe': TransE,\n",
    "    'transr': TransR,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce9203",
   "metadata": {},
   "source": [
    "### 3.2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "474d0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', choices=model_map.keys(), type=str.lower, required=False)\n",
    "# Add this to avoid issues with Jupyter Notebook's internal arguments\n",
    "parser.add_argument(\"-f\", required=False, help=\"Dummy argument to prevent Jupyter errors.\")\n",
    "args = parser.parse_args([])  # Passing an empty list ensures no CLI arguments are processed.\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = osp.join('data', 'FB15k')\n",
    "\n",
    "train_data = FB15k_237(path, split='train')[0].to(device)\n",
    "val_data = FB15k_237(path, split='val')[0].to(device)\n",
    "test_data = FB15k_237(path, split='test')[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c435e",
   "metadata": {},
   "source": [
    "### 3.3. Create Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1effc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_loader(head_index, rel_type, tail_index, batch_size, shuffle):\n",
    "    dataset = TensorDataset(head_index, rel_type, tail_index)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "618b00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg_map = {'rotate': {'margin': 9.0}}\n",
    "args.model = \"transr\"\n",
    "model = model_map[args.model](\n",
    "    num_nodes=train_data.num_nodes,\n",
    "    num_rels=train_data.num_edge_types,\n",
    "    hidden_channels=50,\n",
    "    rel_channels=30,\n",
    "    p=2,\n",
    ").to(device)\n",
    "\n",
    "loader = create_loader(\n",
    "    head_index=train_data.edge_index[0],\n",
    "    rel_type=train_data.edge_type,\n",
    "    tail_index=train_data.edge_index[1],\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f9e2f",
   "metadata": {},
   "source": [
    "### 3.4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97a4cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.5354\n",
      "Epoch: 002, Loss: 1.5340\n",
      "Epoch: 003, Loss: 1.5325\n",
      "Epoch: 004, Loss: 1.5339\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for head_index, rel_type, tail_index in loader:\n",
    "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head_index, rel_type, tail_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * head_index.size(0)\n",
    "        total_examples += head_index.size(0)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}')\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    loader = create_loader(\n",
    "        head_index=data.edge_index[0],\n",
    "        rel_type=data.edge_type,\n",
    "        tail_index=data.edge_index[1],\n",
    "        batch_size=20000,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    total_rank = total_mrr = total_examples = 0\n",
    "    for head_index, rel_type, tail_index in loader:\n",
    "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
    "        rank, mrr, _ = model.test(head_index, rel_type, tail_index, k=10)\n",
    "        total_rank += rank\n",
    "        total_mrr += mrr\n",
    "        total_examples += head_index.size(0)\n",
    "    return total_rank / total_examples, total_mrr / total_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68992277",
   "metadata": {},
   "source": [
    "### 3.5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28477c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, k=10):\n",
    "    model.eval()\n",
    "    loader = create_loader(\n",
    "        head_index=data.edge_index[0],\n",
    "        rel_type=data.edge_type,\n",
    "        tail_index=data.edge_index[1],\n",
    "        batch_size=20000,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    ranks = []\n",
    "    mrr = 0.0\n",
    "    for head_index, rel_type, tail_index in loader:\n",
    "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
    "\n",
    "        # Scores for true triplets\n",
    "        true_scores = model(head_index, rel_type, tail_index)\n",
    "\n",
    "        # Evaluate corrupted triplets (e.g., corrupt tail entity)\n",
    "        all_entities = torch.arange(model.node_emb.num_embeddings, device=device)\n",
    "        corrupt_tail_scores = []\n",
    "        for tail in all_entities:\n",
    "            corrupt_scores = model(head_index, rel_type, tail.repeat(head_index.size(0)))\n",
    "            corrupt_tail_scores.append(corrupt_scores.unsqueeze(-1))\n",
    "        corrupt_tail_scores = torch.cat(corrupt_tail_scores, dim=-1)\n",
    "\n",
    "        # Rank the true triplets among corrupted triplets\n",
    "        for i, true_score in enumerate(true_scores):\n",
    "            all_scores = torch.cat([true_score.unsqueeze(0), corrupt_tail_scores[i]])\n",
    "            rank = (all_scores > true_score).sum().item() + 1  # Rank starts from 1\n",
    "            ranks.append(rank)\n",
    "            mrr += 1 / rank\n",
    "\n",
    "    mean_rank = sum(ranks) / len(ranks)\n",
    "    mean_reciprocal_rank = mrr / len(ranks)\n",
    "    return mean_rank, mean_reciprocal_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74c7a8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Rank: 7060.62, Test MRR: 0.0051\n"
     ]
    }
   ],
   "source": [
    "rank, mrr = test(model, test_data, k=10)\n",
    "print(f'Test Mean Rank: {rank:.2f}, Test MRR: {mrr:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99291df",
   "metadata": {},
   "source": [
    "## 4. Knowledge Graph Embedding by Translating on Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b2bdf",
   "metadata": {},
   "source": [
    "### 4.1. TransH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bf9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import commonly used modules for file paths, PyTorch tensors, and neural networks\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a PyTorch model class for the TransH algorithm\n",
    "class TransH(nn.Module):\n",
    "    def __init__(self, num_nodes, num_rels, embedding_dim):\n",
    "        # Initialize the parent nn.Module class\n",
    "        super(TransH, self).__init__()\n",
    "        \n",
    "        # Define entity and relation embeddings\n",
    "        self.entity_emb = nn.Embedding(num_nodes, embedding_dim)  # Embeddings for entities\n",
    "        self.relation_emb = nn.Embedding(num_rels, embedding_dim)  # Embeddings for relations\n",
    "        self.relation_norm = nn.Embedding(num_rels, embedding_dim)  # Embedding for relation-specific normal vectors\n",
    "\n",
    "        # Initialize embedding parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize embeddings with Xavier uniform distribution\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_norm.weight)\n",
    "\n",
    "    def forward(self, head_idx, rel_idx, tail_idx):\n",
    "        # Fetch embeddings for the head entity, relation, and tail entity\n",
    "        head = self.entity_emb(head_idx)  # Head entity embeddings\n",
    "        rel = self.relation_emb(rel_idx)  # Relation embeddings\n",
    "        norm = self.relation_norm(rel_idx)  # Relation-specific normal vector embeddings\n",
    "        tail = self.entity_emb(tail_idx)  # Tail entity embeddings\n",
    "\n",
    "        # Normalize the normal vector for the hyperplane (L2 norm)\n",
    "        norm = F.normalize(norm, p=2, dim=-1)\n",
    "\n",
    "        # Project head and tail embeddings onto the relation-specific hyperplane\n",
    "        head_proj = head - (head * norm).sum(dim=-1, keepdim=True) * norm\n",
    "        tail_proj = tail - (tail * norm).sum(dim=-1, keepdim=True) * norm\n",
    "\n",
    "        # Compute the score for the triple (head, relation, tail)\n",
    "        score = head_proj + rel - tail_proj\n",
    "        return -torch.norm(score, p=2, dim=-1)  # Return the negative L2 norm of the score\n",
    "\n",
    "    def loss(self, head_idx, rel_idx, tail_idx, neg_tail_idx, margin=1.0):\n",
    "        # Calculate the positive and negative scores for the triples\n",
    "        pos_score = self(head_idx, rel_idx, tail_idx)  # Positive triple score\n",
    "        neg_score = self(head_idx, rel_idx, neg_tail_idx)  # Negative triple score\n",
    "\n",
    "        # Use margin ranking loss to ensure positive scores are greater than negative scores by a margin\n",
    "        return F.margin_ranking_loss(\n",
    "            pos_score, \n",
    "            neg_score, \n",
    "            target=torch.ones_like(pos_score),  # Target label (+1) to indicate positive > negative\n",
    "            margin=margin  # Margin value\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff039e2",
   "metadata": {},
   "source": [
    "### 4.2. Create Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de04860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(edge_index, edge_type, num_nodes, batch_size, shuffle=True):\n",
    "    head_idx = edge_index[0]\n",
    "    tail_idx = edge_index[1]\n",
    "    dataset = TensorDataset(head_idx, edge_type, tail_idx)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc12bf",
   "metadata": {},
   "source": [
    "### 4.3. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7a90840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/train.txt\n",
      "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/valid.txt\n",
      "Downloading https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237/test.txt\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load FB15k-237 dataset\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = osp.join('data', 'FB15k-237')\n",
    "train_data = FB15k_237(path, split='train')[0].to(device)\n",
    "val_data = FB15k_237(path, split='val')[0].to(device)\n",
    "test_data = FB15k_237(path, split='test')[0].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90607d23",
   "metadata": {},
   "source": [
    "### 4.4. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e3ba05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_nodes = train_data.num_nodes\n",
    "num_rels = train_data.num_edge_types\n",
    "model = TransH(num_nodes=num_nodes, num_rels=num_rels, embedding_dim=50).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4fa89eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_loader(train_data.edge_index, train_data.edge_type, num_nodes, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47d436",
   "metadata": {},
   "source": [
    "### 4.5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bc6b99da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5315\n",
      "Epoch 2, Loss: 0.1710\n",
      "Epoch 3, Loss: 0.1004\n",
      "Epoch 4, Loss: 0.0783\n",
      "Epoch 5, Loss: 0.0664\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, 6):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for head, rel, tail in train_loader:\n",
    "        head, rel, tail = head.to(device), rel.to(device), tail.to(device)\n",
    "        neg_tail = torch.randint(0, num_nodes, tail.size(), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head, rel, tail, neg_tail)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bc37c",
   "metadata": {},
   "source": [
    "### 4.6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "388d78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "@torch.no_grad()\n",
    "def test(model, data, num_nodes, k=10):\n",
    "    model.eval()\n",
    "    head_idx = data.edge_index[0]\n",
    "    rel_idx = data.edge_type\n",
    "    tail_idx = data.edge_index[1]\n",
    "\n",
    "    ranks, mrr = [], 0.0\n",
    "    for head, rel, tail in zip(head_idx, rel_idx, tail_idx):\n",
    "        head, rel, tail = head.to(device), rel.to(device), tail.to(device)\n",
    "\n",
    "        # True score\n",
    "        true_score = model(head.unsqueeze(0), rel.unsqueeze(0), tail.unsqueeze(0))\n",
    "\n",
    "        # Corrupted tail scores\n",
    "        all_entities = torch.arange(num_nodes, device=device)\n",
    "        scores = model(head.repeat(num_nodes), rel.repeat(num_nodes), all_entities)\n",
    "\n",
    "        # Rank calculation\n",
    "        rank = (scores > true_score).sum().item() + 1\n",
    "        ranks.append(rank)\n",
    "        mrr += 1 / rank\n",
    "\n",
    "    mean_rank = sum(ranks) / len(ranks)\n",
    "    mean_reciprocal_rank = mrr / len(ranks)\n",
    "    return mean_rank, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ba208969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank: 285.20, Mean Reciprocal Rank: 0.2399\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "mean_rank, mean_reciprocal_rank = test(model, test_data, num_nodes)\n",
    "print(f\"Mean Rank: {mean_rank:.2f}, Mean Reciprocal Rank: {mean_reciprocal_rank:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fc8df",
   "metadata": {},
   "source": [
    "## 5. Embedding Entities and Relations for Learning and Inference in Knowledge Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf67184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "# from torch_geometric.nn import DistMult\n",
    "\n",
    "model_map = {\n",
    "    'distmult': DistMult,\n",
    "    'rotate': RotatE,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744776f7",
   "metadata": {},
   "source": [
    "### 5.1. DistMult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# Import the Knowledge Graph Embedding (KGE) base class\n",
    "from torch_geometric.nn.kge import KGEModel\n",
    "\n",
    "# Define a DistMult class inheriting from the KGEModel\n",
    "class DistMult(KGEModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,         # Number of entities (nodes) in the knowledge graph\n",
    "        num_relations: int,     # Number of relation types in the knowledge graph\n",
    "        hidden_channels: int,   # Dimensionality of the embedding space\n",
    "        margin: float = 1.0,    # Margin for margin ranking loss\n",
    "        sparse: bool = False,   # Whether embeddings are stored sparsely\n",
    "    ):\n",
    "        # Initialize the parent KGEModel with node and relation embeddings\n",
    "        super().__init__(num_nodes, num_relations, hidden_channels, sparse)\n",
    "\n",
    "        # Set the margin value for the loss function\n",
    "        self.margin = margin\n",
    "\n",
    "        # Reset embeddings to initialize their values\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize node embeddings with Xavier uniform distribution\n",
    "        torch.nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "        # Initialize relation embeddings with Xavier uniform distribution\n",
    "        torch.nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        head_index: Tensor,  # Indices of head entities\n",
    "        rel_type: Tensor,    # Indices of relation types\n",
    "        tail_index: Tensor,  # Indices of tail entities\n",
    "    ) -> Tensor:\n",
    "        # Retrieve embeddings for head entities\n",
    "        head = self.node_emb(head_index)\n",
    "        # Retrieve embeddings for relations\n",
    "        rel = self.rel_emb(rel_type)\n",
    "        # Retrieve embeddings for tail entities\n",
    "        tail = self.node_emb(tail_index)\n",
    "\n",
    "        # Compute the DistMult score: element-wise multiplication and summation\n",
    "        return (head * rel * tail).sum(dim=-1)\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        head_index: Tensor,  # Indices of positive head entities\n",
    "        rel_type: Tensor,    # Indices of positive relations\n",
    "        tail_index: Tensor,  # Indices of positive tail entities\n",
    "    ) -> Tensor:\n",
    "        # Compute scores for positive triples\n",
    "        pos_score = self(head_index, rel_type, tail_index)\n",
    "        # Compute scores for negative triples (randomly sampled)\n",
    "        neg_score = self(*self.random_sample(head_index, rel_type, tail_index))\n",
    "\n",
    "        # Apply margin ranking loss to separate positive and negative scores\n",
    "        return F.margin_ranking_loss(\n",
    "            pos_score,                        # Positive scores\n",
    "            neg_score,                        # Negative scores\n",
    "            target=torch.ones_like(pos_score), # Target: 1 for positive ranking\n",
    "            margin=self.margin,               # Margin for the loss\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e3e39",
   "metadata": {},
   "source": [
    "### 5.2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6eb87ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', choices=model_map.keys(), type=str.lower, required=False)\n",
    "# Add this to avoid issues with Jupyter Notebook's internal arguments\n",
    "parser.add_argument(\"-f\", required=False, help=\"Dummy argument to prevent Jupyter errors.\")\n",
    "args = parser.parse_args([])  # Passing an empty list ensures no CLI arguments are processed.\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = osp.join('data', 'FB15k')\n",
    "\n",
    "train_data = FB15k_237(path, split='train')[0].to(device)\n",
    "val_data = FB15k_237(path, split='val')[0].to(device)\n",
    "test_data = FB15k_237(path, split='test')[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ad0f387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg_map = {'rotate': {'margin': 9.0}}\n",
    "args.model = \"distmult\"\n",
    "model = model_map[args.model](\n",
    "    num_nodes=train_data.num_nodes,\n",
    "    num_relations=train_data.num_edge_types,\n",
    "    hidden_channels=50,\n",
    "    **model_arg_map.get(args.model, {}),\n",
    ").to(device)\n",
    "\n",
    "loader = model.loader(\n",
    "    head_index=train_data.edge_index[0],\n",
    "    rel_type=train_data.edge_type,\n",
    "    tail_index=train_data.edge_index[1],\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_map = {\n",
    "    'distmult': optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6),\n",
    "    'rotate': optim.Adam(model.parameters(), lr=1e-3),\n",
    "}\n",
    "optimizer = optimizer_map[args.model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991d887",
   "metadata": {},
   "source": [
    "### 5.3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for head_index, rel_type, tail_index in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head_index, rel_type, tail_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1d12b",
   "metadata": {},
   "source": [
    "### 5.4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4274206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    result = model.test(\n",
    "        head_index=data.edge_index[0],\n",
    "        rel_type=data.edge_type,\n",
    "        tail_index=data.edge_index[1],\n",
    "        batch_size=20000,\n",
    "        k=10,\n",
    "    )\n",
    "    print(result)  # Inspect the result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.2411\n",
      "Epoch: 002, Loss: 0.2315\n",
      "Epoch: 003, Loss: 0.2206\n",
      "Epoch: 004, Loss: 0.2123\n",
      "Epoch: 005, Loss: 0.2032\n",
      "Epoch: 006, Loss: 0.1961\n",
      "Epoch: 007, Loss: 0.1887\n",
      "Epoch: 008, Loss: 0.1829\n",
      "Epoch: 009, Loss: 0.1773\n",
      "Epoch: 010, Loss: 0.1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20466/20466 [00:04<00:00, 4869.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539.7003173828125, 0.22544750571250916, 0.3587413270790579)\n",
      "Test Mean Rank: 539.70, Test MRR: 0.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    if epoch % 25 == 0:\n",
    "        rank, mrr = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}')\n",
    "\n",
    "rank, mrr, _ = test(test_data) \n",
    "print(f'Test Mean Rank: {rank:.2f}, Test MRR: {mrr:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
