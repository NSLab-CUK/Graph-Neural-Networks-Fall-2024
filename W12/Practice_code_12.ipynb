{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0736dbf-54c6-4475-a833-a3043414be16",
   "metadata": {},
   "source": [
    "# 1. Neural Graph Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04b823-c673-4699-b05a-77dce606c5e8",
   "metadata": {},
   "source": [
    "## 1.1. NGCF class construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "764a7171-38d9-4ba7-825a-b46e5bdf327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, args):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.device = args.device\n",
    "        self.emb_size = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.node_dropout = args.node_dropout[0]\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.norm_adj = norm_adj\n",
    "\n",
    "        self.layers = eval(args.layer_size)\n",
    "        self.decay = eval(args.regs)[0]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Init the weight of user-item.\n",
    "        \"\"\"\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Get sparse adj.\n",
    "        \"\"\"\n",
    "        self.sparse_norm_adj = self._convert_sp_mat_to_sp_tensor(self.norm_adj).to(self.device)\n",
    "\n",
    "    def init_weight(self):\n",
    "        # xavier init\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.n_user,\n",
    "                                                 self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.n_item,\n",
    "                                                 self.emb_size)))\n",
    "        })\n",
    "\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layers\n",
    "        for k in range(len(self.layers)):\n",
    "            weight_dict.update({'W_gc_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_gc_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "            weight_dict.update({'W_bi_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_bi_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo()\n",
    "        i = torch.LongTensor([coo.row, coo.col])\n",
    "        v = torch.from_numpy(coo.data).float()\n",
    "        return torch.sparse.FloatTensor(i, v, coo.shape)\n",
    "\n",
    "    def sparse_dropout(self, x, rate, noise_shape):\n",
    "        random_tensor = 1 - rate\n",
    "        random_tensor += torch.rand(noise_shape).to(x.device)\n",
    "        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n",
    "        i = x._indices()\n",
    "        v = x._values()\n",
    "\n",
    "        i = i[:, dropout_mask]\n",
    "        v = v[dropout_mask]\n",
    "\n",
    "        out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device)\n",
    "        return out * (1. / (1 - rate))\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "\n",
    "        # cul regularizer\n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = self.decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss, mf_loss, emb_loss\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items, drop_flag=True):\n",
    "\n",
    "        A_hat = self.sparse_dropout(self.sparse_norm_adj,\n",
    "                                    self.node_dropout,\n",
    "                                    self.sparse_norm_adj._nnz()) if drop_flag else self.sparse_norm_adj\n",
    "\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'],\n",
    "                                    self.embedding_dict['item_emb']], 0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(len(self.layers)):\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "\n",
    "            # transformed sum messages of neighbors.\n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) \\\n",
    "                                             + self.weight_dict['b_gc_%d' % k]\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            # element-wise product\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) \\\n",
    "                                            + self.weight_dict['b_bi_%d' % k]\n",
    "\n",
    "            # non-linear activation.\n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope=0.2)(sum_embeddings + bi_embeddings)\n",
    "\n",
    "            # message dropout.\n",
    "            ego_embeddings = nn.Dropout(self.mess_dropout[k])(ego_embeddings)\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        look up.\n",
    "        \"\"\"\n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        pos_i_g_embeddings = i_g_embeddings[pos_items, :]\n",
    "        neg_i_g_embeddings = i_g_embeddings[neg_items, :]\n",
    "\n",
    "        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103f64f-1fb6-4d17-8b61-5394d18ad257",
   "metadata": {},
   "source": [
    "## 1.2. Data object construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "17a63c87-f02a-43bb-a1c3-fa468e373a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_train += len(items)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')[1:]]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_test += len(items)\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        with open(train_file) as f_train:\n",
    "            with open(test_file) as f_test:\n",
    "                for l in f_train.readlines():\n",
    "                    if len(l) == 0:\n",
    "                        break\n",
    "                    l = l.strip('\\n')\n",
    "                    items = [int(i) for i in l.split(' ')]\n",
    "                    uid, train_items = items[0], items[1:]\n",
    "\n",
    "                    for i in train_items:\n",
    "                        self.R[uid, i] = 1.\n",
    "                        # self.R[uid][i] = 1\n",
    "\n",
    "                    self.train_items[uid] = train_items\n",
    "\n",
    "                for l in f_test.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    uid, test_items = items[0], items[1:]\n",
    "                    self.test_set[uid] = test_items\n",
    "\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def mean_adj_single(adj):\n",
    "            # D^-1 * A\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            # D^-1/2 * A * D^-1/2\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "\n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "            return temp\n",
    "\n",
    "        norm_adj_mat = mean_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        # norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = mean_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "\n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # sample num pos items for u-th user\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # sample num neg items for u-th user\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_set.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_items[uid]\n",
    "            test_iids = self.test_set[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "        return split_uids, split_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e47d1-4f07-4e53-9964-61bfee427ef6",
   "metadata": {},
   "source": [
    "## 1.4. Metrics: \n",
    "\n",
    "precision_at_k, average_precision, mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "200e6638-ea12-488b-9e98-34f9bfda3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k, ground_truth, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "\n",
    "        Low but correct defination\n",
    "    \"\"\"\n",
    "    GT = set(ground_truth)\n",
    "    if len(GT) > k :\n",
    "        sent_list = [1.0] * k\n",
    "    else:\n",
    "        sent_list = [1.0]*len(GT) + [0.0]*(k-len(GT))\n",
    "    dcg_max = dcg_at_k(sent_list, k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    # if all_pos_num == 0:\n",
    "    #     return 0\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def AUC(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a57b3-c6c5-47bc-8f12-2aa5f9dee887",
   "metadata": {},
   "source": [
    "## 1.5. Change to NGCF folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "780aab82-59fd-453b-9db9-9505bde4e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ubuntu/Documents/Graph-Neural-Networks-Fall-2023/W14/NGCF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.getcwd(), 'NGCF'))  # Set the path to NGCF directory if needed.\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ea1d8-5aee-42b9-918a-88691559f459",
   "metadata": {},
   "source": [
    "## 1.6. Run the main.py code\n",
    "\n",
    "Open Visual Studio/Pycharm to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1848405-0d11-48f1-9425-434e3c21eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('python main.py --dataset gowalla --regs [1e-5] --embed_size 64 --layer_size [64,64,64] --lr 0.0001 --save_flag 1 --pretrain 0 --batch_size 1024 --epoch 400 --verbose 1 --node_dropout [0.1] --mess_dropout [0.1,0.1,0.1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e4c59",
   "metadata": {},
   "source": [
    "# 2. KGAT: Knowledge Graph Attention Network for Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d0616170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "        self.message_dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            self.linear = nn.Linear(self.in_dim, self.out_dim)       # W in Equation (6)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            self.linear = nn.Linear(self.in_dim * 2, self.out_dim)   # W in Equation (7)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            self.linear1 = nn.Linear(self.in_dim, self.out_dim)      # W1 in Equation (8)\n",
    "            self.linear2 = nn.Linear(self.in_dim, self.out_dim)      # W2 in Equation (8)\n",
    "            nn.init.xavier_uniform_(self.linear1.weight)\n",
    "            nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        \"\"\"\n",
    "        ego_embeddings:  (n_users + n_entities, in_dim)\n",
    "        A_in:            (n_users + n_entities, n_users + n_entities), torch.sparse.FloatTensor\n",
    "        \"\"\"\n",
    "        # Equation (3)\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            # Equation (6) & (9)\n",
    "            embeddings = ego_embeddings + side_embeddings\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            # Equation (7) & (9)\n",
    "            embeddings = torch.cat([ego_embeddings, side_embeddings], dim=1)\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            # Equation (8) & (9)\n",
    "            sum_embeddings = self.activation(self.linear1(ego_embeddings + side_embeddings))\n",
    "            bi_embeddings = self.activation(self.linear2(ego_embeddings * side_embeddings))\n",
    "            embeddings = bi_embeddings + sum_embeddings\n",
    "\n",
    "        embeddings = self.message_dropout(embeddings)           # (n_users + n_entities, out_dim)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class KGAT(nn.Module):\n",
    "\n",
    "    def __init__(self, args, n_users, n_entities, n_relations, A_in=None, user_pre_embed=None, item_pre_embed=None):\n",
    "        super(KGAT, self).__init__()\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.user_entity_embed = nn.Embedding(n_users + n_entities, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(n_relations, self.embed_dim)\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.relation_dim = args.relation_dim\n",
    "\n",
    "        self.aggregation_type = args.aggregation_type\n",
    "        self.conv_dim_list = [args.embed_dim] + eval(args.conv_dim_list)\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.n_layers = len(eval(args.conv_dim_list))\n",
    "\n",
    "        self.kg_l2loss_lambda = args.kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = args.cf_l2loss_lambda\n",
    "\n",
    "        self.entity_user_embed = nn.Embedding(self.n_entities + self.n_users, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(self.n_relations, self.relation_dim)\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(self.n_relations, self.embed_dim, self.relation_dim))\n",
    "\n",
    "        if (self.use_pretrain == 1) and (user_pre_embed is not None) and (item_pre_embed is not None):\n",
    "            other_entity_embed = nn.Parameter(torch.Tensor(self.n_entities - item_pre_embed.shape[0], self.embed_dim))\n",
    "            nn.init.xavier_uniform_(other_entity_embed)\n",
    "            entity_user_embed = torch.cat([item_pre_embed, other_entity_embed, user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        self.aggregator_layers = nn.ModuleList()\n",
    "        for k in range(self.n_layers):\n",
    "            self.aggregator_layers.append(Aggregator(self.conv_dim_list[k], self.conv_dim_list[k + 1], self.mess_dropout[k], self.aggregation_type))\n",
    "\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in\n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "\n",
    "        for idx, layer in enumerate(self.aggregator_layers):\n",
    "            ego_embed = layer(ego_embed, self.A_in)\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "\n",
    "        # Equation (11)\n",
    "        all_embed = torch.cat(all_embed, dim=1)         # (n_users + n_entities, concat_dim)\n",
    "        return all_embed\n",
    "\n",
    "\n",
    "    def calc_cf_loss(self, user_ids, item_pos_ids, item_neg_ids):\n",
    "        \"\"\"\n",
    "        user_ids:       (cf_batch_size)\n",
    "        item_pos_ids:   (cf_batch_size)\n",
    "        item_neg_ids:   (cf_batch_size)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()                       # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                            # (cf_batch_size, concat_dim)\n",
    "        item_pos_embed = all_embed[item_pos_ids]                    # (cf_batch_size, concat_dim)\n",
    "        item_neg_embed = all_embed[item_neg_ids]                    # (cf_batch_size, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        pos_score = torch.sum(user_embed * item_pos_embed, dim=1)   # (cf_batch_size)\n",
    "        neg_score = torch.sum(user_embed * item_neg_embed, dim=1)   # (cf_batch_size)\n",
    "\n",
    "        # Equation (13)\n",
    "        # cf_loss = F.softplus(neg_score - pos_score)\n",
    "        cf_loss = (-1.0) * F.logsigmoid(pos_score - neg_score)\n",
    "        cf_loss = torch.mean(cf_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(user_embed) + _L2_loss_mean(item_pos_embed) + _L2_loss_mean(item_neg_embed)\n",
    "        loss = cf_loss + self.cf_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def calc_kg_loss(self, h, r, pos_t, neg_t):\n",
    "        \"\"\"\n",
    "        h:      (kg_batch_size)\n",
    "        r:      (kg_batch_size)\n",
    "        pos_t:  (kg_batch_size)\n",
    "        neg_t:  (kg_batch_size)\n",
    "        \"\"\"\n",
    "        r_embed = self.relation_embed(r)                                                # (kg_batch_size, relation_dim)\n",
    "        W_r = self.trans_M[r]                                                           # (kg_batch_size, embed_dim, relation_dim)\n",
    "\n",
    "        h_embed = self.entity_user_embed(h)                                             # (kg_batch_size, embed_dim)\n",
    "        pos_t_embed = self.entity_user_embed(pos_t)                                     # (kg_batch_size, embed_dim)\n",
    "        neg_t_embed = self.entity_user_embed(neg_t)                                     # (kg_batch_size, embed_dim)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)                       # (kg_batch_size, relation_dim)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "\n",
    "        # Equation (1)\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)     # (kg_batch_size)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)     # (kg_batch_size)\n",
    "\n",
    "        # Equation (2)\n",
    "        # kg_loss = F.softplus(pos_score - neg_score)\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(r_mul_h) + _L2_loss_mean(r_embed) + _L2_loss_mean(r_mul_pos_t) + _L2_loss_mean(r_mul_neg_t)\n",
    "        loss = kg_loss + self.kg_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update_attention_batch(self, h_list, t_list, r_idx):\n",
    "        r_embed = self.relation_embed.weight[r_idx]\n",
    "        W_r = self.trans_M[r_idx]\n",
    "\n",
    "        h_embed = self.entity_user_embed.weight[h_list]\n",
    "        t_embed = self.entity_user_embed.weight[t_list]\n",
    "\n",
    "        # Equation (4)\n",
    "        r_mul_h = torch.matmul(h_embed, W_r)\n",
    "        r_mul_t = torch.matmul(t_embed, W_r)\n",
    "        v_list = torch.sum(r_mul_t * torch.tanh(r_mul_h + r_embed), dim=1)\n",
    "        return v_list\n",
    "\n",
    "\n",
    "    def update_attention(self, h_list, t_list, r_list, relations):\n",
    "        device = self.A_in.device\n",
    "\n",
    "        rows = []\n",
    "        cols = []\n",
    "        values = []\n",
    "\n",
    "        for r_idx in relations:\n",
    "            index_list = torch.where(r_list == r_idx)\n",
    "            batch_h_list = h_list[index_list]\n",
    "            batch_t_list = t_list[index_list]\n",
    "\n",
    "            batch_v_list = self.update_attention_batch(batch_h_list, batch_t_list, r_idx)\n",
    "            rows.append(batch_h_list)\n",
    "            cols.append(batch_t_list)\n",
    "            values.append(batch_v_list)\n",
    "\n",
    "        rows = torch.cat(rows)\n",
    "        cols = torch.cat(cols)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        indices = torch.stack([rows, cols])\n",
    "        shape = self.A_in.shape\n",
    "        A_in = torch.sparse.FloatTensor(indices, values, torch.Size(shape))\n",
    "\n",
    "        # Equation (5)\n",
    "        A_in = torch.sparse.softmax(A_in.cpu(), dim=1)\n",
    "        self.A_in.data = A_in.to(device)\n",
    "\n",
    "\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        user_ids:  (n_users)\n",
    "        item_ids:  (n_items)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()           # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                # (n_users, concat_dim)\n",
    "        item_embed = all_embed[item_ids]                # (n_items, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        cf_score = torch.matmul(user_embed, item_embed.transpose(0, 1))    # (n_users, n_items)\n",
    "        return cf_score\n",
    "\n",
    "\n",
    "    def forward(self, *input, mode):\n",
    "        if mode == 'train_cf':\n",
    "            return self.calc_cf_loss(*input)\n",
    "        if mode == 'train_kg':\n",
    "            return self.calc_kg_loss(*input)\n",
    "        if mode == 'update_att':\n",
    "            return self.update_attention(*input)\n",
    "        if mode == 'predict':\n",
    "            return self.calc_score(*input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e19cb6",
   "metadata": {},
   "source": [
    "## log_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "82152e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def create_log_id(dir_path):\n",
    "    log_count = 0\n",
    "    file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n",
    "    while os.path.exists(file_path):\n",
    "        log_count += 1\n",
    "        file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n",
    "    return log_count\n",
    "\n",
    "\n",
    "def logging_config(folder=None, name=None,\n",
    "                   level=logging.DEBUG,\n",
    "                   console_level=logging.DEBUG,\n",
    "                   no_console=True):\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    for handler in logging.root.handlers:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.root.handlers = []\n",
    "    logpath = os.path.join(folder, name + \".log\")\n",
    "    print(\"All logs will be saved to %s\" %logpath)\n",
    "\n",
    "    logging.root.setLevel(level)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    logfile = logging.FileHandler(logpath)\n",
    "    logfile.setLevel(level)\n",
    "    logfile.setFormatter(formatter)\n",
    "    logging.root.addHandler(logfile)\n",
    "\n",
    "    if not no_console:\n",
    "        logconsole = logging.StreamHandler()\n",
    "        logconsole.setLevel(console_level)\n",
    "        logconsole.setFormatter(formatter)\n",
    "        logging.root.addHandler(logconsole)\n",
    "    return folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8e804",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c6a65bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n",
    "\n",
    "\n",
    "def calc_recall(rank, ground_truth, k):\n",
    "    \"\"\"\n",
    "    calculate recall of one example\n",
    "    \"\"\"\n",
    "    return len(set(rank[:k]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "\n",
    "def precision_at_k(hit, k):\n",
    "    \"\"\"\n",
    "    calculate Precision@k\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)[:k]\n",
    "    return np.mean(hit)\n",
    "\n",
    "\n",
    "def precision_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate Precision@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    res = hits[:, :k].mean(axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def average_precision(hit, cut):\n",
    "    \"\"\"\n",
    "    calculate average precision (area under PR curve)\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)\n",
    "    precisions = [precision_at_k(hit, k + 1) for k in range(cut) if len(hit) >= k]\n",
    "    if not precisions:\n",
    "        return 0.\n",
    "    return np.sum(precisions) / float(min(cut, np.sum(hit)))\n",
    "\n",
    "\n",
    "def dcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    calculate discounted cumulative gain (dcg)\n",
    "    rel: list, element is positive real values, can be binary\n",
    "    \"\"\"\n",
    "    rel = np.asfarray(rel)[:k]\n",
    "    dcg = np.sum((2 ** rel - 1) / np.log2(np.arange(2, rel.size + 2)))\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    calculate normalized discounted cumulative gain (ndcg)\n",
    "    rel: list, element is positive real values, can be binary\n",
    "    \"\"\"\n",
    "    idcg = dcg_at_k(sorted(rel, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(rel, k) / idcg\n",
    "\n",
    "\n",
    "def ndcg_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate NDCG@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    hits_k = hits[:, :k]\n",
    "    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]\n",
    "    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    idcg[idcg == 0] = np.inf\n",
    "    ndcg = (dcg / idcg)\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def recall_at_k(hit, k, all_pos_num):\n",
    "    \"\"\"\n",
    "    calculate Recall@k\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asfarray(hit)[:k]\n",
    "    return np.sum(hit) / all_pos_num\n",
    "\n",
    "\n",
    "def recall_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate Recall@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))\n",
    "    return res\n",
    "\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def calc_auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res\n",
    "\n",
    "\n",
    "def logloss(ground_truth, prediction):\n",
    "    logloss = log_loss(np.asarray(ground_truth), np.asarray(prediction))\n",
    "    return logloss\n",
    "\n",
    "\n",
    "def calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks):\n",
    "    \"\"\"\n",
    "    cf_scores: (n_users, n_items)\n",
    "    \"\"\"\n",
    "    test_pos_item_binary = np.zeros([len(user_ids), len(item_ids)], dtype=np.float32)\n",
    "    for idx, u in enumerate(user_ids):\n",
    "        train_pos_item_list = train_user_dict[u]\n",
    "        test_pos_item_list = test_user_dict[u]\n",
    "        cf_scores[idx][train_pos_item_list] = -np.inf\n",
    "        test_pos_item_binary[idx][test_pos_item_list] = 1\n",
    "\n",
    "    try:\n",
    "        _, rank_indices = torch.sort(cf_scores.cuda(), descending=True)    # try to speed up the sorting process\n",
    "    except:\n",
    "        _, rank_indices = torch.sort(cf_scores, descending=True)\n",
    "    rank_indices = rank_indices.cpu()\n",
    "\n",
    "    binary_hit = []\n",
    "    for i in range(len(user_ids)):\n",
    "        binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n",
    "    binary_hit = np.array(binary_hit, dtype=np.float32)\n",
    "\n",
    "    metrics_dict = {}\n",
    "    for k in Ks:\n",
    "        metrics_dict[k] = {}\n",
    "        metrics_dict[k]['precision'] = precision_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['recall']    = recall_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['ndcg']      = ndcg_at_k_batch(binary_hit, k)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54465d",
   "metadata": {},
   "source": [
    "## model_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8ae0d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def early_stopping(recall_list, stopping_steps):\n",
    "    best_recall = max(recall_list)\n",
    "    best_step = recall_list.index(best_recall)\n",
    "    if len(recall_list) - best_step - 1 >= stopping_steps:\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "    return best_recall, should_stop\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, current_epoch, last_best_epoch=None):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(current_epoch))\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': current_epoch}, model_state_file)\n",
    "\n",
    "    if last_best_epoch is not None and current_epoch != last_best_epoch:\n",
    "        old_model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(last_best_epoch))\n",
    "        if os.path.exists(old_model_state_file):\n",
    "            os.system('rm {}'.format(old_model_state_file))\n",
    "\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5c7b4",
   "metadata": {},
   "source": [
    "## loader_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a1f0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataLoaderBase(object):\n",
    "\n",
    "    def __init__(self, args, logging):\n",
    "        self.args = args\n",
    "        self.data_name = args.data_name\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "        self.pretrain_embedding_dir = args.pretrain_embedding_dir\n",
    "\n",
    "        self.data_dir = os.path.join(args.data_dir, args.data_name)\n",
    "        self.train_file = os.path.join(self.data_dir, 'train.txt')\n",
    "        self.test_file = os.path.join(self.data_dir, 'test.txt')\n",
    "        self.kg_file = os.path.join(self.data_dir, \"kg_final.txt\")\n",
    "\n",
    "        self.cf_train_data, self.train_user_dict = self.load_cf(self.train_file)\n",
    "        self.cf_test_data, self.test_user_dict = self.load_cf(self.test_file)\n",
    "        self.statistic_cf()\n",
    "\n",
    "        if self.use_pretrain == 1:\n",
    "            self.load_pretrained_data()\n",
    "\n",
    "\n",
    "    def load_cf(self, filename):\n",
    "        user = []\n",
    "        item = []\n",
    "        user_dict = dict()\n",
    "\n",
    "        lines = open(filename, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmp = l.strip()\n",
    "            inter = [int(i) for i in tmp.split()]\n",
    "\n",
    "            if len(inter) > 1:\n",
    "                user_id, item_ids = inter[0], inter[1:]\n",
    "                item_ids = list(set(item_ids))\n",
    "\n",
    "                for item_id in item_ids:\n",
    "                    user.append(user_id)\n",
    "                    item.append(item_id)\n",
    "                user_dict[user_id] = item_ids\n",
    "\n",
    "        user = np.array(user, dtype=np.int32)\n",
    "        item = np.array(item, dtype=np.int32)\n",
    "        return (user, item), user_dict\n",
    "\n",
    "\n",
    "    def statistic_cf(self):\n",
    "        self.n_users = max(max(self.cf_train_data[0]), max(self.cf_test_data[0])) + 1\n",
    "        self.n_items = max(max(self.cf_train_data[1]), max(self.cf_test_data[1])) + 1\n",
    "        self.n_cf_train = len(self.cf_train_data[0])\n",
    "        self.n_cf_test = len(self.cf_test_data[0])\n",
    "\n",
    "\n",
    "    def load_kg(self, filename):\n",
    "        kg_data = pd.read_csv(filename, sep=' ', names=['h', 'r', 't'], engine='python')\n",
    "        kg_data = kg_data.drop_duplicates()\n",
    "        return kg_data\n",
    "\n",
    "\n",
    "    def sample_pos_items_for_u(self, user_dict, user_id, n_sample_pos_items):\n",
    "        pos_items = user_dict[user_id]\n",
    "        n_pos_items = len(pos_items)\n",
    "\n",
    "        sample_pos_items = []\n",
    "        while True:\n",
    "            if len(sample_pos_items) == n_sample_pos_items:\n",
    "                break\n",
    "\n",
    "            pos_item_idx = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "            pos_item_id = pos_items[pos_item_idx]\n",
    "            if pos_item_id not in sample_pos_items:\n",
    "                sample_pos_items.append(pos_item_id)\n",
    "        return sample_pos_items\n",
    "\n",
    "\n",
    "    def sample_neg_items_for_u(self, user_dict, user_id, n_sample_neg_items):\n",
    "        pos_items = user_dict[user_id]\n",
    "\n",
    "        sample_neg_items = []\n",
    "        while True:\n",
    "            if len(sample_neg_items) == n_sample_neg_items:\n",
    "                break\n",
    "\n",
    "            neg_item_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "            if neg_item_id not in pos_items and neg_item_id not in sample_neg_items:\n",
    "                sample_neg_items.append(neg_item_id)\n",
    "        return sample_neg_items\n",
    "\n",
    "\n",
    "    def generate_cf_batch(self, user_dict, batch_size):\n",
    "        exist_users = list(user_dict.keys())\n",
    "        if batch_size <= len(exist_users):\n",
    "            batch_user = random.sample(exist_users, batch_size)\n",
    "        else:\n",
    "            batch_user = [random.choice(exist_users) for _ in range(batch_size)]\n",
    "\n",
    "        batch_pos_item, batch_neg_item = [], []\n",
    "        for u in batch_user:\n",
    "            batch_pos_item += self.sample_pos_items_for_u(user_dict, u, 1)\n",
    "            batch_neg_item += self.sample_neg_items_for_u(user_dict, u, 1)\n",
    "\n",
    "        batch_user = torch.LongTensor(batch_user)\n",
    "        batch_pos_item = torch.LongTensor(batch_pos_item)\n",
    "        batch_neg_item = torch.LongTensor(batch_neg_item)\n",
    "        return batch_user, batch_pos_item, batch_neg_item\n",
    "\n",
    "\n",
    "    def sample_pos_triples_for_h(self, kg_dict, head, n_sample_pos_triples):\n",
    "        pos_triples = kg_dict[head]\n",
    "        n_pos_triples = len(pos_triples)\n",
    "\n",
    "        sample_relations, sample_pos_tails = [], []\n",
    "        while True:\n",
    "            if len(sample_relations) == n_sample_pos_triples:\n",
    "                break\n",
    "\n",
    "            pos_triple_idx = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "            tail = pos_triples[pos_triple_idx][0]\n",
    "            relation = pos_triples[pos_triple_idx][1]\n",
    "\n",
    "            if relation not in sample_relations and tail not in sample_pos_tails:\n",
    "                sample_relations.append(relation)\n",
    "                sample_pos_tails.append(tail)\n",
    "        return sample_relations, sample_pos_tails\n",
    "\n",
    "\n",
    "    def sample_neg_triples_for_h(self, kg_dict, head, relation, n_sample_neg_triples, highest_neg_idx):\n",
    "        pos_triples = kg_dict[head]\n",
    "\n",
    "        sample_neg_tails = []\n",
    "        while True:\n",
    "            if len(sample_neg_tails) == n_sample_neg_triples:\n",
    "                break\n",
    "\n",
    "            tail = np.random.randint(low=0, high=highest_neg_idx, size=1)[0]\n",
    "            if (tail, relation) not in pos_triples and tail not in sample_neg_tails:\n",
    "                sample_neg_tails.append(tail)\n",
    "        return sample_neg_tails\n",
    "\n",
    "\n",
    "    def generate_kg_batch(self, kg_dict, batch_size, highest_neg_idx):\n",
    "        exist_heads = list(kg_dict.keys())\n",
    "        if batch_size <= len(exist_heads):\n",
    "            batch_head = random.sample(exist_heads, batch_size)\n",
    "        else:\n",
    "            batch_head = [random.choice(exist_heads) for _ in range(batch_size)]\n",
    "\n",
    "        batch_relation, batch_pos_tail, batch_neg_tail = [], [], []\n",
    "        for h in batch_head:\n",
    "            relation, pos_tail = self.sample_pos_triples_for_h(kg_dict, h, 1)\n",
    "            batch_relation += relation\n",
    "            batch_pos_tail += pos_tail\n",
    "\n",
    "            neg_tail = self.sample_neg_triples_for_h(kg_dict, h, relation[0], 1, highest_neg_idx)\n",
    "            batch_neg_tail += neg_tail\n",
    "\n",
    "        batch_head = torch.LongTensor(batch_head)\n",
    "        batch_relation = torch.LongTensor(batch_relation)\n",
    "        batch_pos_tail = torch.LongTensor(batch_pos_tail)\n",
    "        batch_neg_tail = torch.LongTensor(batch_neg_tail)\n",
    "        return batch_head, batch_relation, batch_pos_tail, batch_neg_tail\n",
    "\n",
    "\n",
    "    def load_pretrained_data(self):\n",
    "        pre_model = 'mf'\n",
    "        pretrain_path = '%s/%s/%s.npz' % (self.pretrain_embedding_dir, self.data_name, pre_model)\n",
    "        pretrain_data = np.load(pretrain_path)\n",
    "        self.user_pre_embed = pretrain_data['user_embed']\n",
    "        self.item_pre_embed = pretrain_data['item_embed']\n",
    "\n",
    "        assert self.user_pre_embed.shape[0] == self.n_users\n",
    "        assert self.item_pre_embed.shape[0] == self.n_items\n",
    "        assert self.user_pre_embed.shape[1] == self.args.embed_dim\n",
    "        assert self.item_pre_embed.shape[1] == self.args.embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850efbc0",
   "metadata": {},
   "source": [
    "## loader_kgat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3234fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderKGAT(DataLoaderBase):\n",
    "    def __init__(self, args, logging):\n",
    "        super().__init__(args, logging)\n",
    "        \n",
    "        # Check if self.cf_train_data and self.cf_test_data are loaded correctly\n",
    "        if hasattr(self, 'cf_train_data') and hasattr(self, 'cf_test_data'):\n",
    "            print(\"Train and test data loaded\")\n",
    "\n",
    "        # Assuming cf_train_data and cf_test_data have shape (2, N)\n",
    "        # where index 0 is user IDs and index 1 is item/entity IDs\n",
    "        self.n_users = int(np.max(self.cf_train_data[0]) + 1)\n",
    "        self.n_entities = int(np.max(self.cf_train_data[1]) + 1)\n",
    "\n",
    "        # Print out values to confirm\n",
    "        print(\"Initialized n_users:\", self.n_users, \"Initialized n_entities:\", self.n_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5e65a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class DataLoaderKGAT(DataLoaderBase):\n",
    "\n",
    "    def __init__(self, args, logging):\n",
    "        super().__init__(args, logging)\n",
    "        # Assuming self.cf_train_data and self.cf_test_data contain user and item data\n",
    "        self.n_users = max(self.cf_train_data[0].max(), self.cf_test_data[0].max()) + 1\n",
    "        self.n_entities = max(self.cf_train_data[1].max(), self.cf_test_data[1].max()) + 1\n",
    "        self.cf_batch_size = args.cf_batch_size\n",
    "        self.kg_batch_size = args.kg_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "\n",
    "        kg_data = self.load_kg(self.kg_file)\n",
    "        self.construct_data(kg_data)\n",
    "        self.print_info(logging)\n",
    "\n",
    "        self.laplacian_type = args.laplacian_type\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "\n",
    "\n",
    "    def construct_data(self, kg_data):\n",
    "        # add inverse kg data\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # re-map user id\n",
    "        kg_data['r'] += 2\n",
    "        self.n_relations = max(kg_data['r']) + 1\n",
    "        self.n_entities = max(max(kg_data['h']), max(kg_data['t'])) + 1\n",
    "        self.n_users_entities = self.n_users + self.n_entities\n",
    "\n",
    "        self.cf_train_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_train_data[0]))).astype(np.int32), self.cf_train_data[1].astype(np.int32))\n",
    "        self.cf_test_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_test_data[0]))).astype(np.int32), self.cf_test_data[1].astype(np.int32))\n",
    "\n",
    "        self.train_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.train_user_dict.items()}\n",
    "        self.test_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.test_user_dict.items()}\n",
    "\n",
    "        # add interactions to kg data\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[1]\n",
    "\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[0]\n",
    "\n",
    "        self.kg_train_data = pd.concat([kg_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)\n",
    "\n",
    "        # construct kg dict\n",
    "        h_list = []\n",
    "        t_list = []\n",
    "        r_list = []\n",
    "\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        for row in self.kg_train_data.iterrows():\n",
    "            h, r, t = row[1]\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "\n",
    "        self.h_list = torch.LongTensor(h_list)\n",
    "        self.t_list = torch.LongTensor(t_list)\n",
    "        self.r_list = torch.LongTensor(r_list)\n",
    "\n",
    "\n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "\n",
    "    def create_adjacency_dict(self):\n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj\n",
    "\n",
    "\n",
    "    def create_laplacian_dict(self):\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())\n",
    "\n",
    "\n",
    "    def print_info(self, logging):\n",
    "        logging.info('n_users:           %d' % self.n_users)\n",
    "        logging.info('n_items:           %d' % self.n_items)\n",
    "        logging.info('n_entities:        %d' % self.n_entities)\n",
    "        logging.info('n_users_entities:  %d' % self.n_users_entities)\n",
    "        logging.info('n_relations:       %d' % self.n_relations)\n",
    "\n",
    "        logging.info('n_h_list:          %d' % len(self.h_list))\n",
    "        logging.info('n_t_list:          %d' % len(self.t_list))\n",
    "        logging.info('n_r_list:          %d' % len(self.r_list))\n",
    "\n",
    "        logging.info('n_cf_train:        %d' % self.n_cf_train)\n",
    "        logging.info('n_cf_test:         %d' % self.n_cf_test)\n",
    "\n",
    "        logging.info('n_kg_train:        %d' % self.n_kg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79abcb6",
   "metadata": {},
   "source": [
    "## parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ef68e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parse_kgat_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run KGAT.\")\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=2019,\n",
    "                        help='Random seed.')\n",
    "\n",
    "    parser.add_argument('--data_name', nargs='?', default='amazon-book',\n",
    "                        help='Choose a dataset from {yelp2018, last-fm, amazon-book}')\n",
    "    parser.add_argument('--data_dir', nargs='?', default='/home/ubuntu/Documents/Graph-Neural-Networks-Fall-2023/W14/datasets',\n",
    "                        help='Input data path.')\n",
    "\n",
    "    parser.add_argument('--use_pretrain', type=int, default=1,\n",
    "                        help='0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.')\n",
    "    parser.add_argument('--pretrain_embedding_dir', nargs='?', default='/home/ubuntu/Documents/Graph-Neural-Networks-Fall-2023/W14/datasets/pretrain',\n",
    "                        help='Path of learned embeddings.')\n",
    "    parser.add_argument('--pretrain_model_path', nargs='?', default='trained_model/model.pth',\n",
    "                        help='Path of stored model.')\n",
    "\n",
    "    parser.add_argument('--cf_batch_size', type=int, default=1024,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--kg_batch_size', type=int, default=2048,\n",
    "                        help='KG batch size.')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=10000,\n",
    "                        help='Test batch size (the user number to test every batch).')\n",
    "\n",
    "    parser.add_argument('--embed_dim', type=int, default=64,\n",
    "                        help='User / entity Embedding size.')\n",
    "    parser.add_argument('--relation_dim', type=int, default=64,\n",
    "                        help='Relation Embedding size.')\n",
    "\n",
    "    parser.add_argument('--laplacian_type', type=str, default='random-walk',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {symmetric, random-walk}.')\n",
    "    parser.add_argument('--aggregation_type', type=str, default='bi-interaction',\n",
    "                        help='Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.')\n",
    "    parser.add_argument('--conv_dim_list', nargs='?', default='[64, 32, 16]',\n",
    "                        help='Output sizes of every aggregation layer.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1, 0.1]',\n",
    "                        help='Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.')\n",
    "\n",
    "    parser.add_argument('--kg_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating KG l2 loss.')\n",
    "    parser.add_argument('--cf_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating CF l2 loss.')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--n_epoch', type=int, default=2,\n",
    "                        help='Number of epoch.')\n",
    "    parser.add_argument('--stopping_steps', type=int, default=0,\n",
    "                        help='Number of epoch for early stopping')\n",
    "\n",
    "    parser.add_argument('--cf_print_every', type=int, default=1,\n",
    "                        help='Iter interval of printing CF loss.')\n",
    "    parser.add_argument('--kg_print_every', type=int, default=1,\n",
    "                        help='Iter interval of printing KG loss.')\n",
    "    parser.add_argument('--evaluate_every', type=int, default=2,\n",
    "                        help='Epoch interval of evaluating CF.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[20, 40, 60, 80, 100]',\n",
    "                        help='Calculate metric@K when evaluating.')\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "    save_dir = 'trained_model/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}_pretrain{}/'.format(\n",
    "        args.data_name, args.embed_dim, args.relation_dim, args.laplacian_type, args.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(args.conv_dim_list)]), args.lr, args.use_pretrain)\n",
    "    args.save_dir = save_dir\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b36f9",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0b855f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 04:35:31,989 - root - INFO - Namespace(seed=2019, data_name='amazon-book', data_dir='/home/ubuntu/Documents/Graph-Neural-Networks-Fall-2023/W14/datasets', use_pretrain=1, pretrain_embedding_dir='/home/ubuntu/Documents/Graph-Neural-Networks-Fall-2023/W14/datasets/pretrain', pretrain_model_path='trained_model/model.pth', cf_batch_size=1024, kg_batch_size=2048, test_batch_size=10000, embed_dim=64, relation_dim=64, laplacian_type='random-walk', aggregation_type='bi-interaction', conv_dim_list='[64, 32, 16]', mess_dropout='[0.1, 0.1, 0.1]', kg_l2loss_lambda=1e-05, cf_l2loss_lambda=1e-05, lr=0.0001, n_epoch=2, stopping_steps=0, cf_print_every=1, kg_print_every=1, evaluate_every=2, Ks='[20, 40, 60, 80, 100]', save_dir='trained_model/KGAT/amazon-book/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain1/')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All logs will be saved to trained_model/KGAT/amazon-book/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain1/log17.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 04:37:38,414 - root - INFO - n_users:           70679\n",
      "2024-11-14 04:37:38,415 - root - INFO - n_items:           24915\n",
      "2024-11-14 04:37:38,415 - root - INFO - n_entities:        113487\n",
      "2024-11-14 04:37:38,416 - root - INFO - n_users_entities:  184166\n",
      "2024-11-14 04:37:38,416 - root - INFO - n_relations:       80\n",
      "2024-11-14 04:37:38,416 - root - INFO - n_h_list:          6420520\n",
      "2024-11-14 04:37:38,417 - root - INFO - n_t_list:          6420520\n",
      "2024-11-14 04:37:38,417 - root - INFO - n_r_list:          6420520\n",
      "2024-11-14 04:37:38,417 - root - INFO - n_cf_train:        652514\n",
      "2024-11-14 04:37:38,418 - root - INFO - n_cf_test:         193920\n",
      "2024-11-14 04:37:38,418 - root - INFO - n_kg_train:        6420520\n",
      "/tmp/ipykernel_93580/3756297522.py:118: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "2024-11-14 04:37:43,018 - root - INFO - KGAT(\n",
      "  (user_entity_embed): Embedding(184166, 64)\n",
      "  (relation_embed): Embedding(80, 64)\n",
      "  (entity_user_embed): Embedding(184166, 64)\n",
      "  (aggregator_layers): ModuleList(\n",
      "    (0): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    )\n",
      "    (2): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2024-11-14 04:37:43,246 - root - INFO - CF Training: Epoch 0001 Iter 0001 / 0638 | Time 0.2s | Iter Loss 0.0287 | Iter Mean Loss 0.0287\n",
      "2024-11-14 04:37:43,469 - root - INFO - CF Training: Epoch 0001 Iter 0002 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0256\n",
      "2024-11-14 04:37:43,691 - root - INFO - CF Training: Epoch 0001 Iter 0003 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0254\n",
      "2024-11-14 04:37:43,914 - root - INFO - CF Training: Epoch 0001 Iter 0004 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:37:44,137 - root - INFO - CF Training: Epoch 0001 Iter 0005 / 0638 | Time 0.2s | Iter Loss 0.0248 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:37:44,360 - root - INFO - CF Training: Epoch 0001 Iter 0006 / 0638 | Time 0.2s | Iter Loss 0.0241 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:37:44,581 - root - INFO - CF Training: Epoch 0001 Iter 0007 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:37:44,804 - root - INFO - CF Training: Epoch 0001 Iter 0008 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:37:45,025 - root - INFO - CF Training: Epoch 0001 Iter 0009 / 0638 | Time 0.2s | Iter Loss 0.0234 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:37:45,245 - root - INFO - CF Training: Epoch 0001 Iter 0010 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:37:45,466 - root - INFO - CF Training: Epoch 0001 Iter 0011 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:37:45,686 - root - INFO - CF Training: Epoch 0001 Iter 0012 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:37:45,906 - root - INFO - CF Training: Epoch 0001 Iter 0013 / 0638 | Time 0.2s | Iter Loss 0.0270 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:37:46,127 - root - INFO - CF Training: Epoch 0001 Iter 0014 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:37:46,350 - root - INFO - CF Training: Epoch 0001 Iter 0015 / 0638 | Time 0.2s | Iter Loss 0.0276 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:37:46,572 - root - INFO - CF Training: Epoch 0001 Iter 0016 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:37:46,794 - root - INFO - CF Training: Epoch 0001 Iter 0017 / 0638 | Time 0.2s | Iter Loss 0.0278 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:37:47,017 - root - INFO - CF Training: Epoch 0001 Iter 0018 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:37:47,240 - root - INFO - CF Training: Epoch 0001 Iter 0019 / 0638 | Time 0.2s | Iter Loss 0.0289 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:37:47,463 - root - INFO - CF Training: Epoch 0001 Iter 0020 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:37:47,684 - root - INFO - CF Training: Epoch 0001 Iter 0021 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:37:47,906 - root - INFO - CF Training: Epoch 0001 Iter 0022 / 0638 | Time 0.2s | Iter Loss 0.0292 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:48,129 - root - INFO - CF Training: Epoch 0001 Iter 0023 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:48,354 - root - INFO - CF Training: Epoch 0001 Iter 0024 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:48,575 - root - INFO - CF Training: Epoch 0001 Iter 0025 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:48,796 - root - INFO - CF Training: Epoch 0001 Iter 0026 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:37:49,017 - root - INFO - CF Training: Epoch 0001 Iter 0027 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:37:49,238 - root - INFO - CF Training: Epoch 0001 Iter 0028 / 0638 | Time 0.2s | Iter Loss 0.0245 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:37:49,463 - root - INFO - CF Training: Epoch 0001 Iter 0029 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:37:49,682 - root - INFO - CF Training: Epoch 0001 Iter 0030 / 0638 | Time 0.2s | Iter Loss 0.0266 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:49,903 - root - INFO - CF Training: Epoch 0001 Iter 0031 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:50,124 - root - INFO - CF Training: Epoch 0001 Iter 0032 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:37:50,346 - root - INFO - CF Training: Epoch 0001 Iter 0033 / 0638 | Time 0.2s | Iter Loss 0.0313 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:50,570 - root - INFO - CF Training: Epoch 0001 Iter 0034 / 0638 | Time 0.2s | Iter Loss 0.0270 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:50,791 - root - INFO - CF Training: Epoch 0001 Iter 0035 / 0638 | Time 0.2s | Iter Loss 0.0234 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:51,012 - root - INFO - CF Training: Epoch 0001 Iter 0036 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:51,233 - root - INFO - CF Training: Epoch 0001 Iter 0037 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:51,457 - root - INFO - CF Training: Epoch 0001 Iter 0038 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:51,679 - root - INFO - CF Training: Epoch 0001 Iter 0039 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:51,901 - root - INFO - CF Training: Epoch 0001 Iter 0040 / 0638 | Time 0.2s | Iter Loss 0.0263 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:52,123 - root - INFO - CF Training: Epoch 0001 Iter 0041 / 0638 | Time 0.2s | Iter Loss 0.0287 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:52,344 - root - INFO - CF Training: Epoch 0001 Iter 0042 / 0638 | Time 0.2s | Iter Loss 0.0282 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:52,567 - root - INFO - CF Training: Epoch 0001 Iter 0043 / 0638 | Time 0.2s | Iter Loss 0.0258 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:52,790 - root - INFO - CF Training: Epoch 0001 Iter 0044 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:53,012 - root - INFO - CF Training: Epoch 0001 Iter 0045 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:53,233 - root - INFO - CF Training: Epoch 0001 Iter 0046 / 0638 | Time 0.2s | Iter Loss 0.0332 | Iter Mean Loss 0.0254\n",
      "2024-11-14 04:37:53,456 - root - INFO - CF Training: Epoch 0001 Iter 0047 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0254\n",
      "2024-11-14 04:37:53,679 - root - INFO - CF Training: Epoch 0001 Iter 0048 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0254\n",
      "2024-11-14 04:37:53,899 - root - INFO - CF Training: Epoch 0001 Iter 0049 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0253\n",
      "2024-11-14 04:37:54,119 - root - INFO - CF Training: Epoch 0001 Iter 0050 / 0638 | Time 0.2s | Iter Loss 0.0260 | Iter Mean Loss 0.0253\n",
      "2024-11-14 04:37:54,340 - root - INFO - CF Training: Epoch 0001 Iter 0051 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:54,560 - root - INFO - CF Training: Epoch 0001 Iter 0052 / 0638 | Time 0.2s | Iter Loss 0.0254 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:54,783 - root - INFO - CF Training: Epoch 0001 Iter 0053 / 0638 | Time 0.2s | Iter Loss 0.0259 | Iter Mean Loss 0.0253\n",
      "2024-11-14 04:37:55,002 - root - INFO - CF Training: Epoch 0001 Iter 0054 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:55,224 - root - INFO - CF Training: Epoch 0001 Iter 0055 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:55,446 - root - INFO - CF Training: Epoch 0001 Iter 0056 / 0638 | Time 0.2s | Iter Loss 0.0257 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:55,669 - root - INFO - CF Training: Epoch 0001 Iter 0057 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0252\n",
      "2024-11-14 04:37:55,896 - root - INFO - CF Training: Epoch 0001 Iter 0058 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:56,120 - root - INFO - CF Training: Epoch 0001 Iter 0059 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:56,343 - root - INFO - CF Training: Epoch 0001 Iter 0060 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:56,567 - root - INFO - CF Training: Epoch 0001 Iter 0061 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:56,789 - root - INFO - CF Training: Epoch 0001 Iter 0062 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:57,018 - root - INFO - CF Training: Epoch 0001 Iter 0063 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:57,238 - root - INFO - CF Training: Epoch 0001 Iter 0064 / 0638 | Time 0.2s | Iter Loss 0.0240 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:57,461 - root - INFO - CF Training: Epoch 0001 Iter 0065 / 0638 | Time 0.2s | Iter Loss 0.0288 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:57,684 - root - INFO - CF Training: Epoch 0001 Iter 0066 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:57,906 - root - INFO - CF Training: Epoch 0001 Iter 0067 / 0638 | Time 0.2s | Iter Loss 0.0240 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:58,132 - root - INFO - CF Training: Epoch 0001 Iter 0068 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:58,353 - root - INFO - CF Training: Epoch 0001 Iter 0069 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:58,574 - root - INFO - CF Training: Epoch 0001 Iter 0070 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:58,794 - root - INFO - CF Training: Epoch 0001 Iter 0071 / 0638 | Time 0.2s | Iter Loss 0.0246 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:59,016 - root - INFO - CF Training: Epoch 0001 Iter 0072 / 0638 | Time 0.2s | Iter Loss 0.0270 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:59,235 - root - INFO - CF Training: Epoch 0001 Iter 0073 / 0638 | Time 0.2s | Iter Loss 0.0281 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:59,457 - root - INFO - CF Training: Epoch 0001 Iter 0074 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:37:59,679 - root - INFO - CF Training: Epoch 0001 Iter 0075 / 0638 | Time 0.2s | Iter Loss 0.0294 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:37:59,901 - root - INFO - CF Training: Epoch 0001 Iter 0076 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:00,122 - root - INFO - CF Training: Epoch 0001 Iter 0077 / 0638 | Time 0.2s | Iter Loss 0.0280 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:00,342 - root - INFO - CF Training: Epoch 0001 Iter 0078 / 0638 | Time 0.2s | Iter Loss 0.0273 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:00,563 - root - INFO - CF Training: Epoch 0001 Iter 0079 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:00,784 - root - INFO - CF Training: Epoch 0001 Iter 0080 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:01,004 - root - INFO - CF Training: Epoch 0001 Iter 0081 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:01,225 - root - INFO - CF Training: Epoch 0001 Iter 0082 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:01,447 - root - INFO - CF Training: Epoch 0001 Iter 0083 / 0638 | Time 0.2s | Iter Loss 0.0263 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:01,668 - root - INFO - CF Training: Epoch 0001 Iter 0084 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0251\n",
      "2024-11-14 04:38:01,889 - root - INFO - CF Training: Epoch 0001 Iter 0085 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:02,111 - root - INFO - CF Training: Epoch 0001 Iter 0086 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:02,332 - root - INFO - CF Training: Epoch 0001 Iter 0087 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:02,555 - root - INFO - CF Training: Epoch 0001 Iter 0088 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:02,776 - root - INFO - CF Training: Epoch 0001 Iter 0089 / 0638 | Time 0.2s | Iter Loss 0.0244 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:02,998 - root - INFO - CF Training: Epoch 0001 Iter 0090 / 0638 | Time 0.2s | Iter Loss 0.0267 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:03,219 - root - INFO - CF Training: Epoch 0001 Iter 0091 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:03,440 - root - INFO - CF Training: Epoch 0001 Iter 0092 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:03,662 - root - INFO - CF Training: Epoch 0001 Iter 0093 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:03,884 - root - INFO - CF Training: Epoch 0001 Iter 0094 / 0638 | Time 0.2s | Iter Loss 0.0278 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:04,110 - root - INFO - CF Training: Epoch 0001 Iter 0095 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:04,331 - root - INFO - CF Training: Epoch 0001 Iter 0096 / 0638 | Time 0.2s | Iter Loss 0.0273 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:04,554 - root - INFO - CF Training: Epoch 0001 Iter 0097 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:04,775 - root - INFO - CF Training: Epoch 0001 Iter 0098 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:04,996 - root - INFO - CF Training: Epoch 0001 Iter 0099 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:05,218 - root - INFO - CF Training: Epoch 0001 Iter 0100 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:05,440 - root - INFO - CF Training: Epoch 0001 Iter 0101 / 0638 | Time 0.2s | Iter Loss 0.0268 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:05,662 - root - INFO - CF Training: Epoch 0001 Iter 0102 / 0638 | Time 0.2s | Iter Loss 0.0266 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:05,884 - root - INFO - CF Training: Epoch 0001 Iter 0103 / 0638 | Time 0.2s | Iter Loss 0.0308 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:06,108 - root - INFO - CF Training: Epoch 0001 Iter 0104 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:06,330 - root - INFO - CF Training: Epoch 0001 Iter 0105 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:06,552 - root - INFO - CF Training: Epoch 0001 Iter 0106 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0250\n",
      "2024-11-14 04:38:06,773 - root - INFO - CF Training: Epoch 0001 Iter 0107 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:06,995 - root - INFO - CF Training: Epoch 0001 Iter 0108 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:07,217 - root - INFO - CF Training: Epoch 0001 Iter 0109 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:07,439 - root - INFO - CF Training: Epoch 0001 Iter 0110 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:07,661 - root - INFO - CF Training: Epoch 0001 Iter 0111 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:07,883 - root - INFO - CF Training: Epoch 0001 Iter 0112 / 0638 | Time 0.2s | Iter Loss 0.0222 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:08,105 - root - INFO - CF Training: Epoch 0001 Iter 0113 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:08,329 - root - INFO - CF Training: Epoch 0001 Iter 0114 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:08,550 - root - INFO - CF Training: Epoch 0001 Iter 0115 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:08,771 - root - INFO - CF Training: Epoch 0001 Iter 0116 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:08,996 - root - INFO - CF Training: Epoch 0001 Iter 0117 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:09,220 - root - INFO - CF Training: Epoch 0001 Iter 0118 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:09,444 - root - INFO - CF Training: Epoch 0001 Iter 0119 / 0638 | Time 0.2s | Iter Loss 0.0285 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:09,669 - root - INFO - CF Training: Epoch 0001 Iter 0120 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:09,891 - root - INFO - CF Training: Epoch 0001 Iter 0121 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:10,112 - root - INFO - CF Training: Epoch 0001 Iter 0122 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0249\n",
      "2024-11-14 04:38:10,336 - root - INFO - CF Training: Epoch 0001 Iter 0123 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:10,559 - root - INFO - CF Training: Epoch 0001 Iter 0124 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:10,782 - root - INFO - CF Training: Epoch 0001 Iter 0125 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:11,003 - root - INFO - CF Training: Epoch 0001 Iter 0126 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:11,227 - root - INFO - CF Training: Epoch 0001 Iter 0127 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:11,449 - root - INFO - CF Training: Epoch 0001 Iter 0128 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:11,671 - root - INFO - CF Training: Epoch 0001 Iter 0129 / 0638 | Time 0.2s | Iter Loss 0.0290 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:11,893 - root - INFO - CF Training: Epoch 0001 Iter 0130 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:12,115 - root - INFO - CF Training: Epoch 0001 Iter 0131 / 0638 | Time 0.2s | Iter Loss 0.0263 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:12,337 - root - INFO - CF Training: Epoch 0001 Iter 0132 / 0638 | Time 0.2s | Iter Loss 0.0197 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:12,558 - root - INFO - CF Training: Epoch 0001 Iter 0133 / 0638 | Time 0.2s | Iter Loss 0.0254 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:12,780 - root - INFO - CF Training: Epoch 0001 Iter 0134 / 0638 | Time 0.2s | Iter Loss 0.0222 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:13,002 - root - INFO - CF Training: Epoch 0001 Iter 0135 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:13,226 - root - INFO - CF Training: Epoch 0001 Iter 0136 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0248\n",
      "2024-11-14 04:38:13,449 - root - INFO - CF Training: Epoch 0001 Iter 0137 / 0638 | Time 0.2s | Iter Loss 0.0195 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:13,673 - root - INFO - CF Training: Epoch 0001 Iter 0138 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:13,895 - root - INFO - CF Training: Epoch 0001 Iter 0139 / 0638 | Time 0.2s | Iter Loss 0.0241 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:14,118 - root - INFO - CF Training: Epoch 0001 Iter 0140 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:14,342 - root - INFO - CF Training: Epoch 0001 Iter 0141 / 0638 | Time 0.2s | Iter Loss 0.0258 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:14,565 - root - INFO - CF Training: Epoch 0001 Iter 0142 / 0638 | Time 0.2s | Iter Loss 0.0215 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:14,788 - root - INFO - CF Training: Epoch 0001 Iter 0143 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:15,009 - root - INFO - CF Training: Epoch 0001 Iter 0144 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:15,231 - root - INFO - CF Training: Epoch 0001 Iter 0145 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:15,456 - root - INFO - CF Training: Epoch 0001 Iter 0146 / 0638 | Time 0.2s | Iter Loss 0.0259 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:15,684 - root - INFO - CF Training: Epoch 0001 Iter 0147 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:15,905 - root - INFO - CF Training: Epoch 0001 Iter 0148 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:16,126 - root - INFO - CF Training: Epoch 0001 Iter 0149 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:16,348 - root - INFO - CF Training: Epoch 0001 Iter 0150 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:16,569 - root - INFO - CF Training: Epoch 0001 Iter 0151 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:16,793 - root - INFO - CF Training: Epoch 0001 Iter 0152 / 0638 | Time 0.2s | Iter Loss 0.0288 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:17,014 - root - INFO - CF Training: Epoch 0001 Iter 0153 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:17,235 - root - INFO - CF Training: Epoch 0001 Iter 0154 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:17,457 - root - INFO - CF Training: Epoch 0001 Iter 0155 / 0638 | Time 0.2s | Iter Loss 0.0196 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:17,679 - root - INFO - CF Training: Epoch 0001 Iter 0156 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:17,903 - root - INFO - CF Training: Epoch 0001 Iter 0157 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:18,125 - root - INFO - CF Training: Epoch 0001 Iter 0158 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:18,347 - root - INFO - CF Training: Epoch 0001 Iter 0159 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:18,570 - root - INFO - CF Training: Epoch 0001 Iter 0160 / 0638 | Time 0.2s | Iter Loss 0.0222 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:18,792 - root - INFO - CF Training: Epoch 0001 Iter 0161 / 0638 | Time 0.2s | Iter Loss 0.0257 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:19,014 - root - INFO - CF Training: Epoch 0001 Iter 0162 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:19,235 - root - INFO - CF Training: Epoch 0001 Iter 0163 / 0638 | Time 0.2s | Iter Loss 0.0305 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:19,460 - root - INFO - CF Training: Epoch 0001 Iter 0164 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:19,682 - root - INFO - CF Training: Epoch 0001 Iter 0165 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:19,904 - root - INFO - CF Training: Epoch 0001 Iter 0166 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:20,127 - root - INFO - CF Training: Epoch 0001 Iter 0167 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:20,348 - root - INFO - CF Training: Epoch 0001 Iter 0168 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:20,570 - root - INFO - CF Training: Epoch 0001 Iter 0169 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:20,792 - root - INFO - CF Training: Epoch 0001 Iter 0170 / 0638 | Time 0.2s | Iter Loss 0.0309 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:21,014 - root - INFO - CF Training: Epoch 0001 Iter 0171 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:21,236 - root - INFO - CF Training: Epoch 0001 Iter 0172 / 0638 | Time 0.2s | Iter Loss 0.0267 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:21,458 - root - INFO - CF Training: Epoch 0001 Iter 0173 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0247\n",
      "2024-11-14 04:38:21,680 - root - INFO - CF Training: Epoch 0001 Iter 0174 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:21,902 - root - INFO - CF Training: Epoch 0001 Iter 0175 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:22,125 - root - INFO - CF Training: Epoch 0001 Iter 0176 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:22,347 - root - INFO - CF Training: Epoch 0001 Iter 0177 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:22,570 - root - INFO - CF Training: Epoch 0001 Iter 0178 / 0638 | Time 0.2s | Iter Loss 0.0261 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:22,792 - root - INFO - CF Training: Epoch 0001 Iter 0179 / 0638 | Time 0.2s | Iter Loss 0.0266 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:23,015 - root - INFO - CF Training: Epoch 0001 Iter 0180 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:23,238 - root - INFO - CF Training: Epoch 0001 Iter 0181 / 0638 | Time 0.2s | Iter Loss 0.0286 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:23,460 - root - INFO - CF Training: Epoch 0001 Iter 0182 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:23,683 - root - INFO - CF Training: Epoch 0001 Iter 0183 / 0638 | Time 0.2s | Iter Loss 0.0234 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:23,904 - root - INFO - CF Training: Epoch 0001 Iter 0184 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:24,126 - root - INFO - CF Training: Epoch 0001 Iter 0185 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:24,346 - root - INFO - CF Training: Epoch 0001 Iter 0186 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:24,568 - root - INFO - CF Training: Epoch 0001 Iter 0187 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:24,788 - root - INFO - CF Training: Epoch 0001 Iter 0188 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:25,011 - root - INFO - CF Training: Epoch 0001 Iter 0189 / 0638 | Time 0.2s | Iter Loss 0.0203 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:25,234 - root - INFO - CF Training: Epoch 0001 Iter 0190 / 0638 | Time 0.2s | Iter Loss 0.0238 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:25,457 - root - INFO - CF Training: Epoch 0001 Iter 0191 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:25,681 - root - INFO - CF Training: Epoch 0001 Iter 0192 / 0638 | Time 0.2s | Iter Loss 0.0244 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:25,903 - root - INFO - CF Training: Epoch 0001 Iter 0193 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:26,126 - root - INFO - CF Training: Epoch 0001 Iter 0194 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:26,349 - root - INFO - CF Training: Epoch 0001 Iter 0195 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:26,571 - root - INFO - CF Training: Epoch 0001 Iter 0196 / 0638 | Time 0.2s | Iter Loss 0.0244 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:26,794 - root - INFO - CF Training: Epoch 0001 Iter 0197 / 0638 | Time 0.2s | Iter Loss 0.0260 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:27,015 - root - INFO - CF Training: Epoch 0001 Iter 0198 / 0638 | Time 0.2s | Iter Loss 0.0259 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:27,237 - root - INFO - CF Training: Epoch 0001 Iter 0199 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0246\n",
      "2024-11-14 04:38:27,458 - root - INFO - CF Training: Epoch 0001 Iter 0200 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:27,680 - root - INFO - CF Training: Epoch 0001 Iter 0201 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:27,901 - root - INFO - CF Training: Epoch 0001 Iter 0202 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:28,122 - root - INFO - CF Training: Epoch 0001 Iter 0203 / 0638 | Time 0.2s | Iter Loss 0.0200 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:28,344 - root - INFO - CF Training: Epoch 0001 Iter 0204 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:28,566 - root - INFO - CF Training: Epoch 0001 Iter 0205 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:28,788 - root - INFO - CF Training: Epoch 0001 Iter 0206 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:29,009 - root - INFO - CF Training: Epoch 0001 Iter 0207 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:29,230 - root - INFO - CF Training: Epoch 0001 Iter 0208 / 0638 | Time 0.2s | Iter Loss 0.0245 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:29,452 - root - INFO - CF Training: Epoch 0001 Iter 0209 / 0638 | Time 0.2s | Iter Loss 0.0267 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:29,673 - root - INFO - CF Training: Epoch 0001 Iter 0210 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:29,895 - root - INFO - CF Training: Epoch 0001 Iter 0211 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:30,115 - root - INFO - CF Training: Epoch 0001 Iter 0212 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0245\n",
      "2024-11-14 04:38:30,337 - root - INFO - CF Training: Epoch 0001 Iter 0213 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:30,558 - root - INFO - CF Training: Epoch 0001 Iter 0214 / 0638 | Time 0.2s | Iter Loss 0.0240 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:30,781 - root - INFO - CF Training: Epoch 0001 Iter 0215 / 0638 | Time 0.2s | Iter Loss 0.0197 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:31,003 - root - INFO - CF Training: Epoch 0001 Iter 0216 / 0638 | Time 0.2s | Iter Loss 0.0275 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:31,225 - root - INFO - CF Training: Epoch 0001 Iter 0217 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:31,448 - root - INFO - CF Training: Epoch 0001 Iter 0218 / 0638 | Time 0.2s | Iter Loss 0.0222 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:31,670 - root - INFO - CF Training: Epoch 0001 Iter 0219 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:31,892 - root - INFO - CF Training: Epoch 0001 Iter 0220 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:32,116 - root - INFO - CF Training: Epoch 0001 Iter 0221 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:32,340 - root - INFO - CF Training: Epoch 0001 Iter 0222 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:32,567 - root - INFO - CF Training: Epoch 0001 Iter 0223 / 0638 | Time 0.2s | Iter Loss 0.0287 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:32,789 - root - INFO - CF Training: Epoch 0001 Iter 0224 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:33,011 - root - INFO - CF Training: Epoch 0001 Iter 0225 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:33,233 - root - INFO - CF Training: Epoch 0001 Iter 0226 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:33,455 - root - INFO - CF Training: Epoch 0001 Iter 0227 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:33,677 - root - INFO - CF Training: Epoch 0001 Iter 0228 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:33,898 - root - INFO - CF Training: Epoch 0001 Iter 0229 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:34,120 - root - INFO - CF Training: Epoch 0001 Iter 0230 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:34,341 - root - INFO - CF Training: Epoch 0001 Iter 0231 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:34,565 - root - INFO - CF Training: Epoch 0001 Iter 0232 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:34,788 - root - INFO - CF Training: Epoch 0001 Iter 0233 / 0638 | Time 0.2s | Iter Loss 0.0271 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:35,009 - root - INFO - CF Training: Epoch 0001 Iter 0234 / 0638 | Time 0.2s | Iter Loss 0.0276 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:35,230 - root - INFO - CF Training: Epoch 0001 Iter 0235 / 0638 | Time 0.2s | Iter Loss 0.0204 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:35,452 - root - INFO - CF Training: Epoch 0001 Iter 0236 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0244\n",
      "2024-11-14 04:38:35,673 - root - INFO - CF Training: Epoch 0001 Iter 0237 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:35,894 - root - INFO - CF Training: Epoch 0001 Iter 0238 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:36,116 - root - INFO - CF Training: Epoch 0001 Iter 0239 / 0638 | Time 0.2s | Iter Loss 0.0204 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:36,339 - root - INFO - CF Training: Epoch 0001 Iter 0240 / 0638 | Time 0.2s | Iter Loss 0.0302 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:36,562 - root - INFO - CF Training: Epoch 0001 Iter 0241 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:36,785 - root - INFO - CF Training: Epoch 0001 Iter 0242 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:37,008 - root - INFO - CF Training: Epoch 0001 Iter 0243 / 0638 | Time 0.2s | Iter Loss 0.0200 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:37,232 - root - INFO - CF Training: Epoch 0001 Iter 0244 / 0638 | Time 0.2s | Iter Loss 0.0196 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:37,456 - root - INFO - CF Training: Epoch 0001 Iter 0245 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:37,678 - root - INFO - CF Training: Epoch 0001 Iter 0246 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:37,901 - root - INFO - CF Training: Epoch 0001 Iter 0247 / 0638 | Time 0.2s | Iter Loss 0.0211 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:38,121 - root - INFO - CF Training: Epoch 0001 Iter 0248 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:38,344 - root - INFO - CF Training: Epoch 0001 Iter 0249 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0243\n",
      "2024-11-14 04:38:38,567 - root - INFO - CF Training: Epoch 0001 Iter 0250 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:38,789 - root - INFO - CF Training: Epoch 0001 Iter 0251 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:39,012 - root - INFO - CF Training: Epoch 0001 Iter 0252 / 0638 | Time 0.2s | Iter Loss 0.0207 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:39,234 - root - INFO - CF Training: Epoch 0001 Iter 0253 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:39,456 - root - INFO - CF Training: Epoch 0001 Iter 0254 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:39,679 - root - INFO - CF Training: Epoch 0001 Iter 0255 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:39,903 - root - INFO - CF Training: Epoch 0001 Iter 0256 / 0638 | Time 0.2s | Iter Loss 0.0215 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:40,125 - root - INFO - CF Training: Epoch 0001 Iter 0257 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:40,345 - root - INFO - CF Training: Epoch 0001 Iter 0258 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:40,567 - root - INFO - CF Training: Epoch 0001 Iter 0259 / 0638 | Time 0.2s | Iter Loss 0.0244 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:40,791 - root - INFO - CF Training: Epoch 0001 Iter 0260 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:41,012 - root - INFO - CF Training: Epoch 0001 Iter 0261 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:41,233 - root - INFO - CF Training: Epoch 0001 Iter 0262 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:41,455 - root - INFO - CF Training: Epoch 0001 Iter 0263 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:41,675 - root - INFO - CF Training: Epoch 0001 Iter 0264 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:41,897 - root - INFO - CF Training: Epoch 0001 Iter 0265 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:42,120 - root - INFO - CF Training: Epoch 0001 Iter 0266 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:42,344 - root - INFO - CF Training: Epoch 0001 Iter 0267 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:42,565 - root - INFO - CF Training: Epoch 0001 Iter 0268 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:42,788 - root - INFO - CF Training: Epoch 0001 Iter 0269 / 0638 | Time 0.2s | Iter Loss 0.0261 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:43,008 - root - INFO - CF Training: Epoch 0001 Iter 0270 / 0638 | Time 0.2s | Iter Loss 0.0265 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:43,230 - root - INFO - CF Training: Epoch 0001 Iter 0271 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:43,453 - root - INFO - CF Training: Epoch 0001 Iter 0272 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:43,675 - root - INFO - CF Training: Epoch 0001 Iter 0273 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:43,897 - root - INFO - CF Training: Epoch 0001 Iter 0274 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:44,119 - root - INFO - CF Training: Epoch 0001 Iter 0275 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:44,341 - root - INFO - CF Training: Epoch 0001 Iter 0276 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:44,562 - root - INFO - CF Training: Epoch 0001 Iter 0277 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:44,785 - root - INFO - CF Training: Epoch 0001 Iter 0278 / 0638 | Time 0.2s | Iter Loss 0.0208 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:45,007 - root - INFO - CF Training: Epoch 0001 Iter 0279 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:45,228 - root - INFO - CF Training: Epoch 0001 Iter 0280 / 0638 | Time 0.2s | Iter Loss 0.0254 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:45,450 - root - INFO - CF Training: Epoch 0001 Iter 0281 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:45,672 - root - INFO - CF Training: Epoch 0001 Iter 0282 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0242\n",
      "2024-11-14 04:38:45,893 - root - INFO - CF Training: Epoch 0001 Iter 0283 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:46,116 - root - INFO - CF Training: Epoch 0001 Iter 0284 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:46,338 - root - INFO - CF Training: Epoch 0001 Iter 0285 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:46,558 - root - INFO - CF Training: Epoch 0001 Iter 0286 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:46,780 - root - INFO - CF Training: Epoch 0001 Iter 0287 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:47,000 - root - INFO - CF Training: Epoch 0001 Iter 0288 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:47,222 - root - INFO - CF Training: Epoch 0001 Iter 0289 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:47,443 - root - INFO - CF Training: Epoch 0001 Iter 0290 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:47,664 - root - INFO - CF Training: Epoch 0001 Iter 0291 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:47,887 - root - INFO - CF Training: Epoch 0001 Iter 0292 / 0638 | Time 0.2s | Iter Loss 0.0285 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:48,110 - root - INFO - CF Training: Epoch 0001 Iter 0293 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:48,332 - root - INFO - CF Training: Epoch 0001 Iter 0294 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:48,555 - root - INFO - CF Training: Epoch 0001 Iter 0295 / 0638 | Time 0.2s | Iter Loss 0.0188 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:48,779 - root - INFO - CF Training: Epoch 0001 Iter 0296 / 0638 | Time 0.2s | Iter Loss 0.0222 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:49,002 - root - INFO - CF Training: Epoch 0001 Iter 0297 / 0638 | Time 0.2s | Iter Loss 0.0234 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:49,224 - root - INFO - CF Training: Epoch 0001 Iter 0298 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:49,444 - root - INFO - CF Training: Epoch 0001 Iter 0299 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:49,664 - root - INFO - CF Training: Epoch 0001 Iter 0300 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:49,888 - root - INFO - CF Training: Epoch 0001 Iter 0301 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:50,109 - root - INFO - CF Training: Epoch 0001 Iter 0302 / 0638 | Time 0.2s | Iter Loss 0.0267 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:50,331 - root - INFO - CF Training: Epoch 0001 Iter 0303 / 0638 | Time 0.2s | Iter Loss 0.0207 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:50,551 - root - INFO - CF Training: Epoch 0001 Iter 0304 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:50,774 - root - INFO - CF Training: Epoch 0001 Iter 0305 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:50,997 - root - INFO - CF Training: Epoch 0001 Iter 0306 / 0638 | Time 0.2s | Iter Loss 0.0179 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:51,218 - root - INFO - CF Training: Epoch 0001 Iter 0307 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0241\n",
      "2024-11-14 04:38:51,441 - root - INFO - CF Training: Epoch 0001 Iter 0308 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:51,663 - root - INFO - CF Training: Epoch 0001 Iter 0309 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:51,886 - root - INFO - CF Training: Epoch 0001 Iter 0310 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:52,107 - root - INFO - CF Training: Epoch 0001 Iter 0311 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:52,330 - root - INFO - CF Training: Epoch 0001 Iter 0312 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:52,553 - root - INFO - CF Training: Epoch 0001 Iter 0313 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:52,775 - root - INFO - CF Training: Epoch 0001 Iter 0314 / 0638 | Time 0.2s | Iter Loss 0.0194 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:52,996 - root - INFO - CF Training: Epoch 0001 Iter 0315 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:53,220 - root - INFO - CF Training: Epoch 0001 Iter 0316 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:53,443 - root - INFO - CF Training: Epoch 0001 Iter 0317 / 0638 | Time 0.2s | Iter Loss 0.0201 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:53,666 - root - INFO - CF Training: Epoch 0001 Iter 0318 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:53,889 - root - INFO - CF Training: Epoch 0001 Iter 0319 / 0638 | Time 0.2s | Iter Loss 0.0280 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:54,112 - root - INFO - CF Training: Epoch 0001 Iter 0320 / 0638 | Time 0.2s | Iter Loss 0.0263 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:54,336 - root - INFO - CF Training: Epoch 0001 Iter 0321 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:54,558 - root - INFO - CF Training: Epoch 0001 Iter 0322 / 0638 | Time 0.2s | Iter Loss 0.0241 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:54,782 - root - INFO - CF Training: Epoch 0001 Iter 0323 / 0638 | Time 0.2s | Iter Loss 0.0278 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:55,005 - root - INFO - CF Training: Epoch 0001 Iter 0324 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:55,227 - root - INFO - CF Training: Epoch 0001 Iter 0325 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:55,450 - root - INFO - CF Training: Epoch 0001 Iter 0326 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:55,673 - root - INFO - CF Training: Epoch 0001 Iter 0327 / 0638 | Time 0.2s | Iter Loss 0.0207 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:55,895 - root - INFO - CF Training: Epoch 0001 Iter 0328 / 0638 | Time 0.2s | Iter Loss 0.0298 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:56,117 - root - INFO - CF Training: Epoch 0001 Iter 0329 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:56,338 - root - INFO - CF Training: Epoch 0001 Iter 0330 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:56,562 - root - INFO - CF Training: Epoch 0001 Iter 0331 / 0638 | Time 0.2s | Iter Loss 0.0272 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:56,785 - root - INFO - CF Training: Epoch 0001 Iter 0332 / 0638 | Time 0.2s | Iter Loss 0.0245 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:57,008 - root - INFO - CF Training: Epoch 0001 Iter 0333 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:57,231 - root - INFO - CF Training: Epoch 0001 Iter 0334 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:57,455 - root - INFO - CF Training: Epoch 0001 Iter 0335 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:57,677 - root - INFO - CF Training: Epoch 0001 Iter 0336 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:57,900 - root - INFO - CF Training: Epoch 0001 Iter 0337 / 0638 | Time 0.2s | Iter Loss 0.0238 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:58,123 - root - INFO - CF Training: Epoch 0001 Iter 0338 / 0638 | Time 0.2s | Iter Loss 0.0241 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:58,345 - root - INFO - CF Training: Epoch 0001 Iter 0339 / 0638 | Time 0.2s | Iter Loss 0.0281 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:58,568 - root - INFO - CF Training: Epoch 0001 Iter 0340 / 0638 | Time 0.2s | Iter Loss 0.0211 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:58,788 - root - INFO - CF Training: Epoch 0001 Iter 0341 / 0638 | Time 0.2s | Iter Loss 0.0241 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:59,010 - root - INFO - CF Training: Epoch 0001 Iter 0342 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:59,232 - root - INFO - CF Training: Epoch 0001 Iter 0343 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:59,456 - root - INFO - CF Training: Epoch 0001 Iter 0344 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:59,677 - root - INFO - CF Training: Epoch 0001 Iter 0345 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:38:59,899 - root - INFO - CF Training: Epoch 0001 Iter 0346 / 0638 | Time 0.2s | Iter Loss 0.0234 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:00,125 - root - INFO - CF Training: Epoch 0001 Iter 0347 / 0638 | Time 0.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:00,347 - root - INFO - CF Training: Epoch 0001 Iter 0348 / 0638 | Time 0.2s | Iter Loss 0.0277 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:00,568 - root - INFO - CF Training: Epoch 0001 Iter 0349 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:00,801 - root - INFO - CF Training: Epoch 0001 Iter 0350 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:01,023 - root - INFO - CF Training: Epoch 0001 Iter 0351 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:01,249 - root - INFO - CF Training: Epoch 0001 Iter 0352 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:01,471 - root - INFO - CF Training: Epoch 0001 Iter 0353 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:01,694 - root - INFO - CF Training: Epoch 0001 Iter 0354 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:01,916 - root - INFO - CF Training: Epoch 0001 Iter 0355 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:02,139 - root - INFO - CF Training: Epoch 0001 Iter 0356 / 0638 | Time 0.2s | Iter Loss 0.0260 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:02,366 - root - INFO - CF Training: Epoch 0001 Iter 0357 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:02,588 - root - INFO - CF Training: Epoch 0001 Iter 0358 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:02,811 - root - INFO - CF Training: Epoch 0001 Iter 0359 / 0638 | Time 0.2s | Iter Loss 0.0215 | Iter Mean Loss 0.0240\n",
      "2024-11-14 04:39:03,032 - root - INFO - CF Training: Epoch 0001 Iter 0360 / 0638 | Time 0.2s | Iter Loss 0.0211 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:03,254 - root - INFO - CF Training: Epoch 0001 Iter 0361 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:03,479 - root - INFO - CF Training: Epoch 0001 Iter 0362 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:03,703 - root - INFO - CF Training: Epoch 0001 Iter 0363 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:03,923 - root - INFO - CF Training: Epoch 0001 Iter 0364 / 0638 | Time 0.2s | Iter Loss 0.0208 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:04,146 - root - INFO - CF Training: Epoch 0001 Iter 0365 / 0638 | Time 0.2s | Iter Loss 0.0159 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:04,369 - root - INFO - CF Training: Epoch 0001 Iter 0366 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:04,592 - root - INFO - CF Training: Epoch 0001 Iter 0367 / 0638 | Time 0.2s | Iter Loss 0.0203 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:04,814 - root - INFO - CF Training: Epoch 0001 Iter 0368 / 0638 | Time 0.2s | Iter Loss 0.0267 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:05,039 - root - INFO - CF Training: Epoch 0001 Iter 0369 / 0638 | Time 0.2s | Iter Loss 0.0261 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:05,261 - root - INFO - CF Training: Epoch 0001 Iter 0370 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:05,484 - root - INFO - CF Training: Epoch 0001 Iter 0371 / 0638 | Time 0.2s | Iter Loss 0.0188 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:05,706 - root - INFO - CF Training: Epoch 0001 Iter 0372 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:05,927 - root - INFO - CF Training: Epoch 0001 Iter 0373 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:06,150 - root - INFO - CF Training: Epoch 0001 Iter 0374 / 0638 | Time 0.2s | Iter Loss 0.0245 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:06,372 - root - INFO - CF Training: Epoch 0001 Iter 0375 / 0638 | Time 0.2s | Iter Loss 0.0222 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:06,596 - root - INFO - CF Training: Epoch 0001 Iter 0376 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:06,823 - root - INFO - CF Training: Epoch 0001 Iter 0377 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:07,046 - root - INFO - CF Training: Epoch 0001 Iter 0378 / 0638 | Time 0.2s | Iter Loss 0.0240 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:07,279 - root - INFO - CF Training: Epoch 0001 Iter 0379 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:07,500 - root - INFO - CF Training: Epoch 0001 Iter 0380 / 0638 | Time 0.2s | Iter Loss 0.0211 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:07,723 - root - INFO - CF Training: Epoch 0001 Iter 0381 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:07,945 - root - INFO - CF Training: Epoch 0001 Iter 0382 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:08,168 - root - INFO - CF Training: Epoch 0001 Iter 0383 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:08,390 - root - INFO - CF Training: Epoch 0001 Iter 0384 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:08,613 - root - INFO - CF Training: Epoch 0001 Iter 0385 / 0638 | Time 0.2s | Iter Loss 0.0192 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:08,833 - root - INFO - CF Training: Epoch 0001 Iter 0386 / 0638 | Time 0.2s | Iter Loss 0.0266 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:09,054 - root - INFO - CF Training: Epoch 0001 Iter 0387 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:09,275 - root - INFO - CF Training: Epoch 0001 Iter 0388 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:09,498 - root - INFO - CF Training: Epoch 0001 Iter 0389 / 0638 | Time 0.2s | Iter Loss 0.0259 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:09,721 - root - INFO - CF Training: Epoch 0001 Iter 0390 / 0638 | Time 0.2s | Iter Loss 0.0293 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:09,943 - root - INFO - CF Training: Epoch 0001 Iter 0391 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:10,164 - root - INFO - CF Training: Epoch 0001 Iter 0392 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:10,387 - root - INFO - CF Training: Epoch 0001 Iter 0393 / 0638 | Time 0.2s | Iter Loss 0.0279 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:10,608 - root - INFO - CF Training: Epoch 0001 Iter 0394 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:10,830 - root - INFO - CF Training: Epoch 0001 Iter 0395 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:11,050 - root - INFO - CF Training: Epoch 0001 Iter 0396 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:11,272 - root - INFO - CF Training: Epoch 0001 Iter 0397 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:11,492 - root - INFO - CF Training: Epoch 0001 Iter 0398 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:11,712 - root - INFO - CF Training: Epoch 0001 Iter 0399 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:11,933 - root - INFO - CF Training: Epoch 0001 Iter 0400 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:12,154 - root - INFO - CF Training: Epoch 0001 Iter 0401 / 0638 | Time 0.2s | Iter Loss 0.0245 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:12,378 - root - INFO - CF Training: Epoch 0001 Iter 0402 / 0638 | Time 0.2s | Iter Loss 0.0180 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:12,601 - root - INFO - CF Training: Epoch 0001 Iter 0403 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:12,822 - root - INFO - CF Training: Epoch 0001 Iter 0404 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:13,044 - root - INFO - CF Training: Epoch 0001 Iter 0405 / 0638 | Time 0.2s | Iter Loss 0.0278 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:13,266 - root - INFO - CF Training: Epoch 0001 Iter 0406 / 0638 | Time 0.2s | Iter Loss 0.0200 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:13,495 - root - INFO - CF Training: Epoch 0001 Iter 0407 / 0638 | Time 0.2s | Iter Loss 0.0226 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:13,716 - root - INFO - CF Training: Epoch 0001 Iter 0408 / 0638 | Time 0.2s | Iter Loss 0.0285 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:13,937 - root - INFO - CF Training: Epoch 0001 Iter 0409 / 0638 | Time 0.2s | Iter Loss 0.0187 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:14,160 - root - INFO - CF Training: Epoch 0001 Iter 0410 / 0638 | Time 0.2s | Iter Loss 0.0241 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:14,381 - root - INFO - CF Training: Epoch 0001 Iter 0411 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:14,603 - root - INFO - CF Training: Epoch 0001 Iter 0412 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:14,827 - root - INFO - CF Training: Epoch 0001 Iter 0413 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:15,050 - root - INFO - CF Training: Epoch 0001 Iter 0414 / 0638 | Time 0.2s | Iter Loss 0.0188 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:15,271 - root - INFO - CF Training: Epoch 0001 Iter 0415 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:15,494 - root - INFO - CF Training: Epoch 0001 Iter 0416 / 0638 | Time 0.2s | Iter Loss 0.0211 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:15,717 - root - INFO - CF Training: Epoch 0001 Iter 0417 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:15,940 - root - INFO - CF Training: Epoch 0001 Iter 0418 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0239\n",
      "2024-11-14 04:39:16,168 - root - INFO - CF Training: Epoch 0001 Iter 0419 / 0638 | Time 0.2s | Iter Loss 0.0203 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:16,389 - root - INFO - CF Training: Epoch 0001 Iter 0420 / 0638 | Time 0.2s | Iter Loss 0.0181 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:16,620 - root - INFO - CF Training: Epoch 0001 Iter 0421 / 0638 | Time 0.2s | Iter Loss 0.0274 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:16,842 - root - INFO - CF Training: Epoch 0001 Iter 0422 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:17,064 - root - INFO - CF Training: Epoch 0001 Iter 0423 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:17,285 - root - INFO - CF Training: Epoch 0001 Iter 0424 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:17,508 - root - INFO - CF Training: Epoch 0001 Iter 0425 / 0638 | Time 0.2s | Iter Loss 0.0194 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:17,732 - root - INFO - CF Training: Epoch 0001 Iter 0426 / 0638 | Time 0.2s | Iter Loss 0.0194 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:17,953 - root - INFO - CF Training: Epoch 0001 Iter 0427 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:18,176 - root - INFO - CF Training: Epoch 0001 Iter 0428 / 0638 | Time 0.2s | Iter Loss 0.0238 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:18,398 - root - INFO - CF Training: Epoch 0001 Iter 0429 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:18,620 - root - INFO - CF Training: Epoch 0001 Iter 0430 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:18,842 - root - INFO - CF Training: Epoch 0001 Iter 0431 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:19,064 - root - INFO - CF Training: Epoch 0001 Iter 0432 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:19,289 - root - INFO - CF Training: Epoch 0001 Iter 0433 / 0638 | Time 0.2s | Iter Loss 0.0184 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:19,511 - root - INFO - CF Training: Epoch 0001 Iter 0434 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:19,736 - root - INFO - CF Training: Epoch 0001 Iter 0435 / 0638 | Time 0.2s | Iter Loss 0.0258 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:19,956 - root - INFO - CF Training: Epoch 0001 Iter 0436 / 0638 | Time 0.2s | Iter Loss 0.0194 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:20,179 - root - INFO - CF Training: Epoch 0001 Iter 0437 / 0638 | Time 0.2s | Iter Loss 0.0265 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:20,404 - root - INFO - CF Training: Epoch 0001 Iter 0438 / 0638 | Time 0.2s | Iter Loss 0.0175 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:20,626 - root - INFO - CF Training: Epoch 0001 Iter 0439 / 0638 | Time 0.2s | Iter Loss 0.0198 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:20,849 - root - INFO - CF Training: Epoch 0001 Iter 0440 / 0638 | Time 0.2s | Iter Loss 0.0261 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:21,071 - root - INFO - CF Training: Epoch 0001 Iter 0441 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:21,294 - root - INFO - CF Training: Epoch 0001 Iter 0442 / 0638 | Time 0.2s | Iter Loss 0.0265 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:21,517 - root - INFO - CF Training: Epoch 0001 Iter 0443 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:21,740 - root - INFO - CF Training: Epoch 0001 Iter 0444 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:21,963 - root - INFO - CF Training: Epoch 0001 Iter 0445 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:22,185 - root - INFO - CF Training: Epoch 0001 Iter 0446 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:22,407 - root - INFO - CF Training: Epoch 0001 Iter 0447 / 0638 | Time 0.2s | Iter Loss 0.0246 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:22,628 - root - INFO - CF Training: Epoch 0001 Iter 0448 / 0638 | Time 0.2s | Iter Loss 0.0283 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:22,850 - root - INFO - CF Training: Epoch 0001 Iter 0449 / 0638 | Time 0.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:23,071 - root - INFO - CF Training: Epoch 0001 Iter 0450 / 0638 | Time 0.2s | Iter Loss 0.0247 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:23,293 - root - INFO - CF Training: Epoch 0001 Iter 0451 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:23,515 - root - INFO - CF Training: Epoch 0001 Iter 0452 / 0638 | Time 0.2s | Iter Loss 0.0314 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:23,739 - root - INFO - CF Training: Epoch 0001 Iter 0453 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:23,961 - root - INFO - CF Training: Epoch 0001 Iter 0454 / 0638 | Time 0.2s | Iter Loss 0.0248 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:24,184 - root - INFO - CF Training: Epoch 0001 Iter 0455 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:24,407 - root - INFO - CF Training: Epoch 0001 Iter 0456 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:24,631 - root - INFO - CF Training: Epoch 0001 Iter 0457 / 0638 | Time 0.2s | Iter Loss 0.0286 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:24,854 - root - INFO - CF Training: Epoch 0001 Iter 0458 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:25,079 - root - INFO - CF Training: Epoch 0001 Iter 0459 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:25,302 - root - INFO - CF Training: Epoch 0001 Iter 0460 / 0638 | Time 0.2s | Iter Loss 0.0263 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:25,526 - root - INFO - CF Training: Epoch 0001 Iter 0461 / 0638 | Time 0.2s | Iter Loss 0.0197 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:25,748 - root - INFO - CF Training: Epoch 0001 Iter 0462 / 0638 | Time 0.2s | Iter Loss 0.0244 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:25,970 - root - INFO - CF Training: Epoch 0001 Iter 0463 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:26,191 - root - INFO - CF Training: Epoch 0001 Iter 0464 / 0638 | Time 0.2s | Iter Loss 0.0244 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:26,414 - root - INFO - CF Training: Epoch 0001 Iter 0465 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:26,635 - root - INFO - CF Training: Epoch 0001 Iter 0466 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:26,857 - root - INFO - CF Training: Epoch 0001 Iter 0467 / 0638 | Time 0.2s | Iter Loss 0.0259 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:27,080 - root - INFO - CF Training: Epoch 0001 Iter 0468 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:27,303 - root - INFO - CF Training: Epoch 0001 Iter 0469 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:27,526 - root - INFO - CF Training: Epoch 0001 Iter 0470 / 0638 | Time 0.2s | Iter Loss 0.0189 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:27,749 - root - INFO - CF Training: Epoch 0001 Iter 0471 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:27,972 - root - INFO - CF Training: Epoch 0001 Iter 0472 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:28,195 - root - INFO - CF Training: Epoch 0001 Iter 0473 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:28,418 - root - INFO - CF Training: Epoch 0001 Iter 0474 / 0638 | Time 0.2s | Iter Loss 0.0267 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:28,641 - root - INFO - CF Training: Epoch 0001 Iter 0475 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:28,863 - root - INFO - CF Training: Epoch 0001 Iter 0476 / 0638 | Time 0.2s | Iter Loss 0.0248 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:29,086 - root - INFO - CF Training: Epoch 0001 Iter 0477 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:29,314 - root - INFO - CF Training: Epoch 0001 Iter 0478 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:29,536 - root - INFO - CF Training: Epoch 0001 Iter 0479 / 0638 | Time 0.2s | Iter Loss 0.0193 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:29,761 - root - INFO - CF Training: Epoch 0001 Iter 0480 / 0638 | Time 0.2s | Iter Loss 0.0257 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:29,988 - root - INFO - CF Training: Epoch 0001 Iter 0481 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:30,211 - root - INFO - CF Training: Epoch 0001 Iter 0482 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:30,433 - root - INFO - CF Training: Epoch 0001 Iter 0483 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:30,655 - root - INFO - CF Training: Epoch 0001 Iter 0484 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:30,878 - root - INFO - CF Training: Epoch 0001 Iter 0485 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:31,100 - root - INFO - CF Training: Epoch 0001 Iter 0486 / 0638 | Time 0.2s | Iter Loss 0.0203 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:31,323 - root - INFO - CF Training: Epoch 0001 Iter 0487 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:31,548 - root - INFO - CF Training: Epoch 0001 Iter 0488 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:31,772 - root - INFO - CF Training: Epoch 0001 Iter 0489 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:31,998 - root - INFO - CF Training: Epoch 0001 Iter 0490 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0238\n",
      "2024-11-14 04:39:32,221 - root - INFO - CF Training: Epoch 0001 Iter 0491 / 0638 | Time 0.2s | Iter Loss 0.0189 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:32,443 - root - INFO - CF Training: Epoch 0001 Iter 0492 / 0638 | Time 0.2s | Iter Loss 0.0200 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:32,664 - root - INFO - CF Training: Epoch 0001 Iter 0493 / 0638 | Time 0.2s | Iter Loss 0.0249 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:32,883 - root - INFO - CF Training: Epoch 0001 Iter 0494 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:33,105 - root - INFO - CF Training: Epoch 0001 Iter 0495 / 0638 | Time 0.2s | Iter Loss 0.0176 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:33,327 - root - INFO - CF Training: Epoch 0001 Iter 0496 / 0638 | Time 0.2s | Iter Loss 0.0272 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:33,549 - root - INFO - CF Training: Epoch 0001 Iter 0497 / 0638 | Time 0.2s | Iter Loss 0.0219 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:33,770 - root - INFO - CF Training: Epoch 0001 Iter 0498 / 0638 | Time 0.2s | Iter Loss 0.0215 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:33,991 - root - INFO - CF Training: Epoch 0001 Iter 0499 / 0638 | Time 0.2s | Iter Loss 0.0208 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:34,211 - root - INFO - CF Training: Epoch 0001 Iter 0500 / 0638 | Time 0.2s | Iter Loss 0.0227 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:34,431 - root - INFO - CF Training: Epoch 0001 Iter 0501 / 0638 | Time 0.2s | Iter Loss 0.0200 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:34,652 - root - INFO - CF Training: Epoch 0001 Iter 0502 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:34,876 - root - INFO - CF Training: Epoch 0001 Iter 0503 / 0638 | Time 0.2s | Iter Loss 0.0260 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:35,099 - root - INFO - CF Training: Epoch 0001 Iter 0504 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:35,325 - root - INFO - CF Training: Epoch 0001 Iter 0505 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:35,546 - root - INFO - CF Training: Epoch 0001 Iter 0506 / 0638 | Time 0.2s | Iter Loss 0.0308 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:35,767 - root - INFO - CF Training: Epoch 0001 Iter 0507 / 0638 | Time 0.2s | Iter Loss 0.0256 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:35,990 - root - INFO - CF Training: Epoch 0001 Iter 0508 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:36,211 - root - INFO - CF Training: Epoch 0001 Iter 0509 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:36,432 - root - INFO - CF Training: Epoch 0001 Iter 0510 / 0638 | Time 0.2s | Iter Loss 0.0253 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:36,652 - root - INFO - CF Training: Epoch 0001 Iter 0511 / 0638 | Time 0.2s | Iter Loss 0.0232 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:36,878 - root - INFO - CF Training: Epoch 0001 Iter 0512 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:37,100 - root - INFO - CF Training: Epoch 0001 Iter 0513 / 0638 | Time 0.2s | Iter Loss 0.0191 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:37,325 - root - INFO - CF Training: Epoch 0001 Iter 0514 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:37,548 - root - INFO - CF Training: Epoch 0001 Iter 0515 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:37,771 - root - INFO - CF Training: Epoch 0001 Iter 0516 / 0638 | Time 0.2s | Iter Loss 0.0199 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:37,994 - root - INFO - CF Training: Epoch 0001 Iter 0517 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:38,217 - root - INFO - CF Training: Epoch 0001 Iter 0518 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:38,440 - root - INFO - CF Training: Epoch 0001 Iter 0519 / 0638 | Time 0.2s | Iter Loss 0.0204 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:38,664 - root - INFO - CF Training: Epoch 0001 Iter 0520 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:38,886 - root - INFO - CF Training: Epoch 0001 Iter 0521 / 0638 | Time 0.2s | Iter Loss 0.0187 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:39,108 - root - INFO - CF Training: Epoch 0001 Iter 0522 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:39,329 - root - INFO - CF Training: Epoch 0001 Iter 0523 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:39,552 - root - INFO - CF Training: Epoch 0001 Iter 0524 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:39,774 - root - INFO - CF Training: Epoch 0001 Iter 0525 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:39,994 - root - INFO - CF Training: Epoch 0001 Iter 0526 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:40,216 - root - INFO - CF Training: Epoch 0001 Iter 0527 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:40,437 - root - INFO - CF Training: Epoch 0001 Iter 0528 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:40,657 - root - INFO - CF Training: Epoch 0001 Iter 0529 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:40,879 - root - INFO - CF Training: Epoch 0001 Iter 0530 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:41,100 - root - INFO - CF Training: Epoch 0001 Iter 0531 / 0638 | Time 0.2s | Iter Loss 0.0207 | Iter Mean Loss 0.0237\n",
      "2024-11-14 04:39:41,321 - root - INFO - CF Training: Epoch 0001 Iter 0532 / 0638 | Time 0.2s | Iter Loss 0.0190 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:41,542 - root - INFO - CF Training: Epoch 0001 Iter 0533 / 0638 | Time 0.2s | Iter Loss 0.0254 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:41,764 - root - INFO - CF Training: Epoch 0001 Iter 0534 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:41,988 - root - INFO - CF Training: Epoch 0001 Iter 0535 / 0638 | Time 0.2s | Iter Loss 0.0246 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:42,210 - root - INFO - CF Training: Epoch 0001 Iter 0536 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:42,431 - root - INFO - CF Training: Epoch 0001 Iter 0537 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:42,653 - root - INFO - CF Training: Epoch 0001 Iter 0538 / 0638 | Time 0.2s | Iter Loss 0.0205 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:42,875 - root - INFO - CF Training: Epoch 0001 Iter 0539 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:43,096 - root - INFO - CF Training: Epoch 0001 Iter 0540 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:43,318 - root - INFO - CF Training: Epoch 0001 Iter 0541 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:43,538 - root - INFO - CF Training: Epoch 0001 Iter 0542 / 0638 | Time 0.2s | Iter Loss 0.0242 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:43,761 - root - INFO - CF Training: Epoch 0001 Iter 0543 / 0638 | Time 0.2s | Iter Loss 0.0209 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:43,984 - root - INFO - CF Training: Epoch 0001 Iter 0544 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:44,207 - root - INFO - CF Training: Epoch 0001 Iter 0545 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:44,428 - root - INFO - CF Training: Epoch 0001 Iter 0546 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:44,649 - root - INFO - CF Training: Epoch 0001 Iter 0547 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:44,872 - root - INFO - CF Training: Epoch 0001 Iter 0548 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:45,094 - root - INFO - CF Training: Epoch 0001 Iter 0549 / 0638 | Time 0.2s | Iter Loss 0.0193 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:45,316 - root - INFO - CF Training: Epoch 0001 Iter 0550 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:45,538 - root - INFO - CF Training: Epoch 0001 Iter 0551 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:45,761 - root - INFO - CF Training: Epoch 0001 Iter 0552 / 0638 | Time 0.2s | Iter Loss 0.0250 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:45,984 - root - INFO - CF Training: Epoch 0001 Iter 0553 / 0638 | Time 0.2s | Iter Loss 0.0181 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:46,206 - root - INFO - CF Training: Epoch 0001 Iter 0554 / 0638 | Time 0.2s | Iter Loss 0.0235 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:46,427 - root - INFO - CF Training: Epoch 0001 Iter 0555 / 0638 | Time 0.2s | Iter Loss 0.0264 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:46,650 - root - INFO - CF Training: Epoch 0001 Iter 0556 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:46,873 - root - INFO - CF Training: Epoch 0001 Iter 0557 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:47,094 - root - INFO - CF Training: Epoch 0001 Iter 0558 / 0638 | Time 0.2s | Iter Loss 0.0201 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:47,319 - root - INFO - CF Training: Epoch 0001 Iter 0559 / 0638 | Time 0.2s | Iter Loss 0.0192 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:47,540 - root - INFO - CF Training: Epoch 0001 Iter 0560 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:47,762 - root - INFO - CF Training: Epoch 0001 Iter 0561 / 0638 | Time 0.2s | Iter Loss 0.0207 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:47,984 - root - INFO - CF Training: Epoch 0001 Iter 0562 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:48,207 - root - INFO - CF Training: Epoch 0001 Iter 0563 / 0638 | Time 0.2s | Iter Loss 0.0191 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:48,429 - root - INFO - CF Training: Epoch 0001 Iter 0564 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:48,651 - root - INFO - CF Training: Epoch 0001 Iter 0565 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:48,874 - root - INFO - CF Training: Epoch 0001 Iter 0566 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:49,095 - root - INFO - CF Training: Epoch 0001 Iter 0567 / 0638 | Time 0.2s | Iter Loss 0.0289 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:49,317 - root - INFO - CF Training: Epoch 0001 Iter 0568 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:49,540 - root - INFO - CF Training: Epoch 0001 Iter 0569 / 0638 | Time 0.2s | Iter Loss 0.0212 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:49,763 - root - INFO - CF Training: Epoch 0001 Iter 0570 / 0638 | Time 0.2s | Iter Loss 0.0233 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:49,984 - root - INFO - CF Training: Epoch 0001 Iter 0571 / 0638 | Time 0.2s | Iter Loss 0.0223 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:50,207 - root - INFO - CF Training: Epoch 0001 Iter 0572 / 0638 | Time 0.2s | Iter Loss 0.0196 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:50,427 - root - INFO - CF Training: Epoch 0001 Iter 0573 / 0638 | Time 0.2s | Iter Loss 0.0217 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:50,649 - root - INFO - CF Training: Epoch 0001 Iter 0574 / 0638 | Time 0.2s | Iter Loss 0.0255 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:50,872 - root - INFO - CF Training: Epoch 0001 Iter 0575 / 0638 | Time 0.2s | Iter Loss 0.0259 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:51,094 - root - INFO - CF Training: Epoch 0001 Iter 0576 / 0638 | Time 0.2s | Iter Loss 0.0285 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:51,317 - root - INFO - CF Training: Epoch 0001 Iter 0577 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:51,538 - root - INFO - CF Training: Epoch 0001 Iter 0578 / 0638 | Time 0.2s | Iter Loss 0.0211 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:51,761 - root - INFO - CF Training: Epoch 0001 Iter 0579 / 0638 | Time 0.2s | Iter Loss 0.0201 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:51,984 - root - INFO - CF Training: Epoch 0001 Iter 0580 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:52,207 - root - INFO - CF Training: Epoch 0001 Iter 0581 / 0638 | Time 0.2s | Iter Loss 0.0231 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:52,429 - root - INFO - CF Training: Epoch 0001 Iter 0582 / 0638 | Time 0.2s | Iter Loss 0.0201 | Iter Mean Loss 0.0236\n",
      "2024-11-14 04:39:52,650 - root - INFO - CF Training: Epoch 0001 Iter 0583 / 0638 | Time 0.2s | Iter Loss 0.0179 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:52,872 - root - INFO - CF Training: Epoch 0001 Iter 0584 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:53,095 - root - INFO - CF Training: Epoch 0001 Iter 0585 / 0638 | Time 0.2s | Iter Loss 0.0215 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:53,322 - root - INFO - CF Training: Epoch 0001 Iter 0586 / 0638 | Time 0.2s | Iter Loss 0.0229 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:53,543 - root - INFO - CF Training: Epoch 0001 Iter 0587 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:53,766 - root - INFO - CF Training: Epoch 0001 Iter 0588 / 0638 | Time 0.2s | Iter Loss 0.0236 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:53,997 - root - INFO - CF Training: Epoch 0001 Iter 0589 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:54,228 - root - INFO - CF Training: Epoch 0001 Iter 0590 / 0638 | Time 0.2s | Iter Loss 0.0237 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:54,455 - root - INFO - CF Training: Epoch 0001 Iter 0591 / 0638 | Time 0.2s | Iter Loss 0.0206 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:54,679 - root - INFO - CF Training: Epoch 0001 Iter 0592 / 0638 | Time 0.2s | Iter Loss 0.0238 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:54,903 - root - INFO - CF Training: Epoch 0001 Iter 0593 / 0638 | Time 0.2s | Iter Loss 0.0190 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:55,130 - root - INFO - CF Training: Epoch 0001 Iter 0594 / 0638 | Time 0.2s | Iter Loss 0.0262 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:55,360 - root - INFO - CF Training: Epoch 0001 Iter 0595 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:55,588 - root - INFO - CF Training: Epoch 0001 Iter 0596 / 0638 | Time 0.2s | Iter Loss 0.0214 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:55,814 - root - INFO - CF Training: Epoch 0001 Iter 0597 / 0638 | Time 0.2s | Iter Loss 0.0260 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:56,037 - root - INFO - CF Training: Epoch 0001 Iter 0598 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:56,258 - root - INFO - CF Training: Epoch 0001 Iter 0599 / 0638 | Time 0.2s | Iter Loss 0.0208 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:56,479 - root - INFO - CF Training: Epoch 0001 Iter 0600 / 0638 | Time 0.2s | Iter Loss 0.0168 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:56,702 - root - INFO - CF Training: Epoch 0001 Iter 0601 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:56,926 - root - INFO - CF Training: Epoch 0001 Iter 0602 / 0638 | Time 0.2s | Iter Loss 0.0221 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:57,148 - root - INFO - CF Training: Epoch 0001 Iter 0603 / 0638 | Time 0.2s | Iter Loss 0.0300 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:57,370 - root - INFO - CF Training: Epoch 0001 Iter 0604 / 0638 | Time 0.2s | Iter Loss 0.0258 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:57,592 - root - INFO - CF Training: Epoch 0001 Iter 0605 / 0638 | Time 0.2s | Iter Loss 0.0165 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:57,815 - root - INFO - CF Training: Epoch 0001 Iter 0606 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:58,036 - root - INFO - CF Training: Epoch 0001 Iter 0607 / 0638 | Time 0.2s | Iter Loss 0.0225 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:58,258 - root - INFO - CF Training: Epoch 0001 Iter 0608 / 0638 | Time 0.2s | Iter Loss 0.0238 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:58,479 - root - INFO - CF Training: Epoch 0001 Iter 0609 / 0638 | Time 0.2s | Iter Loss 0.0188 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:58,699 - root - INFO - CF Training: Epoch 0001 Iter 0610 / 0638 | Time 0.2s | Iter Loss 0.0192 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:58,919 - root - INFO - CF Training: Epoch 0001 Iter 0611 / 0638 | Time 0.2s | Iter Loss 0.0257 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:59,140 - root - INFO - CF Training: Epoch 0001 Iter 0612 / 0638 | Time 0.2s | Iter Loss 0.0224 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:59,360 - root - INFO - CF Training: Epoch 0001 Iter 0613 / 0638 | Time 0.2s | Iter Loss 0.0207 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:59,583 - root - INFO - CF Training: Epoch 0001 Iter 0614 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:39:59,805 - root - INFO - CF Training: Epoch 0001 Iter 0615 / 0638 | Time 0.2s | Iter Loss 0.0252 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:00,028 - root - INFO - CF Training: Epoch 0001 Iter 0616 / 0638 | Time 0.2s | Iter Loss 0.0239 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:00,250 - root - INFO - CF Training: Epoch 0001 Iter 0617 / 0638 | Time 0.2s | Iter Loss 0.0220 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:00,473 - root - INFO - CF Training: Epoch 0001 Iter 0618 / 0638 | Time 0.2s | Iter Loss 0.0201 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:00,695 - root - INFO - CF Training: Epoch 0001 Iter 0619 / 0638 | Time 0.2s | Iter Loss 0.0210 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:00,918 - root - INFO - CF Training: Epoch 0001 Iter 0620 / 0638 | Time 0.2s | Iter Loss 0.0258 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:01,140 - root - INFO - CF Training: Epoch 0001 Iter 0621 / 0638 | Time 0.2s | Iter Loss 0.0195 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:01,362 - root - INFO - CF Training: Epoch 0001 Iter 0622 / 0638 | Time 0.2s | Iter Loss 0.0230 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:01,585 - root - INFO - CF Training: Epoch 0001 Iter 0623 / 0638 | Time 0.2s | Iter Loss 0.0238 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:01,807 - root - INFO - CF Training: Epoch 0001 Iter 0624 / 0638 | Time 0.2s | Iter Loss 0.0218 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:02,030 - root - INFO - CF Training: Epoch 0001 Iter 0625 / 0638 | Time 0.2s | Iter Loss 0.0215 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:02,253 - root - INFO - CF Training: Epoch 0001 Iter 0626 / 0638 | Time 0.2s | Iter Loss 0.0179 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:02,478 - root - INFO - CF Training: Epoch 0001 Iter 0627 / 0638 | Time 0.2s | Iter Loss 0.0243 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:02,700 - root - INFO - CF Training: Epoch 0001 Iter 0628 / 0638 | Time 0.2s | Iter Loss 0.0251 | Iter Mean Loss 0.0235\n",
      "2024-11-14 04:40:02,923 - root - INFO - CF Training: Epoch 0001 Iter 0629 / 0638 | Time 0.2s | Iter Loss 0.0183 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:03,145 - root - INFO - CF Training: Epoch 0001 Iter 0630 / 0638 | Time 0.2s | Iter Loss 0.0257 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:03,367 - root - INFO - CF Training: Epoch 0001 Iter 0631 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:03,590 - root - INFO - CF Training: Epoch 0001 Iter 0632 / 0638 | Time 0.2s | Iter Loss 0.0213 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:03,812 - root - INFO - CF Training: Epoch 0001 Iter 0633 / 0638 | Time 0.2s | Iter Loss 0.0202 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,034 - root - INFO - CF Training: Epoch 0001 Iter 0634 / 0638 | Time 0.2s | Iter Loss 0.0191 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,256 - root - INFO - CF Training: Epoch 0001 Iter 0635 / 0638 | Time 0.2s | Iter Loss 0.0266 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,478 - root - INFO - CF Training: Epoch 0001 Iter 0636 / 0638 | Time 0.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,698 - root - INFO - CF Training: Epoch 0001 Iter 0637 / 0638 | Time 0.2s | Iter Loss 0.0195 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,919 - root - INFO - CF Training: Epoch 0001 Iter 0638 / 0638 | Time 0.2s | Iter Loss 0.0203 | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,920 - root - INFO - CF Training: Epoch 0001 Total Iter 0638 | Total Time 141.9s | Iter Mean Loss 0.0234\n",
      "2024-11-14 04:40:04,982 - root - INFO - KG Training: Epoch 0001 Iter 0001 / 3136 | Time 0.1s | Iter Loss 0.6653 | Iter Mean Loss 0.6653\n",
      "2024-11-14 04:40:05,038 - root - INFO - KG Training: Epoch 0001 Iter 0002 / 3136 | Time 0.1s | Iter Loss 0.6650 | Iter Mean Loss 0.6651\n",
      "2024-11-14 04:40:05,099 - root - INFO - KG Training: Epoch 0001 Iter 0003 / 3136 | Time 0.1s | Iter Loss 0.6640 | Iter Mean Loss 0.6648\n",
      "2024-11-14 04:40:05,159 - root - INFO - KG Training: Epoch 0001 Iter 0004 / 3136 | Time 0.1s | Iter Loss 0.6627 | Iter Mean Loss 0.6643\n",
      "2024-11-14 04:40:05,218 - root - INFO - KG Training: Epoch 0001 Iter 0005 / 3136 | Time 0.1s | Iter Loss 0.6631 | Iter Mean Loss 0.6640\n",
      "2024-11-14 04:40:05,276 - root - INFO - KG Training: Epoch 0001 Iter 0006 / 3136 | Time 0.1s | Iter Loss 0.6608 | Iter Mean Loss 0.6635\n",
      "2024-11-14 04:40:05,336 - root - INFO - KG Training: Epoch 0001 Iter 0007 / 3136 | Time 0.1s | Iter Loss 0.6611 | Iter Mean Loss 0.6632\n",
      "2024-11-14 04:40:05,395 - root - INFO - KG Training: Epoch 0001 Iter 0008 / 3136 | Time 0.1s | Iter Loss 0.6579 | Iter Mean Loss 0.6625\n",
      "2024-11-14 04:40:05,455 - root - INFO - KG Training: Epoch 0001 Iter 0009 / 3136 | Time 0.1s | Iter Loss 0.6584 | Iter Mean Loss 0.6620\n",
      "2024-11-14 04:40:05,518 - root - INFO - KG Training: Epoch 0001 Iter 0010 / 3136 | Time 0.1s | Iter Loss 0.6548 | Iter Mean Loss 0.6613\n",
      "2024-11-14 04:40:05,579 - root - INFO - KG Training: Epoch 0001 Iter 0011 / 3136 | Time 0.1s | Iter Loss 0.6550 | Iter Mean Loss 0.6607\n",
      "2024-11-14 04:40:05,648 - root - INFO - KG Training: Epoch 0001 Iter 0012 / 3136 | Time 0.1s | Iter Loss 0.6533 | Iter Mean Loss 0.6601\n",
      "2024-11-14 04:40:06,201 - root - INFO - KG Training: Epoch 0001 Iter 0013 / 3136 | Time 0.6s | Iter Loss 0.6539 | Iter Mean Loss 0.6596\n",
      "2024-11-14 04:40:06,277 - root - INFO - KG Training: Epoch 0001 Iter 0014 / 3136 | Time 0.1s | Iter Loss 0.6532 | Iter Mean Loss 0.6592\n",
      "2024-11-14 04:40:06,360 - root - INFO - KG Training: Epoch 0001 Iter 0015 / 3136 | Time 0.1s | Iter Loss 0.6509 | Iter Mean Loss 0.6586\n",
      "2024-11-14 04:40:06,434 - root - INFO - KG Training: Epoch 0001 Iter 0016 / 3136 | Time 0.1s | Iter Loss 0.6510 | Iter Mean Loss 0.6582\n",
      "2024-11-14 04:40:06,495 - root - INFO - KG Training: Epoch 0001 Iter 0017 / 3136 | Time 0.1s | Iter Loss 0.6482 | Iter Mean Loss 0.6576\n",
      "2024-11-14 04:40:06,554 - root - INFO - KG Training: Epoch 0001 Iter 0018 / 3136 | Time 0.1s | Iter Loss 0.6500 | Iter Mean Loss 0.6572\n",
      "2024-11-14 04:40:06,641 - root - INFO - KG Training: Epoch 0001 Iter 0019 / 3136 | Time 0.1s | Iter Loss 0.6473 | Iter Mean Loss 0.6566\n",
      "2024-11-14 04:40:06,699 - root - INFO - KG Training: Epoch 0001 Iter 0020 / 3136 | Time 0.1s | Iter Loss 0.6470 | Iter Mean Loss 0.6562\n",
      "2024-11-14 04:40:06,757 - root - INFO - KG Training: Epoch 0001 Iter 0021 / 3136 | Time 0.1s | Iter Loss 0.6442 | Iter Mean Loss 0.6556\n",
      "2024-11-14 04:40:06,816 - root - INFO - KG Training: Epoch 0001 Iter 0022 / 3136 | Time 0.1s | Iter Loss 0.6436 | Iter Mean Loss 0.6550\n",
      "2024-11-14 04:40:06,894 - root - INFO - KG Training: Epoch 0001 Iter 0023 / 3136 | Time 0.1s | Iter Loss 0.6433 | Iter Mean Loss 0.6545\n",
      "2024-11-14 04:40:06,968 - root - INFO - KG Training: Epoch 0001 Iter 0024 / 3136 | Time 0.1s | Iter Loss 0.6387 | Iter Mean Loss 0.6539\n",
      "2024-11-14 04:40:07,025 - root - INFO - KG Training: Epoch 0001 Iter 0025 / 3136 | Time 0.1s | Iter Loss 0.6389 | Iter Mean Loss 0.6533\n",
      "2024-11-14 04:40:07,083 - root - INFO - KG Training: Epoch 0001 Iter 0026 / 3136 | Time 0.1s | Iter Loss 0.6380 | Iter Mean Loss 0.6527\n",
      "2024-11-14 04:40:07,160 - root - INFO - KG Training: Epoch 0001 Iter 0027 / 3136 | Time 0.1s | Iter Loss 0.6353 | Iter Mean Loss 0.6520\n",
      "2024-11-14 04:40:07,232 - root - INFO - KG Training: Epoch 0001 Iter 0028 / 3136 | Time 0.1s | Iter Loss 0.6407 | Iter Mean Loss 0.6516\n",
      "2024-11-14 04:40:07,290 - root - INFO - KG Training: Epoch 0001 Iter 0029 / 3136 | Time 0.1s | Iter Loss 0.6357 | Iter Mean Loss 0.6511\n",
      "2024-11-14 04:40:07,364 - root - INFO - KG Training: Epoch 0001 Iter 0030 / 3136 | Time 0.1s | Iter Loss 0.6371 | Iter Mean Loss 0.6506\n",
      "2024-11-14 04:40:07,440 - root - INFO - KG Training: Epoch 0001 Iter 0031 / 3136 | Time 0.1s | Iter Loss 0.6340 | Iter Mean Loss 0.6501\n",
      "2024-11-14 04:40:07,518 - root - INFO - KG Training: Epoch 0001 Iter 0032 / 3136 | Time 0.1s | Iter Loss 0.6342 | Iter Mean Loss 0.6496\n",
      "2024-11-14 04:40:07,581 - root - INFO - KG Training: Epoch 0001 Iter 0033 / 3136 | Time 0.1s | Iter Loss 0.6307 | Iter Mean Loss 0.6490\n",
      "2024-11-14 04:40:07,696 - root - INFO - KG Training: Epoch 0001 Iter 0034 / 3136 | Time 0.1s | Iter Loss 0.6263 | Iter Mean Loss 0.6483\n",
      "2024-11-14 04:40:07,760 - root - INFO - KG Training: Epoch 0001 Iter 0035 / 3136 | Time 0.1s | Iter Loss 0.6266 | Iter Mean Loss 0.6477\n",
      "2024-11-14 04:40:07,822 - root - INFO - KG Training: Epoch 0001 Iter 0036 / 3136 | Time 0.1s | Iter Loss 0.6293 | Iter Mean Loss 0.6472\n",
      "2024-11-14 04:40:07,880 - root - INFO - KG Training: Epoch 0001 Iter 0037 / 3136 | Time 0.1s | Iter Loss 0.6264 | Iter Mean Loss 0.6467\n",
      "2024-11-14 04:40:07,940 - root - INFO - KG Training: Epoch 0001 Iter 0038 / 3136 | Time 0.1s | Iter Loss 0.6263 | Iter Mean Loss 0.6461\n",
      "2024-11-14 04:40:08,003 - root - INFO - KG Training: Epoch 0001 Iter 0039 / 3136 | Time 0.1s | Iter Loss 0.6227 | Iter Mean Loss 0.6455\n",
      "2024-11-14 04:40:08,119 - root - INFO - KG Training: Epoch 0001 Iter 0040 / 3136 | Time 0.1s | Iter Loss 0.6207 | Iter Mean Loss 0.6449\n",
      "2024-11-14 04:40:08,178 - root - INFO - KG Training: Epoch 0001 Iter 0041 / 3136 | Time 0.1s | Iter Loss 0.6225 | Iter Mean Loss 0.6443\n",
      "2024-11-14 04:40:08,241 - root - INFO - KG Training: Epoch 0001 Iter 0042 / 3136 | Time 0.1s | Iter Loss 0.6230 | Iter Mean Loss 0.6438\n",
      "2024-11-14 04:40:08,301 - root - INFO - KG Training: Epoch 0001 Iter 0043 / 3136 | Time 0.1s | Iter Loss 0.6150 | Iter Mean Loss 0.6432\n",
      "2024-11-14 04:40:08,362 - root - INFO - KG Training: Epoch 0001 Iter 0044 / 3136 | Time 0.1s | Iter Loss 0.6133 | Iter Mean Loss 0.6425\n",
      "2024-11-14 04:40:08,423 - root - INFO - KG Training: Epoch 0001 Iter 0045 / 3136 | Time 0.1s | Iter Loss 0.6156 | Iter Mean Loss 0.6419\n",
      "2024-11-14 04:40:08,484 - root - INFO - KG Training: Epoch 0001 Iter 0046 / 3136 | Time 0.1s | Iter Loss 0.6155 | Iter Mean Loss 0.6413\n",
      "2024-11-14 04:40:08,552 - root - INFO - KG Training: Epoch 0001 Iter 0047 / 3136 | Time 0.1s | Iter Loss 0.6166 | Iter Mean Loss 0.6408\n",
      "2024-11-14 04:40:08,615 - root - INFO - KG Training: Epoch 0001 Iter 0048 / 3136 | Time 0.1s | Iter Loss 0.6153 | Iter Mean Loss 0.6403\n",
      "2024-11-14 04:40:08,676 - root - INFO - KG Training: Epoch 0001 Iter 0049 / 3136 | Time 0.1s | Iter Loss 0.6128 | Iter Mean Loss 0.6397\n",
      "2024-11-14 04:40:08,736 - root - INFO - KG Training: Epoch 0001 Iter 0050 / 3136 | Time 0.1s | Iter Loss 0.6103 | Iter Mean Loss 0.6391\n",
      "2024-11-14 04:40:08,797 - root - INFO - KG Training: Epoch 0001 Iter 0051 / 3136 | Time 0.1s | Iter Loss 0.6097 | Iter Mean Loss 0.6385\n",
      "2024-11-14 04:40:08,860 - root - INFO - KG Training: Epoch 0001 Iter 0052 / 3136 | Time 0.1s | Iter Loss 0.6088 | Iter Mean Loss 0.6380\n",
      "2024-11-14 04:40:08,919 - root - INFO - KG Training: Epoch 0001 Iter 0053 / 3136 | Time 0.1s | Iter Loss 0.6053 | Iter Mean Loss 0.6373\n",
      "2024-11-14 04:40:08,982 - root - INFO - KG Training: Epoch 0001 Iter 0054 / 3136 | Time 0.1s | Iter Loss 0.6085 | Iter Mean Loss 0.6368\n",
      "2024-11-14 04:40:09,043 - root - INFO - KG Training: Epoch 0001 Iter 0055 / 3136 | Time 0.1s | Iter Loss 0.6009 | Iter Mean Loss 0.6362\n",
      "2024-11-14 04:40:09,106 - root - INFO - KG Training: Epoch 0001 Iter 0056 / 3136 | Time 0.1s | Iter Loss 0.6034 | Iter Mean Loss 0.6356\n",
      "2024-11-14 04:40:09,168 - root - INFO - KG Training: Epoch 0001 Iter 0057 / 3136 | Time 0.1s | Iter Loss 0.6023 | Iter Mean Loss 0.6350\n",
      "2024-11-14 04:40:09,231 - root - INFO - KG Training: Epoch 0001 Iter 0058 / 3136 | Time 0.1s | Iter Loss 0.6004 | Iter Mean Loss 0.6344\n",
      "2024-11-14 04:40:09,292 - root - INFO - KG Training: Epoch 0001 Iter 0059 / 3136 | Time 0.1s | Iter Loss 0.5986 | Iter Mean Loss 0.6338\n",
      "2024-11-14 04:40:09,351 - root - INFO - KG Training: Epoch 0001 Iter 0060 / 3136 | Time 0.1s | Iter Loss 0.5984 | Iter Mean Loss 0.6332\n",
      "2024-11-14 04:40:09,411 - root - INFO - KG Training: Epoch 0001 Iter 0061 / 3136 | Time 0.1s | Iter Loss 0.5970 | Iter Mean Loss 0.6326\n",
      "2024-11-14 04:40:09,472 - root - INFO - KG Training: Epoch 0001 Iter 0062 / 3136 | Time 0.1s | Iter Loss 0.5955 | Iter Mean Loss 0.6320\n",
      "2024-11-14 04:40:09,535 - root - INFO - KG Training: Epoch 0001 Iter 0063 / 3136 | Time 0.1s | Iter Loss 0.6005 | Iter Mean Loss 0.6315\n",
      "2024-11-14 04:40:09,597 - root - INFO - KG Training: Epoch 0001 Iter 0064 / 3136 | Time 0.1s | Iter Loss 0.5939 | Iter Mean Loss 0.6309\n",
      "2024-11-14 04:40:09,659 - root - INFO - KG Training: Epoch 0001 Iter 0065 / 3136 | Time 0.1s | Iter Loss 0.5929 | Iter Mean Loss 0.6303\n",
      "2024-11-14 04:40:09,723 - root - INFO - KG Training: Epoch 0001 Iter 0066 / 3136 | Time 0.1s | Iter Loss 0.5854 | Iter Mean Loss 0.6297\n",
      "2024-11-14 04:40:09,793 - root - INFO - KG Training: Epoch 0001 Iter 0067 / 3136 | Time 0.1s | Iter Loss 0.5884 | Iter Mean Loss 0.6290\n",
      "2024-11-14 04:40:09,888 - root - INFO - KG Training: Epoch 0001 Iter 0068 / 3136 | Time 0.1s | Iter Loss 0.5901 | Iter Mean Loss 0.6285\n",
      "2024-11-14 04:40:09,957 - root - INFO - KG Training: Epoch 0001 Iter 0069 / 3136 | Time 0.1s | Iter Loss 0.5860 | Iter Mean Loss 0.6279\n",
      "2024-11-14 04:40:10,019 - root - INFO - KG Training: Epoch 0001 Iter 0070 / 3136 | Time 0.1s | Iter Loss 0.5867 | Iter Mean Loss 0.6273\n",
      "2024-11-14 04:40:10,082 - root - INFO - KG Training: Epoch 0001 Iter 0071 / 3136 | Time 0.1s | Iter Loss 0.5834 | Iter Mean Loss 0.6266\n",
      "2024-11-14 04:40:10,320 - root - INFO - KG Training: Epoch 0001 Iter 0072 / 3136 | Time 0.2s | Iter Loss 0.5800 | Iter Mean Loss 0.6260\n",
      "2024-11-14 04:40:10,465 - root - INFO - KG Training: Epoch 0001 Iter 0073 / 3136 | Time 0.1s | Iter Loss 0.5824 | Iter Mean Loss 0.6254\n",
      "2024-11-14 04:40:10,523 - root - INFO - KG Training: Epoch 0001 Iter 0074 / 3136 | Time 0.1s | Iter Loss 0.5711 | Iter Mean Loss 0.6247\n",
      "2024-11-14 04:40:10,583 - root - INFO - KG Training: Epoch 0001 Iter 0075 / 3136 | Time 0.1s | Iter Loss 0.5766 | Iter Mean Loss 0.6240\n",
      "2024-11-14 04:40:10,647 - root - INFO - KG Training: Epoch 0001 Iter 0076 / 3136 | Time 0.1s | Iter Loss 0.5796 | Iter Mean Loss 0.6234\n",
      "2024-11-14 04:40:10,710 - root - INFO - KG Training: Epoch 0001 Iter 0077 / 3136 | Time 0.1s | Iter Loss 0.5803 | Iter Mean Loss 0.6229\n",
      "2024-11-14 04:40:10,773 - root - INFO - KG Training: Epoch 0001 Iter 0078 / 3136 | Time 0.1s | Iter Loss 0.5692 | Iter Mean Loss 0.6222\n",
      "2024-11-14 04:40:10,917 - root - INFO - KG Training: Epoch 0001 Iter 0079 / 3136 | Time 0.1s | Iter Loss 0.5789 | Iter Mean Loss 0.6216\n",
      "2024-11-14 04:40:10,982 - root - INFO - KG Training: Epoch 0001 Iter 0080 / 3136 | Time 0.1s | Iter Loss 0.5658 | Iter Mean Loss 0.6209\n",
      "2024-11-14 04:40:11,041 - root - INFO - KG Training: Epoch 0001 Iter 0081 / 3136 | Time 0.1s | Iter Loss 0.5674 | Iter Mean Loss 0.6203\n",
      "2024-11-14 04:40:11,102 - root - INFO - KG Training: Epoch 0001 Iter 0082 / 3136 | Time 0.1s | Iter Loss 0.5700 | Iter Mean Loss 0.6197\n",
      "2024-11-14 04:40:11,173 - root - INFO - KG Training: Epoch 0001 Iter 0083 / 3136 | Time 0.1s | Iter Loss 0.5617 | Iter Mean Loss 0.6190\n",
      "2024-11-14 04:40:11,231 - root - INFO - KG Training: Epoch 0001 Iter 0084 / 3136 | Time 0.1s | Iter Loss 0.5655 | Iter Mean Loss 0.6183\n",
      "2024-11-14 04:40:11,340 - root - INFO - KG Training: Epoch 0001 Iter 0085 / 3136 | Time 0.1s | Iter Loss 0.5652 | Iter Mean Loss 0.6177\n",
      "2024-11-14 04:40:11,401 - root - INFO - KG Training: Epoch 0001 Iter 0086 / 3136 | Time 0.1s | Iter Loss 0.5636 | Iter Mean Loss 0.6171\n",
      "2024-11-14 04:40:11,463 - root - INFO - KG Training: Epoch 0001 Iter 0087 / 3136 | Time 0.1s | Iter Loss 0.5630 | Iter Mean Loss 0.6165\n",
      "2024-11-14 04:40:11,521 - root - INFO - KG Training: Epoch 0001 Iter 0088 / 3136 | Time 0.1s | Iter Loss 0.5568 | Iter Mean Loss 0.6158\n",
      "2024-11-14 04:40:11,582 - root - INFO - KG Training: Epoch 0001 Iter 0089 / 3136 | Time 0.1s | Iter Loss 0.5567 | Iter Mean Loss 0.6151\n",
      "2024-11-14 04:40:11,644 - root - INFO - KG Training: Epoch 0001 Iter 0090 / 3136 | Time 0.1s | Iter Loss 0.5575 | Iter Mean Loss 0.6145\n",
      "2024-11-14 04:40:11,706 - root - INFO - KG Training: Epoch 0001 Iter 0091 / 3136 | Time 0.1s | Iter Loss 0.5540 | Iter Mean Loss 0.6138\n",
      "2024-11-14 04:40:11,765 - root - INFO - KG Training: Epoch 0001 Iter 0092 / 3136 | Time 0.1s | Iter Loss 0.5517 | Iter Mean Loss 0.6131\n",
      "2024-11-14 04:40:11,826 - root - INFO - KG Training: Epoch 0001 Iter 0093 / 3136 | Time 0.1s | Iter Loss 0.5490 | Iter Mean Loss 0.6125\n",
      "2024-11-14 04:40:11,889 - root - INFO - KG Training: Epoch 0001 Iter 0094 / 3136 | Time 0.1s | Iter Loss 0.5500 | Iter Mean Loss 0.6118\n",
      "2024-11-14 04:40:11,951 - root - INFO - KG Training: Epoch 0001 Iter 0095 / 3136 | Time 0.1s | Iter Loss 0.5458 | Iter Mean Loss 0.6111\n",
      "2024-11-14 04:40:12,014 - root - INFO - KG Training: Epoch 0001 Iter 0096 / 3136 | Time 0.1s | Iter Loss 0.5474 | Iter Mean Loss 0.6104\n",
      "2024-11-14 04:40:12,079 - root - INFO - KG Training: Epoch 0001 Iter 0097 / 3136 | Time 0.1s | Iter Loss 0.5502 | Iter Mean Loss 0.6098\n",
      "2024-11-14 04:40:12,143 - root - INFO - KG Training: Epoch 0001 Iter 0098 / 3136 | Time 0.1s | Iter Loss 0.5518 | Iter Mean Loss 0.6092\n",
      "2024-11-14 04:40:12,204 - root - INFO - KG Training: Epoch 0001 Iter 0099 / 3136 | Time 0.1s | Iter Loss 0.5405 | Iter Mean Loss 0.6085\n",
      "2024-11-14 04:40:12,266 - root - INFO - KG Training: Epoch 0001 Iter 0100 / 3136 | Time 0.1s | Iter Loss 0.5442 | Iter Mean Loss 0.6079\n",
      "2024-11-14 04:40:12,330 - root - INFO - KG Training: Epoch 0001 Iter 0101 / 3136 | Time 0.1s | Iter Loss 0.5441 | Iter Mean Loss 0.6072\n",
      "2024-11-14 04:40:12,393 - root - INFO - KG Training: Epoch 0001 Iter 0102 / 3136 | Time 0.1s | Iter Loss 0.5370 | Iter Mean Loss 0.6066\n",
      "2024-11-14 04:40:12,457 - root - INFO - KG Training: Epoch 0001 Iter 0103 / 3136 | Time 0.1s | Iter Loss 0.5333 | Iter Mean Loss 0.6058\n",
      "2024-11-14 04:40:12,520 - root - INFO - KG Training: Epoch 0001 Iter 0104 / 3136 | Time 0.1s | Iter Loss 0.5349 | Iter Mean Loss 0.6052\n",
      "2024-11-14 04:40:12,582 - root - INFO - KG Training: Epoch 0001 Iter 0105 / 3136 | Time 0.1s | Iter Loss 0.5342 | Iter Mean Loss 0.6045\n",
      "2024-11-14 04:40:12,652 - root - INFO - KG Training: Epoch 0001 Iter 0106 / 3136 | Time 0.1s | Iter Loss 0.5366 | Iter Mean Loss 0.6038\n",
      "2024-11-14 04:40:12,713 - root - INFO - KG Training: Epoch 0001 Iter 0107 / 3136 | Time 0.1s | Iter Loss 0.5336 | Iter Mean Loss 0.6032\n",
      "2024-11-14 04:40:12,776 - root - INFO - KG Training: Epoch 0001 Iter 0108 / 3136 | Time 0.1s | Iter Loss 0.5361 | Iter Mean Loss 0.6026\n",
      "2024-11-14 04:40:12,849 - root - INFO - KG Training: Epoch 0001 Iter 0109 / 3136 | Time 0.1s | Iter Loss 0.5339 | Iter Mean Loss 0.6019\n",
      "2024-11-14 04:40:12,982 - root - INFO - KG Training: Epoch 0001 Iter 0110 / 3136 | Time 0.1s | Iter Loss 0.5318 | Iter Mean Loss 0.6013\n",
      "2024-11-14 04:40:13,043 - root - INFO - KG Training: Epoch 0001 Iter 0111 / 3136 | Time 0.1s | Iter Loss 0.5233 | Iter Mean Loss 0.6006\n",
      "2024-11-14 04:40:13,104 - root - INFO - KG Training: Epoch 0001 Iter 0112 / 3136 | Time 0.1s | Iter Loss 0.5226 | Iter Mean Loss 0.5999\n",
      "2024-11-14 04:40:13,165 - root - INFO - KG Training: Epoch 0001 Iter 0113 / 3136 | Time 0.1s | Iter Loss 0.5270 | Iter Mean Loss 0.5993\n",
      "2024-11-14 04:40:13,225 - root - INFO - KG Training: Epoch 0001 Iter 0114 / 3136 | Time 0.1s | Iter Loss 0.5202 | Iter Mean Loss 0.5986\n",
      "2024-11-14 04:40:13,299 - root - INFO - KG Training: Epoch 0001 Iter 0115 / 3136 | Time 0.1s | Iter Loss 0.5250 | Iter Mean Loss 0.5979\n",
      "2024-11-14 04:40:13,360 - root - INFO - KG Training: Epoch 0001 Iter 0116 / 3136 | Time 0.1s | Iter Loss 0.5187 | Iter Mean Loss 0.5972\n",
      "2024-11-14 04:40:13,437 - root - INFO - KG Training: Epoch 0001 Iter 0117 / 3136 | Time 0.1s | Iter Loss 0.5192 | Iter Mean Loss 0.5966\n",
      "2024-11-14 04:40:13,505 - root - INFO - KG Training: Epoch 0001 Iter 0118 / 3136 | Time 0.1s | Iter Loss 0.5099 | Iter Mean Loss 0.5958\n",
      "2024-11-14 04:40:13,577 - root - INFO - KG Training: Epoch 0001 Iter 0119 / 3136 | Time 0.1s | Iter Loss 0.5098 | Iter Mean Loss 0.5951\n",
      "2024-11-14 04:40:13,632 - root - INFO - KG Training: Epoch 0001 Iter 0120 / 3136 | Time 0.1s | Iter Loss 0.5188 | Iter Mean Loss 0.5945\n",
      "2024-11-14 04:40:13,708 - root - INFO - KG Training: Epoch 0001 Iter 0121 / 3136 | Time 0.1s | Iter Loss 0.5106 | Iter Mean Loss 0.5938\n",
      "2024-11-14 04:40:13,765 - root - INFO - KG Training: Epoch 0001 Iter 0122 / 3136 | Time 0.1s | Iter Loss 0.5140 | Iter Mean Loss 0.5931\n",
      "2024-11-14 04:40:13,832 - root - INFO - KG Training: Epoch 0001 Iter 0123 / 3136 | Time 0.1s | Iter Loss 0.5132 | Iter Mean Loss 0.5925\n",
      "2024-11-14 04:40:13,891 - root - INFO - KG Training: Epoch 0001 Iter 0124 / 3136 | Time 0.1s | Iter Loss 0.5003 | Iter Mean Loss 0.5917\n",
      "2024-11-14 04:40:13,951 - root - INFO - KG Training: Epoch 0001 Iter 0125 / 3136 | Time 0.1s | Iter Loss 0.4999 | Iter Mean Loss 0.5910\n",
      "2024-11-14 04:40:14,011 - root - INFO - KG Training: Epoch 0001 Iter 0126 / 3136 | Time 0.1s | Iter Loss 0.5109 | Iter Mean Loss 0.5904\n",
      "2024-11-14 04:40:14,073 - root - INFO - KG Training: Epoch 0001 Iter 0127 / 3136 | Time 0.1s | Iter Loss 0.4910 | Iter Mean Loss 0.5896\n",
      "2024-11-14 04:40:14,179 - root - INFO - KG Training: Epoch 0001 Iter 0128 / 3136 | Time 0.1s | Iter Loss 0.5075 | Iter Mean Loss 0.5889\n",
      "2024-11-14 04:40:14,293 - root - INFO - KG Training: Epoch 0001 Iter 0129 / 3136 | Time 0.1s | Iter Loss 0.5084 | Iter Mean Loss 0.5883\n",
      "2024-11-14 04:40:14,353 - root - INFO - KG Training: Epoch 0001 Iter 0130 / 3136 | Time 0.1s | Iter Loss 0.4941 | Iter Mean Loss 0.5876\n",
      "2024-11-14 04:40:14,412 - root - INFO - KG Training: Epoch 0001 Iter 0131 / 3136 | Time 0.1s | Iter Loss 0.4888 | Iter Mean Loss 0.5868\n",
      "2024-11-14 04:40:14,470 - root - INFO - KG Training: Epoch 0001 Iter 0132 / 3136 | Time 0.1s | Iter Loss 0.4961 | Iter Mean Loss 0.5862\n",
      "2024-11-14 04:40:14,528 - root - INFO - KG Training: Epoch 0001 Iter 0133 / 3136 | Time 0.1s | Iter Loss 0.4860 | Iter Mean Loss 0.5854\n",
      "2024-11-14 04:40:14,639 - root - INFO - KG Training: Epoch 0001 Iter 0134 / 3136 | Time 0.1s | Iter Loss 0.4945 | Iter Mean Loss 0.5847\n",
      "2024-11-14 04:40:14,698 - root - INFO - KG Training: Epoch 0001 Iter 0135 / 3136 | Time 0.1s | Iter Loss 0.4889 | Iter Mean Loss 0.5840\n",
      "2024-11-14 04:40:14,757 - root - INFO - KG Training: Epoch 0001 Iter 0136 / 3136 | Time 0.1s | Iter Loss 0.4904 | Iter Mean Loss 0.5833\n",
      "2024-11-14 04:40:14,819 - root - INFO - KG Training: Epoch 0001 Iter 0137 / 3136 | Time 0.1s | Iter Loss 0.4879 | Iter Mean Loss 0.5826\n",
      "2024-11-14 04:40:14,938 - root - INFO - KG Training: Epoch 0001 Iter 0138 / 3136 | Time 0.1s | Iter Loss 0.4921 | Iter Mean Loss 0.5820\n",
      "2024-11-14 04:40:14,999 - root - INFO - KG Training: Epoch 0001 Iter 0139 / 3136 | Time 0.1s | Iter Loss 0.4948 | Iter Mean Loss 0.5813\n",
      "2024-11-14 04:40:15,058 - root - INFO - KG Training: Epoch 0001 Iter 0140 / 3136 | Time 0.1s | Iter Loss 0.4931 | Iter Mean Loss 0.5807\n",
      "2024-11-14 04:40:15,119 - root - INFO - KG Training: Epoch 0001 Iter 0141 / 3136 | Time 0.1s | Iter Loss 0.4895 | Iter Mean Loss 0.5801\n",
      "2024-11-14 04:40:15,181 - root - INFO - KG Training: Epoch 0001 Iter 0142 / 3136 | Time 0.1s | Iter Loss 0.4892 | Iter Mean Loss 0.5794\n",
      "2024-11-14 04:40:15,243 - root - INFO - KG Training: Epoch 0001 Iter 0143 / 3136 | Time 0.1s | Iter Loss 0.4794 | Iter Mean Loss 0.5787\n",
      "2024-11-14 04:40:15,305 - root - INFO - KG Training: Epoch 0001 Iter 0144 / 3136 | Time 0.1s | Iter Loss 0.4827 | Iter Mean Loss 0.5781\n",
      "2024-11-14 04:40:15,368 - root - INFO - KG Training: Epoch 0001 Iter 0145 / 3136 | Time 0.1s | Iter Loss 0.4785 | Iter Mean Loss 0.5774\n",
      "2024-11-14 04:40:15,427 - root - INFO - KG Training: Epoch 0001 Iter 0146 / 3136 | Time 0.1s | Iter Loss 0.4821 | Iter Mean Loss 0.5767\n",
      "2024-11-14 04:40:15,486 - root - INFO - KG Training: Epoch 0001 Iter 0147 / 3136 | Time 0.1s | Iter Loss 0.4738 | Iter Mean Loss 0.5760\n",
      "2024-11-14 04:40:15,546 - root - INFO - KG Training: Epoch 0001 Iter 0148 / 3136 | Time 0.1s | Iter Loss 0.4723 | Iter Mean Loss 0.5753\n",
      "2024-11-14 04:40:15,606 - root - INFO - KG Training: Epoch 0001 Iter 0149 / 3136 | Time 0.1s | Iter Loss 0.4810 | Iter Mean Loss 0.5747\n",
      "2024-11-14 04:40:15,665 - root - INFO - KG Training: Epoch 0001 Iter 0150 / 3136 | Time 0.1s | Iter Loss 0.4745 | Iter Mean Loss 0.5740\n",
      "2024-11-14 04:40:15,727 - root - INFO - KG Training: Epoch 0001 Iter 0151 / 3136 | Time 0.1s | Iter Loss 0.4738 | Iter Mean Loss 0.5734\n",
      "2024-11-14 04:40:15,792 - root - INFO - KG Training: Epoch 0001 Iter 0152 / 3136 | Time 0.1s | Iter Loss 0.4685 | Iter Mean Loss 0.5727\n",
      "2024-11-14 04:40:15,853 - root - INFO - KG Training: Epoch 0001 Iter 0153 / 3136 | Time 0.1s | Iter Loss 0.4691 | Iter Mean Loss 0.5720\n",
      "2024-11-14 04:40:15,912 - root - INFO - KG Training: Epoch 0001 Iter 0154 / 3136 | Time 0.1s | Iter Loss 0.4733 | Iter Mean Loss 0.5714\n",
      "2024-11-14 04:40:15,973 - root - INFO - KG Training: Epoch 0001 Iter 0155 / 3136 | Time 0.1s | Iter Loss 0.4606 | Iter Mean Loss 0.5706\n",
      "2024-11-14 04:40:16,035 - root - INFO - KG Training: Epoch 0001 Iter 0156 / 3136 | Time 0.1s | Iter Loss 0.4684 | Iter Mean Loss 0.5700\n",
      "2024-11-14 04:40:16,095 - root - INFO - KG Training: Epoch 0001 Iter 0157 / 3136 | Time 0.1s | Iter Loss 0.4751 | Iter Mean Loss 0.5694\n",
      "2024-11-14 04:40:16,154 - root - INFO - KG Training: Epoch 0001 Iter 0158 / 3136 | Time 0.1s | Iter Loss 0.4613 | Iter Mean Loss 0.5687\n",
      "2024-11-14 04:40:16,218 - root - INFO - KG Training: Epoch 0001 Iter 0159 / 3136 | Time 0.1s | Iter Loss 0.4646 | Iter Mean Loss 0.5680\n",
      "2024-11-14 04:40:16,281 - root - INFO - KG Training: Epoch 0001 Iter 0160 / 3136 | Time 0.1s | Iter Loss 0.4575 | Iter Mean Loss 0.5673\n",
      "2024-11-14 04:40:16,351 - root - INFO - KG Training: Epoch 0001 Iter 0161 / 3136 | Time 0.1s | Iter Loss 0.4669 | Iter Mean Loss 0.5667\n",
      "2024-11-14 04:40:16,413 - root - INFO - KG Training: Epoch 0001 Iter 0162 / 3136 | Time 0.1s | Iter Loss 0.4607 | Iter Mean Loss 0.5661\n",
      "2024-11-14 04:40:16,473 - root - INFO - KG Training: Epoch 0001 Iter 0163 / 3136 | Time 0.1s | Iter Loss 0.4530 | Iter Mean Loss 0.5654\n",
      "2024-11-14 04:40:16,586 - root - INFO - KG Training: Epoch 0001 Iter 0164 / 3136 | Time 0.1s | Iter Loss 0.4487 | Iter Mean Loss 0.5647\n",
      "2024-11-14 04:40:16,648 - root - INFO - KG Training: Epoch 0001 Iter 0165 / 3136 | Time 0.1s | Iter Loss 0.4575 | Iter Mean Loss 0.5640\n",
      "2024-11-14 04:40:16,710 - root - INFO - KG Training: Epoch 0001 Iter 0166 / 3136 | Time 0.1s | Iter Loss 0.4559 | Iter Mean Loss 0.5634\n",
      "2024-11-14 04:40:16,770 - root - INFO - KG Training: Epoch 0001 Iter 0167 / 3136 | Time 0.1s | Iter Loss 0.4567 | Iter Mean Loss 0.5627\n",
      "2024-11-14 04:40:16,832 - root - INFO - KG Training: Epoch 0001 Iter 0168 / 3136 | Time 0.1s | Iter Loss 0.4633 | Iter Mean Loss 0.5621\n",
      "2024-11-14 04:40:17,052 - root - INFO - KG Training: Epoch 0001 Iter 0169 / 3136 | Time 0.2s | Iter Loss 0.4422 | Iter Mean Loss 0.5614\n",
      "2024-11-14 04:40:17,115 - root - INFO - KG Training: Epoch 0001 Iter 0170 / 3136 | Time 0.1s | Iter Loss 0.4462 | Iter Mean Loss 0.5607\n",
      "2024-11-14 04:40:17,177 - root - INFO - KG Training: Epoch 0001 Iter 0171 / 3136 | Time 0.1s | Iter Loss 0.4495 | Iter Mean Loss 0.5601\n",
      "2024-11-14 04:40:17,242 - root - INFO - KG Training: Epoch 0001 Iter 0172 / 3136 | Time 0.1s | Iter Loss 0.4526 | Iter Mean Loss 0.5595\n",
      "2024-11-14 04:40:17,304 - root - INFO - KG Training: Epoch 0001 Iter 0173 / 3136 | Time 0.1s | Iter Loss 0.4446 | Iter Mean Loss 0.5588\n",
      "2024-11-14 04:40:17,365 - root - INFO - KG Training: Epoch 0001 Iter 0174 / 3136 | Time 0.1s | Iter Loss 0.4438 | Iter Mean Loss 0.5581\n",
      "2024-11-14 04:40:17,431 - root - INFO - KG Training: Epoch 0001 Iter 0175 / 3136 | Time 0.1s | Iter Loss 0.4468 | Iter Mean Loss 0.5575\n",
      "2024-11-14 04:40:17,513 - root - INFO - KG Training: Epoch 0001 Iter 0176 / 3136 | Time 0.1s | Iter Loss 0.4339 | Iter Mean Loss 0.5568\n",
      "2024-11-14 04:40:17,577 - root - INFO - KG Training: Epoch 0001 Iter 0177 / 3136 | Time 0.1s | Iter Loss 0.4343 | Iter Mean Loss 0.5561\n",
      "2024-11-14 04:40:17,647 - root - INFO - KG Training: Epoch 0001 Iter 0178 / 3136 | Time 0.1s | Iter Loss 0.4347 | Iter Mean Loss 0.5554\n",
      "2024-11-14 04:40:17,709 - root - INFO - KG Training: Epoch 0001 Iter 0179 / 3136 | Time 0.1s | Iter Loss 0.4366 | Iter Mean Loss 0.5548\n",
      "2024-11-14 04:40:17,772 - root - INFO - KG Training: Epoch 0001 Iter 0180 / 3136 | Time 0.1s | Iter Loss 0.4418 | Iter Mean Loss 0.5541\n",
      "2024-11-14 04:40:17,834 - root - INFO - KG Training: Epoch 0001 Iter 0181 / 3136 | Time 0.1s | Iter Loss 0.4291 | Iter Mean Loss 0.5534\n",
      "2024-11-14 04:40:17,987 - root - INFO - KG Training: Epoch 0001 Iter 0182 / 3136 | Time 0.2s | Iter Loss 0.4356 | Iter Mean Loss 0.5528\n",
      "2024-11-14 04:40:18,051 - root - INFO - KG Training: Epoch 0001 Iter 0183 / 3136 | Time 0.1s | Iter Loss 0.4296 | Iter Mean Loss 0.5521\n",
      "2024-11-14 04:40:18,116 - root - INFO - KG Training: Epoch 0001 Iter 0184 / 3136 | Time 0.1s | Iter Loss 0.4374 | Iter Mean Loss 0.5515\n",
      "2024-11-14 04:40:18,180 - root - INFO - KG Training: Epoch 0001 Iter 0185 / 3136 | Time 0.1s | Iter Loss 0.4264 | Iter Mean Loss 0.5508\n",
      "2024-11-14 04:40:18,244 - root - INFO - KG Training: Epoch 0001 Iter 0186 / 3136 | Time 0.1s | Iter Loss 0.4338 | Iter Mean Loss 0.5502\n",
      "2024-11-14 04:40:18,305 - root - INFO - KG Training: Epoch 0001 Iter 0187 / 3136 | Time 0.1s | Iter Loss 0.4317 | Iter Mean Loss 0.5496\n",
      "2024-11-14 04:40:18,366 - root - INFO - KG Training: Epoch 0001 Iter 0188 / 3136 | Time 0.1s | Iter Loss 0.4305 | Iter Mean Loss 0.5489\n",
      "2024-11-14 04:40:18,427 - root - INFO - KG Training: Epoch 0001 Iter 0189 / 3136 | Time 0.1s | Iter Loss 0.4288 | Iter Mean Loss 0.5483\n",
      "2024-11-14 04:40:18,491 - root - INFO - KG Training: Epoch 0001 Iter 0190 / 3136 | Time 0.1s | Iter Loss 0.4295 | Iter Mean Loss 0.5477\n",
      "2024-11-14 04:40:18,584 - root - INFO - KG Training: Epoch 0001 Iter 0191 / 3136 | Time 0.1s | Iter Loss 0.4182 | Iter Mean Loss 0.5470\n",
      "2024-11-14 04:40:18,696 - root - INFO - KG Training: Epoch 0001 Iter 0192 / 3136 | Time 0.1s | Iter Loss 0.4229 | Iter Mean Loss 0.5463\n",
      "2024-11-14 04:40:18,758 - root - INFO - KG Training: Epoch 0001 Iter 0193 / 3136 | Time 0.1s | Iter Loss 0.4265 | Iter Mean Loss 0.5457\n",
      "2024-11-14 04:40:18,819 - root - INFO - KG Training: Epoch 0001 Iter 0194 / 3136 | Time 0.1s | Iter Loss 0.4165 | Iter Mean Loss 0.5451\n",
      "2024-11-14 04:40:18,879 - root - INFO - KG Training: Epoch 0001 Iter 0195 / 3136 | Time 0.1s | Iter Loss 0.4234 | Iter Mean Loss 0.5444\n",
      "2024-11-14 04:40:18,936 - root - INFO - KG Training: Epoch 0001 Iter 0196 / 3136 | Time 0.1s | Iter Loss 0.4118 | Iter Mean Loss 0.5438\n",
      "2024-11-14 04:40:18,997 - root - INFO - KG Training: Epoch 0001 Iter 0197 / 3136 | Time 0.1s | Iter Loss 0.4179 | Iter Mean Loss 0.5431\n",
      "2024-11-14 04:40:19,137 - root - INFO - KG Training: Epoch 0001 Iter 0198 / 3136 | Time 0.1s | Iter Loss 0.4242 | Iter Mean Loss 0.5425\n",
      "2024-11-14 04:40:19,326 - root - INFO - KG Training: Epoch 0001 Iter 0199 / 3136 | Time 0.2s | Iter Loss 0.4172 | Iter Mean Loss 0.5419\n",
      "2024-11-14 04:40:19,385 - root - INFO - KG Training: Epoch 0001 Iter 0200 / 3136 | Time 0.1s | Iter Loss 0.4232 | Iter Mean Loss 0.5413\n",
      "2024-11-14 04:40:19,446 - root - INFO - KG Training: Epoch 0001 Iter 0201 / 3136 | Time 0.1s | Iter Loss 0.4154 | Iter Mean Loss 0.5407\n",
      "2024-11-14 04:40:19,506 - root - INFO - KG Training: Epoch 0001 Iter 0202 / 3136 | Time 0.1s | Iter Loss 0.4222 | Iter Mean Loss 0.5401\n",
      "2024-11-14 04:40:19,614 - root - INFO - KG Training: Epoch 0001 Iter 0203 / 3136 | Time 0.1s | Iter Loss 0.4198 | Iter Mean Loss 0.5395\n",
      "2024-11-14 04:40:19,770 - root - INFO - KG Training: Epoch 0001 Iter 0204 / 3136 | Time 0.2s | Iter Loss 0.4119 | Iter Mean Loss 0.5389\n",
      "2024-11-14 04:40:19,828 - root - INFO - KG Training: Epoch 0001 Iter 0205 / 3136 | Time 0.1s | Iter Loss 0.4127 | Iter Mean Loss 0.5383\n",
      "2024-11-14 04:40:19,888 - root - INFO - KG Training: Epoch 0001 Iter 0206 / 3136 | Time 0.1s | Iter Loss 0.4084 | Iter Mean Loss 0.5376\n",
      "2024-11-14 04:40:19,945 - root - INFO - KG Training: Epoch 0001 Iter 0207 / 3136 | Time 0.1s | Iter Loss 0.4073 | Iter Mean Loss 0.5370\n",
      "2024-11-14 04:40:20,004 - root - INFO - KG Training: Epoch 0001 Iter 0208 / 3136 | Time 0.1s | Iter Loss 0.4047 | Iter Mean Loss 0.5364\n",
      "2024-11-14 04:40:20,061 - root - INFO - KG Training: Epoch 0001 Iter 0209 / 3136 | Time 0.1s | Iter Loss 0.4047 | Iter Mean Loss 0.5357\n",
      "2024-11-14 04:40:20,121 - root - INFO - KG Training: Epoch 0001 Iter 0210 / 3136 | Time 0.1s | Iter Loss 0.4098 | Iter Mean Loss 0.5351\n",
      "2024-11-14 04:40:20,179 - root - INFO - KG Training: Epoch 0001 Iter 0211 / 3136 | Time 0.1s | Iter Loss 0.4024 | Iter Mean Loss 0.5345\n",
      "2024-11-14 04:40:20,242 - root - INFO - KG Training: Epoch 0001 Iter 0212 / 3136 | Time 0.1s | Iter Loss 0.4027 | Iter Mean Loss 0.5339\n",
      "2024-11-14 04:40:20,304 - root - INFO - KG Training: Epoch 0001 Iter 0213 / 3136 | Time 0.1s | Iter Loss 0.4039 | Iter Mean Loss 0.5333\n",
      "2024-11-14 04:40:20,362 - root - INFO - KG Training: Epoch 0001 Iter 0214 / 3136 | Time 0.1s | Iter Loss 0.4035 | Iter Mean Loss 0.5327\n",
      "2024-11-14 04:40:20,422 - root - INFO - KG Training: Epoch 0001 Iter 0215 / 3136 | Time 0.1s | Iter Loss 0.3950 | Iter Mean Loss 0.5320\n",
      "2024-11-14 04:40:20,570 - root - INFO - KG Training: Epoch 0001 Iter 0216 / 3136 | Time 0.1s | Iter Loss 0.3909 | Iter Mean Loss 0.5314\n",
      "2024-11-14 04:40:20,641 - root - INFO - KG Training: Epoch 0001 Iter 0217 / 3136 | Time 0.1s | Iter Loss 0.3836 | Iter Mean Loss 0.5307\n",
      "2024-11-14 04:40:20,703 - root - INFO - KG Training: Epoch 0001 Iter 0218 / 3136 | Time 0.1s | Iter Loss 0.3935 | Iter Mean Loss 0.5301\n",
      "2024-11-14 04:40:20,767 - root - INFO - KG Training: Epoch 0001 Iter 0219 / 3136 | Time 0.1s | Iter Loss 0.3963 | Iter Mean Loss 0.5294\n",
      "2024-11-14 04:40:20,833 - root - INFO - KG Training: Epoch 0001 Iter 0220 / 3136 | Time 0.1s | Iter Loss 0.3931 | Iter Mean Loss 0.5288\n",
      "2024-11-14 04:40:20,897 - root - INFO - KG Training: Epoch 0001 Iter 0221 / 3136 | Time 0.1s | Iter Loss 0.3865 | Iter Mean Loss 0.5282\n",
      "2024-11-14 04:40:20,959 - root - INFO - KG Training: Epoch 0001 Iter 0222 / 3136 | Time 0.1s | Iter Loss 0.3981 | Iter Mean Loss 0.5276\n",
      "2024-11-14 04:40:21,026 - root - INFO - KG Training: Epoch 0001 Iter 0223 / 3136 | Time 0.1s | Iter Loss 0.3884 | Iter Mean Loss 0.5270\n",
      "2024-11-14 04:40:21,090 - root - INFO - KG Training: Epoch 0001 Iter 0224 / 3136 | Time 0.1s | Iter Loss 0.3911 | Iter Mean Loss 0.5264\n",
      "2024-11-14 04:40:21,154 - root - INFO - KG Training: Epoch 0001 Iter 0225 / 3136 | Time 0.1s | Iter Loss 0.3909 | Iter Mean Loss 0.5258\n",
      "2024-11-14 04:40:21,216 - root - INFO - KG Training: Epoch 0001 Iter 0226 / 3136 | Time 0.1s | Iter Loss 0.3895 | Iter Mean Loss 0.5252\n",
      "2024-11-14 04:40:21,280 - root - INFO - KG Training: Epoch 0001 Iter 0227 / 3136 | Time 0.1s | Iter Loss 0.3860 | Iter Mean Loss 0.5245\n",
      "2024-11-14 04:40:21,344 - root - INFO - KG Training: Epoch 0001 Iter 0228 / 3136 | Time 0.1s | Iter Loss 0.3840 | Iter Mean Loss 0.5239\n",
      "2024-11-14 04:40:21,406 - root - INFO - KG Training: Epoch 0001 Iter 0229 / 3136 | Time 0.1s | Iter Loss 0.3834 | Iter Mean Loss 0.5233\n",
      "2024-11-14 04:40:21,470 - root - INFO - KG Training: Epoch 0001 Iter 0230 / 3136 | Time 0.1s | Iter Loss 0.3853 | Iter Mean Loss 0.5227\n",
      "2024-11-14 04:40:21,532 - root - INFO - KG Training: Epoch 0001 Iter 0231 / 3136 | Time 0.1s | Iter Loss 0.3846 | Iter Mean Loss 0.5221\n",
      "2024-11-14 04:40:21,596 - root - INFO - KG Training: Epoch 0001 Iter 0232 / 3136 | Time 0.1s | Iter Loss 0.3842 | Iter Mean Loss 0.5215\n",
      "2024-11-14 04:40:21,662 - root - INFO - KG Training: Epoch 0001 Iter 0233 / 3136 | Time 0.1s | Iter Loss 0.3846 | Iter Mean Loss 0.5209\n",
      "2024-11-14 04:40:21,826 - root - INFO - KG Training: Epoch 0001 Iter 0234 / 3136 | Time 0.2s | Iter Loss 0.3700 | Iter Mean Loss 0.5203\n",
      "2024-11-14 04:40:21,899 - root - INFO - KG Training: Epoch 0001 Iter 0235 / 3136 | Time 0.1s | Iter Loss 0.3738 | Iter Mean Loss 0.5197\n",
      "2024-11-14 04:40:21,958 - root - INFO - KG Training: Epoch 0001 Iter 0236 / 3136 | Time 0.1s | Iter Loss 0.3686 | Iter Mean Loss 0.5190\n",
      "2024-11-14 04:40:22,021 - root - INFO - KG Training: Epoch 0001 Iter 0237 / 3136 | Time 0.1s | Iter Loss 0.3778 | Iter Mean Loss 0.5184\n",
      "2024-11-14 04:40:22,088 - root - INFO - KG Training: Epoch 0001 Iter 0238 / 3136 | Time 0.1s | Iter Loss 0.3589 | Iter Mean Loss 0.5178\n",
      "2024-11-14 04:40:22,157 - root - INFO - KG Training: Epoch 0001 Iter 0239 / 3136 | Time 0.1s | Iter Loss 0.3843 | Iter Mean Loss 0.5172\n",
      "2024-11-14 04:40:22,220 - root - INFO - KG Training: Epoch 0001 Iter 0240 / 3136 | Time 0.1s | Iter Loss 0.3779 | Iter Mean Loss 0.5166\n",
      "2024-11-14 04:40:22,284 - root - INFO - KG Training: Epoch 0001 Iter 0241 / 3136 | Time 0.1s | Iter Loss 0.3842 | Iter Mean Loss 0.5161\n",
      "2024-11-14 04:40:22,346 - root - INFO - KG Training: Epoch 0001 Iter 0242 / 3136 | Time 0.1s | Iter Loss 0.3702 | Iter Mean Loss 0.5155\n",
      "2024-11-14 04:40:22,408 - root - INFO - KG Training: Epoch 0001 Iter 0243 / 3136 | Time 0.1s | Iter Loss 0.3662 | Iter Mean Loss 0.5149\n",
      "2024-11-14 04:40:22,470 - root - INFO - KG Training: Epoch 0001 Iter 0244 / 3136 | Time 0.1s | Iter Loss 0.3711 | Iter Mean Loss 0.5143\n",
      "2024-11-14 04:40:22,529 - root - INFO - KG Training: Epoch 0001 Iter 0245 / 3136 | Time 0.1s | Iter Loss 0.3748 | Iter Mean Loss 0.5137\n",
      "2024-11-14 04:40:22,600 - root - INFO - KG Training: Epoch 0001 Iter 0246 / 3136 | Time 0.1s | Iter Loss 0.3702 | Iter Mean Loss 0.5131\n",
      "2024-11-14 04:40:22,662 - root - INFO - KG Training: Epoch 0001 Iter 0247 / 3136 | Time 0.1s | Iter Loss 0.3719 | Iter Mean Loss 0.5125\n",
      "2024-11-14 04:40:22,721 - root - INFO - KG Training: Epoch 0001 Iter 0248 / 3136 | Time 0.1s | Iter Loss 0.3639 | Iter Mean Loss 0.5119\n",
      "2024-11-14 04:40:22,783 - root - INFO - KG Training: Epoch 0001 Iter 0249 / 3136 | Time 0.1s | Iter Loss 0.3643 | Iter Mean Loss 0.5113\n",
      "2024-11-14 04:40:22,849 - root - INFO - KG Training: Epoch 0001 Iter 0250 / 3136 | Time 0.1s | Iter Loss 0.3693 | Iter Mean Loss 0.5108\n",
      "2024-11-14 04:40:22,909 - root - INFO - KG Training: Epoch 0001 Iter 0251 / 3136 | Time 0.1s | Iter Loss 0.3677 | Iter Mean Loss 0.5102\n",
      "2024-11-14 04:40:22,972 - root - INFO - KG Training: Epoch 0001 Iter 0252 / 3136 | Time 0.1s | Iter Loss 0.3608 | Iter Mean Loss 0.5096\n",
      "2024-11-14 04:40:23,031 - root - INFO - KG Training: Epoch 0001 Iter 0253 / 3136 | Time 0.1s | Iter Loss 0.3591 | Iter Mean Loss 0.5090\n",
      "2024-11-14 04:40:23,093 - root - INFO - KG Training: Epoch 0001 Iter 0254 / 3136 | Time 0.1s | Iter Loss 0.3500 | Iter Mean Loss 0.5084\n",
      "2024-11-14 04:40:23,155 - root - INFO - KG Training: Epoch 0001 Iter 0255 / 3136 | Time 0.1s | Iter Loss 0.3686 | Iter Mean Loss 0.5078\n",
      "2024-11-14 04:40:23,216 - root - INFO - KG Training: Epoch 0001 Iter 0256 / 3136 | Time 0.1s | Iter Loss 0.3595 | Iter Mean Loss 0.5073\n",
      "2024-11-14 04:40:23,277 - root - INFO - KG Training: Epoch 0001 Iter 0257 / 3136 | Time 0.1s | Iter Loss 0.3642 | Iter Mean Loss 0.5067\n",
      "2024-11-14 04:40:23,336 - root - INFO - KG Training: Epoch 0001 Iter 0258 / 3136 | Time 0.1s | Iter Loss 0.3590 | Iter Mean Loss 0.5061\n",
      "2024-11-14 04:40:23,397 - root - INFO - KG Training: Epoch 0001 Iter 0259 / 3136 | Time 0.1s | Iter Loss 0.3590 | Iter Mean Loss 0.5056\n",
      "2024-11-14 04:40:23,462 - root - INFO - KG Training: Epoch 0001 Iter 0260 / 3136 | Time 0.1s | Iter Loss 0.3452 | Iter Mean Loss 0.5050\n",
      "2024-11-14 04:40:23,523 - root - INFO - KG Training: Epoch 0001 Iter 0261 / 3136 | Time 0.1s | Iter Loss 0.3525 | Iter Mean Loss 0.5044\n",
      "2024-11-14 04:40:23,584 - root - INFO - KG Training: Epoch 0001 Iter 0262 / 3136 | Time 0.1s | Iter Loss 0.3483 | Iter Mean Loss 0.5038\n",
      "2024-11-14 04:40:23,645 - root - INFO - KG Training: Epoch 0001 Iter 0263 / 3136 | Time 0.1s | Iter Loss 0.3515 | Iter Mean Loss 0.5032\n",
      "2024-11-14 04:40:23,704 - root - INFO - KG Training: Epoch 0001 Iter 0264 / 3136 | Time 0.1s | Iter Loss 0.3556 | Iter Mean Loss 0.5026\n",
      "2024-11-14 04:40:23,765 - root - INFO - KG Training: Epoch 0001 Iter 0265 / 3136 | Time 0.1s | Iter Loss 0.3540 | Iter Mean Loss 0.5021\n",
      "2024-11-14 04:40:23,825 - root - INFO - KG Training: Epoch 0001 Iter 0266 / 3136 | Time 0.1s | Iter Loss 0.3578 | Iter Mean Loss 0.5015\n",
      "2024-11-14 04:40:23,889 - root - INFO - KG Training: Epoch 0001 Iter 0267 / 3136 | Time 0.1s | Iter Loss 0.3509 | Iter Mean Loss 0.5010\n",
      "2024-11-14 04:40:23,954 - root - INFO - KG Training: Epoch 0001 Iter 0268 / 3136 | Time 0.1s | Iter Loss 0.3433 | Iter Mean Loss 0.5004\n",
      "2024-11-14 04:40:24,014 - root - INFO - KG Training: Epoch 0001 Iter 0269 / 3136 | Time 0.1s | Iter Loss 0.3508 | Iter Mean Loss 0.4998\n",
      "2024-11-14 04:40:24,076 - root - INFO - KG Training: Epoch 0001 Iter 0270 / 3136 | Time 0.1s | Iter Loss 0.3516 | Iter Mean Loss 0.4993\n",
      "2024-11-14 04:40:24,140 - root - INFO - KG Training: Epoch 0001 Iter 0271 / 3136 | Time 0.1s | Iter Loss 0.3417 | Iter Mean Loss 0.4987\n",
      "2024-11-14 04:40:24,202 - root - INFO - KG Training: Epoch 0001 Iter 0272 / 3136 | Time 0.1s | Iter Loss 0.3500 | Iter Mean Loss 0.4981\n",
      "2024-11-14 04:40:24,273 - root - INFO - KG Training: Epoch 0001 Iter 0273 / 3136 | Time 0.1s | Iter Loss 0.3393 | Iter Mean Loss 0.4976\n",
      "2024-11-14 04:40:24,337 - root - INFO - KG Training: Epoch 0001 Iter 0274 / 3136 | Time 0.1s | Iter Loss 0.3340 | Iter Mean Loss 0.4970\n",
      "2024-11-14 04:40:24,453 - root - INFO - KG Training: Epoch 0001 Iter 0275 / 3136 | Time 0.1s | Iter Loss 0.3435 | Iter Mean Loss 0.4964\n",
      "2024-11-14 04:40:24,517 - root - INFO - KG Training: Epoch 0001 Iter 0276 / 3136 | Time 0.1s | Iter Loss 0.3352 | Iter Mean Loss 0.4958\n",
      "2024-11-14 04:40:24,579 - root - INFO - KG Training: Epoch 0001 Iter 0277 / 3136 | Time 0.1s | Iter Loss 0.3432 | Iter Mean Loss 0.4953\n",
      "2024-11-14 04:40:24,646 - root - INFO - KG Training: Epoch 0001 Iter 0278 / 3136 | Time 0.1s | Iter Loss 0.3387 | Iter Mean Loss 0.4947\n",
      "2024-11-14 04:40:24,708 - root - INFO - KG Training: Epoch 0001 Iter 0279 / 3136 | Time 0.1s | Iter Loss 0.3364 | Iter Mean Loss 0.4941\n",
      "2024-11-14 04:40:24,771 - root - INFO - KG Training: Epoch 0001 Iter 0280 / 3136 | Time 0.1s | Iter Loss 0.3368 | Iter Mean Loss 0.4936\n",
      "2024-11-14 04:40:24,833 - root - INFO - KG Training: Epoch 0001 Iter 0281 / 3136 | Time 0.1s | Iter Loss 0.3470 | Iter Mean Loss 0.4931\n",
      "2024-11-14 04:40:24,896 - root - INFO - KG Training: Epoch 0001 Iter 0282 / 3136 | Time 0.1s | Iter Loss 0.3378 | Iter Mean Loss 0.4925\n",
      "2024-11-14 04:40:24,960 - root - INFO - KG Training: Epoch 0001 Iter 0283 / 3136 | Time 0.1s | Iter Loss 0.3394 | Iter Mean Loss 0.4920\n",
      "2024-11-14 04:40:25,024 - root - INFO - KG Training: Epoch 0001 Iter 0284 / 3136 | Time 0.1s | Iter Loss 0.3293 | Iter Mean Loss 0.4914\n",
      "2024-11-14 04:40:25,086 - root - INFO - KG Training: Epoch 0001 Iter 0285 / 3136 | Time 0.1s | Iter Loss 0.3286 | Iter Mean Loss 0.4908\n",
      "2024-11-14 04:40:25,150 - root - INFO - KG Training: Epoch 0001 Iter 0286 / 3136 | Time 0.1s | Iter Loss 0.3332 | Iter Mean Loss 0.4903\n",
      "2024-11-14 04:40:25,216 - root - INFO - KG Training: Epoch 0001 Iter 0287 / 3136 | Time 0.1s | Iter Loss 0.3225 | Iter Mean Loss 0.4897\n",
      "2024-11-14 04:40:25,281 - root - INFO - KG Training: Epoch 0001 Iter 0288 / 3136 | Time 0.1s | Iter Loss 0.3357 | Iter Mean Loss 0.4892\n",
      "2024-11-14 04:40:25,346 - root - INFO - KG Training: Epoch 0001 Iter 0289 / 3136 | Time 0.1s | Iter Loss 0.3294 | Iter Mean Loss 0.4886\n",
      "2024-11-14 04:40:25,409 - root - INFO - KG Training: Epoch 0001 Iter 0290 / 3136 | Time 0.1s | Iter Loss 0.3220 | Iter Mean Loss 0.4880\n",
      "2024-11-14 04:40:25,470 - root - INFO - KG Training: Epoch 0001 Iter 0291 / 3136 | Time 0.1s | Iter Loss 0.3276 | Iter Mean Loss 0.4875\n",
      "2024-11-14 04:40:25,531 - root - INFO - KG Training: Epoch 0001 Iter 0292 / 3136 | Time 0.1s | Iter Loss 0.3202 | Iter Mean Loss 0.4869\n",
      "2024-11-14 04:40:25,594 - root - INFO - KG Training: Epoch 0001 Iter 0293 / 3136 | Time 0.1s | Iter Loss 0.3229 | Iter Mean Loss 0.4863\n",
      "2024-11-14 04:40:25,875 - root - INFO - KG Training: Epoch 0001 Iter 0294 / 3136 | Time 0.3s | Iter Loss 0.3221 | Iter Mean Loss 0.4858\n",
      "2024-11-14 04:40:25,942 - root - INFO - KG Training: Epoch 0001 Iter 0295 / 3136 | Time 0.1s | Iter Loss 0.3215 | Iter Mean Loss 0.4852\n",
      "2024-11-14 04:40:26,005 - root - INFO - KG Training: Epoch 0001 Iter 0296 / 3136 | Time 0.1s | Iter Loss 0.3094 | Iter Mean Loss 0.4846\n",
      "2024-11-14 04:40:26,069 - root - INFO - KG Training: Epoch 0001 Iter 0297 / 3136 | Time 0.1s | Iter Loss 0.3266 | Iter Mean Loss 0.4841\n",
      "2024-11-14 04:40:26,239 - root - INFO - KG Training: Epoch 0001 Iter 0298 / 3136 | Time 0.2s | Iter Loss 0.3240 | Iter Mean Loss 0.4836\n",
      "2024-11-14 04:40:26,471 - root - INFO - KG Training: Epoch 0001 Iter 0299 / 3136 | Time 0.2s | Iter Loss 0.3169 | Iter Mean Loss 0.4830\n",
      "2024-11-14 04:40:26,535 - root - INFO - KG Training: Epoch 0001 Iter 0300 / 3136 | Time 0.1s | Iter Loss 0.3143 | Iter Mean Loss 0.4824\n",
      "2024-11-14 04:40:26,665 - root - INFO - KG Training: Epoch 0001 Iter 0301 / 3136 | Time 0.1s | Iter Loss 0.3280 | Iter Mean Loss 0.4819\n",
      "2024-11-14 04:40:26,727 - root - INFO - KG Training: Epoch 0001 Iter 0302 / 3136 | Time 0.1s | Iter Loss 0.3141 | Iter Mean Loss 0.4814\n",
      "2024-11-14 04:40:26,795 - root - INFO - KG Training: Epoch 0001 Iter 0303 / 3136 | Time 0.1s | Iter Loss 0.3194 | Iter Mean Loss 0.4808\n",
      "2024-11-14 04:40:26,864 - root - INFO - KG Training: Epoch 0001 Iter 0304 / 3136 | Time 0.1s | Iter Loss 0.3179 | Iter Mean Loss 0.4803\n",
      "2024-11-14 04:40:26,927 - root - INFO - KG Training: Epoch 0001 Iter 0305 / 3136 | Time 0.1s | Iter Loss 0.3101 | Iter Mean Loss 0.4797\n",
      "2024-11-14 04:40:27,312 - root - INFO - KG Training: Epoch 0001 Iter 0306 / 3136 | Time 0.4s | Iter Loss 0.3179 | Iter Mean Loss 0.4792\n",
      "2024-11-14 04:40:27,381 - root - INFO - KG Training: Epoch 0001 Iter 0307 / 3136 | Time 0.1s | Iter Loss 0.3036 | Iter Mean Loss 0.4786\n",
      "2024-11-14 04:40:27,442 - root - INFO - KG Training: Epoch 0001 Iter 0308 / 3136 | Time 0.1s | Iter Loss 0.3202 | Iter Mean Loss 0.4781\n",
      "2024-11-14 04:40:27,502 - root - INFO - KG Training: Epoch 0001 Iter 0309 / 3136 | Time 0.1s | Iter Loss 0.3115 | Iter Mean Loss 0.4776\n",
      "2024-11-14 04:40:27,561 - root - INFO - KG Training: Epoch 0001 Iter 0310 / 3136 | Time 0.1s | Iter Loss 0.3152 | Iter Mean Loss 0.4771\n",
      "2024-11-14 04:40:27,622 - root - INFO - KG Training: Epoch 0001 Iter 0311 / 3136 | Time 0.1s | Iter Loss 0.3186 | Iter Mean Loss 0.4766\n",
      "2024-11-14 04:40:27,785 - root - INFO - KG Training: Epoch 0001 Iter 0312 / 3136 | Time 0.2s | Iter Loss 0.3277 | Iter Mean Loss 0.4761\n",
      "2024-11-14 04:40:27,848 - root - INFO - KG Training: Epoch 0001 Iter 0313 / 3136 | Time 0.1s | Iter Loss 0.3197 | Iter Mean Loss 0.4756\n",
      "2024-11-14 04:40:27,904 - root - INFO - KG Training: Epoch 0001 Iter 0314 / 3136 | Time 0.1s | Iter Loss 0.3167 | Iter Mean Loss 0.4751\n",
      "2024-11-14 04:40:27,964 - root - INFO - KG Training: Epoch 0001 Iter 0315 / 3136 | Time 0.1s | Iter Loss 0.3044 | Iter Mean Loss 0.4745\n",
      "2024-11-14 04:40:28,035 - root - INFO - KG Training: Epoch 0001 Iter 0316 / 3136 | Time 0.1s | Iter Loss 0.3056 | Iter Mean Loss 0.4740\n",
      "2024-11-14 04:40:28,098 - root - INFO - KG Training: Epoch 0001 Iter 0317 / 3136 | Time 0.1s | Iter Loss 0.3031 | Iter Mean Loss 0.4735\n",
      "2024-11-14 04:40:28,164 - root - INFO - KG Training: Epoch 0001 Iter 0318 / 3136 | Time 0.1s | Iter Loss 0.3047 | Iter Mean Loss 0.4729\n",
      "2024-11-14 04:40:28,276 - root - INFO - KG Training: Epoch 0001 Iter 0319 / 3136 | Time 0.1s | Iter Loss 0.3113 | Iter Mean Loss 0.4724\n",
      "2024-11-14 04:40:28,338 - root - INFO - KG Training: Epoch 0001 Iter 0320 / 3136 | Time 0.1s | Iter Loss 0.3014 | Iter Mean Loss 0.4719\n",
      "2024-11-14 04:40:28,401 - root - INFO - KG Training: Epoch 0001 Iter 0321 / 3136 | Time 0.1s | Iter Loss 0.3073 | Iter Mean Loss 0.4714\n",
      "2024-11-14 04:40:28,464 - root - INFO - KG Training: Epoch 0001 Iter 0322 / 3136 | Time 0.1s | Iter Loss 0.3025 | Iter Mean Loss 0.4709\n",
      "2024-11-14 04:40:28,525 - root - INFO - KG Training: Epoch 0001 Iter 0323 / 3136 | Time 0.1s | Iter Loss 0.3009 | Iter Mean Loss 0.4703\n",
      "2024-11-14 04:40:28,587 - root - INFO - KG Training: Epoch 0001 Iter 0324 / 3136 | Time 0.1s | Iter Loss 0.2898 | Iter Mean Loss 0.4698\n",
      "2024-11-14 04:40:28,649 - root - INFO - KG Training: Epoch 0001 Iter 0325 / 3136 | Time 0.1s | Iter Loss 0.3009 | Iter Mean Loss 0.4693\n",
      "2024-11-14 04:40:28,711 - root - INFO - KG Training: Epoch 0001 Iter 0326 / 3136 | Time 0.1s | Iter Loss 0.3025 | Iter Mean Loss 0.4687\n",
      "2024-11-14 04:40:28,775 - root - INFO - KG Training: Epoch 0001 Iter 0327 / 3136 | Time 0.1s | Iter Loss 0.2983 | Iter Mean Loss 0.4682\n",
      "2024-11-14 04:40:28,840 - root - INFO - KG Training: Epoch 0001 Iter 0328 / 3136 | Time 0.1s | Iter Loss 0.3025 | Iter Mean Loss 0.4677\n",
      "2024-11-14 04:40:28,900 - root - INFO - KG Training: Epoch 0001 Iter 0329 / 3136 | Time 0.1s | Iter Loss 0.2975 | Iter Mean Loss 0.4672\n",
      "2024-11-14 04:40:28,959 - root - INFO - KG Training: Epoch 0001 Iter 0330 / 3136 | Time 0.1s | Iter Loss 0.2990 | Iter Mean Loss 0.4667\n",
      "2024-11-14 04:40:29,021 - root - INFO - KG Training: Epoch 0001 Iter 0331 / 3136 | Time 0.1s | Iter Loss 0.2952 | Iter Mean Loss 0.4662\n",
      "2024-11-14 04:40:29,082 - root - INFO - KG Training: Epoch 0001 Iter 0332 / 3136 | Time 0.1s | Iter Loss 0.3037 | Iter Mean Loss 0.4657\n",
      "2024-11-14 04:40:29,142 - root - INFO - KG Training: Epoch 0001 Iter 0333 / 3136 | Time 0.1s | Iter Loss 0.2994 | Iter Mean Loss 0.4652\n",
      "2024-11-14 04:40:29,203 - root - INFO - KG Training: Epoch 0001 Iter 0334 / 3136 | Time 0.1s | Iter Loss 0.2876 | Iter Mean Loss 0.4646\n",
      "2024-11-14 04:40:29,313 - root - INFO - KG Training: Epoch 0001 Iter 0335 / 3136 | Time 0.1s | Iter Loss 0.2848 | Iter Mean Loss 0.4641\n",
      "2024-11-14 04:40:29,374 - root - INFO - KG Training: Epoch 0001 Iter 0336 / 3136 | Time 0.1s | Iter Loss 0.2993 | Iter Mean Loss 0.4636\n",
      "2024-11-14 04:40:29,436 - root - INFO - KG Training: Epoch 0001 Iter 0337 / 3136 | Time 0.1s | Iter Loss 0.2939 | Iter Mean Loss 0.4631\n",
      "2024-11-14 04:40:29,497 - root - INFO - KG Training: Epoch 0001 Iter 0338 / 3136 | Time 0.1s | Iter Loss 0.2834 | Iter Mean Loss 0.4626\n",
      "2024-11-14 04:40:29,559 - root - INFO - KG Training: Epoch 0001 Iter 0339 / 3136 | Time 0.1s | Iter Loss 0.2895 | Iter Mean Loss 0.4621\n",
      "2024-11-14 04:40:29,619 - root - INFO - KG Training: Epoch 0001 Iter 0340 / 3136 | Time 0.1s | Iter Loss 0.2881 | Iter Mean Loss 0.4616\n",
      "2024-11-14 04:40:29,679 - root - INFO - KG Training: Epoch 0001 Iter 0341 / 3136 | Time 0.1s | Iter Loss 0.2769 | Iter Mean Loss 0.4610\n",
      "2024-11-14 04:40:29,742 - root - INFO - KG Training: Epoch 0001 Iter 0342 / 3136 | Time 0.1s | Iter Loss 0.2853 | Iter Mean Loss 0.4605\n",
      "2024-11-14 04:40:29,807 - root - INFO - KG Training: Epoch 0001 Iter 0343 / 3136 | Time 0.1s | Iter Loss 0.2785 | Iter Mean Loss 0.4600\n",
      "2024-11-14 04:40:29,869 - root - INFO - KG Training: Epoch 0001 Iter 0344 / 3136 | Time 0.1s | Iter Loss 0.2846 | Iter Mean Loss 0.4595\n",
      "2024-11-14 04:40:29,930 - root - INFO - KG Training: Epoch 0001 Iter 0345 / 3136 | Time 0.1s | Iter Loss 0.2942 | Iter Mean Loss 0.4590\n",
      "2024-11-14 04:40:29,993 - root - INFO - KG Training: Epoch 0001 Iter 0346 / 3136 | Time 0.1s | Iter Loss 0.2798 | Iter Mean Loss 0.4585\n",
      "2024-11-14 04:40:30,062 - root - INFO - KG Training: Epoch 0001 Iter 0347 / 3136 | Time 0.1s | Iter Loss 0.2869 | Iter Mean Loss 0.4580\n",
      "2024-11-14 04:40:30,129 - root - INFO - KG Training: Epoch 0001 Iter 0348 / 3136 | Time 0.1s | Iter Loss 0.2902 | Iter Mean Loss 0.4575\n",
      "2024-11-14 04:40:30,194 - root - INFO - KG Training: Epoch 0001 Iter 0349 / 3136 | Time 0.1s | Iter Loss 0.2813 | Iter Mean Loss 0.4570\n",
      "2024-11-14 04:40:30,251 - root - INFO - KG Training: Epoch 0001 Iter 0350 / 3136 | Time 0.1s | Iter Loss 0.2849 | Iter Mean Loss 0.4565\n",
      "2024-11-14 04:40:30,316 - root - INFO - KG Training: Epoch 0001 Iter 0351 / 3136 | Time 0.1s | Iter Loss 0.2879 | Iter Mean Loss 0.4560\n",
      "2024-11-14 04:40:30,377 - root - INFO - KG Training: Epoch 0001 Iter 0352 / 3136 | Time 0.1s | Iter Loss 0.2868 | Iter Mean Loss 0.4555\n",
      "2024-11-14 04:40:30,440 - root - INFO - KG Training: Epoch 0001 Iter 0353 / 3136 | Time 0.1s | Iter Loss 0.2772 | Iter Mean Loss 0.4550\n",
      "2024-11-14 04:40:30,506 - root - INFO - KG Training: Epoch 0001 Iter 0354 / 3136 | Time 0.1s | Iter Loss 0.2902 | Iter Mean Loss 0.4546\n",
      "2024-11-14 04:40:30,566 - root - INFO - KG Training: Epoch 0001 Iter 0355 / 3136 | Time 0.1s | Iter Loss 0.2790 | Iter Mean Loss 0.4541\n",
      "2024-11-14 04:40:30,627 - root - INFO - KG Training: Epoch 0001 Iter 0356 / 3136 | Time 0.1s | Iter Loss 0.2852 | Iter Mean Loss 0.4536\n",
      "2024-11-14 04:40:30,785 - root - INFO - KG Training: Epoch 0001 Iter 0357 / 3136 | Time 0.2s | Iter Loss 0.2834 | Iter Mean Loss 0.4531\n",
      "2024-11-14 04:40:30,851 - root - INFO - KG Training: Epoch 0001 Iter 0358 / 3136 | Time 0.1s | Iter Loss 0.2833 | Iter Mean Loss 0.4526\n",
      "2024-11-14 04:40:30,964 - root - INFO - KG Training: Epoch 0001 Iter 0359 / 3136 | Time 0.1s | Iter Loss 0.2713 | Iter Mean Loss 0.4521\n",
      "2024-11-14 04:40:31,028 - root - INFO - KG Training: Epoch 0001 Iter 0360 / 3136 | Time 0.1s | Iter Loss 0.2731 | Iter Mean Loss 0.4516\n",
      "2024-11-14 04:40:31,094 - root - INFO - KG Training: Epoch 0001 Iter 0361 / 3136 | Time 0.1s | Iter Loss 0.2765 | Iter Mean Loss 0.4512\n",
      "2024-11-14 04:40:31,158 - root - INFO - KG Training: Epoch 0001 Iter 0362 / 3136 | Time 0.1s | Iter Loss 0.2790 | Iter Mean Loss 0.4507\n",
      "2024-11-14 04:40:31,223 - root - INFO - KG Training: Epoch 0001 Iter 0363 / 3136 | Time 0.1s | Iter Loss 0.2692 | Iter Mean Loss 0.4502\n",
      "2024-11-14 04:40:31,290 - root - INFO - KG Training: Epoch 0001 Iter 0364 / 3136 | Time 0.1s | Iter Loss 0.2712 | Iter Mean Loss 0.4497\n",
      "2024-11-14 04:40:31,352 - root - INFO - KG Training: Epoch 0001 Iter 0365 / 3136 | Time 0.1s | Iter Loss 0.2682 | Iter Mean Loss 0.4492\n",
      "2024-11-14 04:40:31,415 - root - INFO - KG Training: Epoch 0001 Iter 0366 / 3136 | Time 0.1s | Iter Loss 0.2751 | Iter Mean Loss 0.4487\n",
      "2024-11-14 04:40:31,477 - root - INFO - KG Training: Epoch 0001 Iter 0367 / 3136 | Time 0.1s | Iter Loss 0.2662 | Iter Mean Loss 0.4482\n",
      "2024-11-14 04:40:31,541 - root - INFO - KG Training: Epoch 0001 Iter 0368 / 3136 | Time 0.1s | Iter Loss 0.2764 | Iter Mean Loss 0.4478\n",
      "2024-11-14 04:40:31,606 - root - INFO - KG Training: Epoch 0001 Iter 0369 / 3136 | Time 0.1s | Iter Loss 0.2794 | Iter Mean Loss 0.4473\n",
      "2024-11-14 04:40:31,668 - root - INFO - KG Training: Epoch 0001 Iter 0370 / 3136 | Time 0.1s | Iter Loss 0.2631 | Iter Mean Loss 0.4468\n",
      "2024-11-14 04:40:31,733 - root - INFO - KG Training: Epoch 0001 Iter 0371 / 3136 | Time 0.1s | Iter Loss 0.2729 | Iter Mean Loss 0.4463\n",
      "2024-11-14 04:40:31,796 - root - INFO - KG Training: Epoch 0001 Iter 0372 / 3136 | Time 0.1s | Iter Loss 0.2668 | Iter Mean Loss 0.4458\n",
      "2024-11-14 04:40:31,862 - root - INFO - KG Training: Epoch 0001 Iter 0373 / 3136 | Time 0.1s | Iter Loss 0.2628 | Iter Mean Loss 0.4454\n",
      "2024-11-14 04:40:31,928 - root - INFO - KG Training: Epoch 0001 Iter 0374 / 3136 | Time 0.1s | Iter Loss 0.2630 | Iter Mean Loss 0.4449\n",
      "2024-11-14 04:40:31,993 - root - INFO - KG Training: Epoch 0001 Iter 0375 / 3136 | Time 0.1s | Iter Loss 0.2598 | Iter Mean Loss 0.4444\n",
      "2024-11-14 04:40:32,176 - root - INFO - KG Training: Epoch 0001 Iter 0376 / 3136 | Time 0.2s | Iter Loss 0.2766 | Iter Mean Loss 0.4439\n",
      "2024-11-14 04:40:32,242 - root - INFO - KG Training: Epoch 0001 Iter 0377 / 3136 | Time 0.1s | Iter Loss 0.2717 | Iter Mean Loss 0.4435\n",
      "2024-11-14 04:40:32,299 - root - INFO - KG Training: Epoch 0001 Iter 0378 / 3136 | Time 0.1s | Iter Loss 0.2632 | Iter Mean Loss 0.4430\n",
      "2024-11-14 04:40:32,377 - root - INFO - KG Training: Epoch 0001 Iter 0379 / 3136 | Time 0.1s | Iter Loss 0.2560 | Iter Mean Loss 0.4425\n",
      "2024-11-14 04:40:32,436 - root - INFO - KG Training: Epoch 0001 Iter 0380 / 3136 | Time 0.1s | Iter Loss 0.2636 | Iter Mean Loss 0.4420\n",
      "2024-11-14 04:40:32,512 - root - INFO - KG Training: Epoch 0001 Iter 0381 / 3136 | Time 0.1s | Iter Loss 0.2632 | Iter Mean Loss 0.4416\n",
      "2024-11-14 04:40:32,591 - root - INFO - KG Training: Epoch 0001 Iter 0382 / 3136 | Time 0.1s | Iter Loss 0.2624 | Iter Mean Loss 0.4411\n",
      "2024-11-14 04:40:32,653 - root - INFO - KG Training: Epoch 0001 Iter 0383 / 3136 | Time 0.1s | Iter Loss 0.2538 | Iter Mean Loss 0.4406\n",
      "2024-11-14 04:40:32,713 - root - INFO - KG Training: Epoch 0001 Iter 0384 / 3136 | Time 0.1s | Iter Loss 0.2575 | Iter Mean Loss 0.4401\n",
      "2024-11-14 04:40:32,771 - root - INFO - KG Training: Epoch 0001 Iter 0385 / 3136 | Time 0.1s | Iter Loss 0.2689 | Iter Mean Loss 0.4397\n",
      "2024-11-14 04:40:32,848 - root - INFO - KG Training: Epoch 0001 Iter 0386 / 3136 | Time 0.1s | Iter Loss 0.2610 | Iter Mean Loss 0.4392\n",
      "2024-11-14 04:40:32,911 - root - INFO - KG Training: Epoch 0001 Iter 0387 / 3136 | Time 0.1s | Iter Loss 0.2638 | Iter Mean Loss 0.4388\n",
      "2024-11-14 04:40:32,993 - root - INFO - KG Training: Epoch 0001 Iter 0388 / 3136 | Time 0.1s | Iter Loss 0.2543 | Iter Mean Loss 0.4383\n",
      "2024-11-14 04:40:33,168 - root - INFO - KG Training: Epoch 0001 Iter 0389 / 3136 | Time 0.2s | Iter Loss 0.2601 | Iter Mean Loss 0.4378\n",
      "2024-11-14 04:40:33,237 - root - INFO - KG Training: Epoch 0001 Iter 0390 / 3136 | Time 0.1s | Iter Loss 0.2641 | Iter Mean Loss 0.4374\n",
      "2024-11-14 04:40:33,300 - root - INFO - KG Training: Epoch 0001 Iter 0391 / 3136 | Time 0.1s | Iter Loss 0.2675 | Iter Mean Loss 0.4370\n",
      "2024-11-14 04:40:33,362 - root - INFO - KG Training: Epoch 0001 Iter 0392 / 3136 | Time 0.1s | Iter Loss 0.2432 | Iter Mean Loss 0.4365\n",
      "2024-11-14 04:40:33,427 - root - INFO - KG Training: Epoch 0001 Iter 0393 / 3136 | Time 0.1s | Iter Loss 0.2527 | Iter Mean Loss 0.4360\n",
      "2024-11-14 04:40:33,489 - root - INFO - KG Training: Epoch 0001 Iter 0394 / 3136 | Time 0.1s | Iter Loss 0.2548 | Iter Mean Loss 0.4355\n",
      "2024-11-14 04:40:33,550 - root - INFO - KG Training: Epoch 0001 Iter 0395 / 3136 | Time 0.1s | Iter Loss 0.2519 | Iter Mean Loss 0.4351\n",
      "2024-11-14 04:40:33,616 - root - INFO - KG Training: Epoch 0001 Iter 0396 / 3136 | Time 0.1s | Iter Loss 0.2664 | Iter Mean Loss 0.4346\n",
      "2024-11-14 04:40:33,675 - root - INFO - KG Training: Epoch 0001 Iter 0397 / 3136 | Time 0.1s | Iter Loss 0.2618 | Iter Mean Loss 0.4342\n",
      "2024-11-14 04:40:33,744 - root - INFO - KG Training: Epoch 0001 Iter 0398 / 3136 | Time 0.1s | Iter Loss 0.2483 | Iter Mean Loss 0.4337\n",
      "2024-11-14 04:40:33,933 - root - INFO - KG Training: Epoch 0001 Iter 0399 / 3136 | Time 0.2s | Iter Loss 0.2577 | Iter Mean Loss 0.4333\n",
      "2024-11-14 04:40:34,009 - root - INFO - KG Training: Epoch 0001 Iter 0400 / 3136 | Time 0.1s | Iter Loss 0.2591 | Iter Mean Loss 0.4329\n",
      "2024-11-14 04:40:34,070 - root - INFO - KG Training: Epoch 0001 Iter 0401 / 3136 | Time 0.1s | Iter Loss 0.2440 | Iter Mean Loss 0.4324\n",
      "2024-11-14 04:40:34,145 - root - INFO - KG Training: Epoch 0001 Iter 0402 / 3136 | Time 0.1s | Iter Loss 0.2585 | Iter Mean Loss 0.4320\n",
      "2024-11-14 04:40:34,202 - root - INFO - KG Training: Epoch 0001 Iter 0403 / 3136 | Time 0.1s | Iter Loss 0.2456 | Iter Mean Loss 0.4315\n",
      "2024-11-14 04:40:34,271 - root - INFO - KG Training: Epoch 0001 Iter 0404 / 3136 | Time 0.1s | Iter Loss 0.2608 | Iter Mean Loss 0.4311\n",
      "2024-11-14 04:40:34,329 - root - INFO - KG Training: Epoch 0001 Iter 0405 / 3136 | Time 0.1s | Iter Loss 0.2553 | Iter Mean Loss 0.4306\n",
      "2024-11-14 04:40:34,405 - root - INFO - KG Training: Epoch 0001 Iter 0406 / 3136 | Time 0.1s | Iter Loss 0.2394 | Iter Mean Loss 0.4302\n",
      "2024-11-14 04:40:34,462 - root - INFO - KG Training: Epoch 0001 Iter 0407 / 3136 | Time 0.1s | Iter Loss 0.2486 | Iter Mean Loss 0.4297\n",
      "2024-11-14 04:40:34,534 - root - INFO - KG Training: Epoch 0001 Iter 0408 / 3136 | Time 0.1s | Iter Loss 0.2483 | Iter Mean Loss 0.4293\n",
      "2024-11-14 04:40:34,595 - root - INFO - KG Training: Epoch 0001 Iter 0409 / 3136 | Time 0.1s | Iter Loss 0.2564 | Iter Mean Loss 0.4289\n",
      "2024-11-14 04:40:34,658 - root - INFO - KG Training: Epoch 0001 Iter 0410 / 3136 | Time 0.1s | Iter Loss 0.2408 | Iter Mean Loss 0.4284\n",
      "2024-11-14 04:40:34,724 - root - INFO - KG Training: Epoch 0001 Iter 0411 / 3136 | Time 0.1s | Iter Loss 0.2421 | Iter Mean Loss 0.4279\n",
      "2024-11-14 04:40:34,785 - root - INFO - KG Training: Epoch 0001 Iter 0412 / 3136 | Time 0.1s | Iter Loss 0.2543 | Iter Mean Loss 0.4275\n",
      "2024-11-14 04:40:34,847 - root - INFO - KG Training: Epoch 0001 Iter 0413 / 3136 | Time 0.1s | Iter Loss 0.2452 | Iter Mean Loss 0.4271\n",
      "2024-11-14 04:40:34,914 - root - INFO - KG Training: Epoch 0001 Iter 0414 / 3136 | Time 0.1s | Iter Loss 0.2373 | Iter Mean Loss 0.4266\n",
      "2024-11-14 04:40:34,975 - root - INFO - KG Training: Epoch 0001 Iter 0415 / 3136 | Time 0.1s | Iter Loss 0.2466 | Iter Mean Loss 0.4262\n",
      "2024-11-14 04:40:35,035 - root - INFO - KG Training: Epoch 0001 Iter 0416 / 3136 | Time 0.1s | Iter Loss 0.2392 | Iter Mean Loss 0.4257\n",
      "2024-11-14 04:40:35,096 - root - INFO - KG Training: Epoch 0001 Iter 0417 / 3136 | Time 0.1s | Iter Loss 0.2460 | Iter Mean Loss 0.4253\n",
      "2024-11-14 04:40:35,155 - root - INFO - KG Training: Epoch 0001 Iter 0418 / 3136 | Time 0.1s | Iter Loss 0.2433 | Iter Mean Loss 0.4249\n",
      "2024-11-14 04:40:35,217 - root - INFO - KG Training: Epoch 0001 Iter 0419 / 3136 | Time 0.1s | Iter Loss 0.2452 | Iter Mean Loss 0.4244\n",
      "2024-11-14 04:40:35,280 - root - INFO - KG Training: Epoch 0001 Iter 0420 / 3136 | Time 0.1s | Iter Loss 0.2462 | Iter Mean Loss 0.4240\n",
      "2024-11-14 04:40:35,342 - root - INFO - KG Training: Epoch 0001 Iter 0421 / 3136 | Time 0.1s | Iter Loss 0.2511 | Iter Mean Loss 0.4236\n",
      "2024-11-14 04:40:35,404 - root - INFO - KG Training: Epoch 0001 Iter 0422 / 3136 | Time 0.1s | Iter Loss 0.2381 | Iter Mean Loss 0.4232\n",
      "2024-11-14 04:40:35,468 - root - INFO - KG Training: Epoch 0001 Iter 0423 / 3136 | Time 0.1s | Iter Loss 0.2460 | Iter Mean Loss 0.4227\n",
      "2024-11-14 04:40:35,531 - root - INFO - KG Training: Epoch 0001 Iter 0424 / 3136 | Time 0.1s | Iter Loss 0.2466 | Iter Mean Loss 0.4223\n",
      "2024-11-14 04:40:35,594 - root - INFO - KG Training: Epoch 0001 Iter 0425 / 3136 | Time 0.1s | Iter Loss 0.2368 | Iter Mean Loss 0.4219\n",
      "2024-11-14 04:40:35,659 - root - INFO - KG Training: Epoch 0001 Iter 0426 / 3136 | Time 0.1s | Iter Loss 0.2467 | Iter Mean Loss 0.4215\n",
      "2024-11-14 04:40:35,723 - root - INFO - KG Training: Epoch 0001 Iter 0427 / 3136 | Time 0.1s | Iter Loss 0.2361 | Iter Mean Loss 0.4211\n",
      "2024-11-14 04:40:35,784 - root - INFO - KG Training: Epoch 0001 Iter 0428 / 3136 | Time 0.1s | Iter Loss 0.2391 | Iter Mean Loss 0.4206\n",
      "2024-11-14 04:40:35,844 - root - INFO - KG Training: Epoch 0001 Iter 0429 / 3136 | Time 0.1s | Iter Loss 0.2397 | Iter Mean Loss 0.4202\n",
      "2024-11-14 04:40:35,910 - root - INFO - KG Training: Epoch 0001 Iter 0430 / 3136 | Time 0.1s | Iter Loss 0.2367 | Iter Mean Loss 0.4198\n",
      "2024-11-14 04:40:35,972 - root - INFO - KG Training: Epoch 0001 Iter 0431 / 3136 | Time 0.1s | Iter Loss 0.2328 | Iter Mean Loss 0.4193\n",
      "2024-11-14 04:40:36,035 - root - INFO - KG Training: Epoch 0001 Iter 0432 / 3136 | Time 0.1s | Iter Loss 0.2279 | Iter Mean Loss 0.4189\n",
      "2024-11-14 04:40:36,094 - root - INFO - KG Training: Epoch 0001 Iter 0433 / 3136 | Time 0.1s | Iter Loss 0.2278 | Iter Mean Loss 0.4185\n",
      "2024-11-14 04:40:36,154 - root - INFO - KG Training: Epoch 0001 Iter 0434 / 3136 | Time 0.1s | Iter Loss 0.2390 | Iter Mean Loss 0.4180\n",
      "2024-11-14 04:40:36,213 - root - INFO - KG Training: Epoch 0001 Iter 0435 / 3136 | Time 0.1s | Iter Loss 0.2337 | Iter Mean Loss 0.4176\n",
      "2024-11-14 04:40:36,272 - root - INFO - KG Training: Epoch 0001 Iter 0436 / 3136 | Time 0.1s | Iter Loss 0.2385 | Iter Mean Loss 0.4172\n",
      "2024-11-14 04:40:36,333 - root - INFO - KG Training: Epoch 0001 Iter 0437 / 3136 | Time 0.1s | Iter Loss 0.2316 | Iter Mean Loss 0.4168\n",
      "2024-11-14 04:40:36,393 - root - INFO - KG Training: Epoch 0001 Iter 0438 / 3136 | Time 0.1s | Iter Loss 0.2515 | Iter Mean Loss 0.4164\n",
      "2024-11-14 04:40:36,453 - root - INFO - KG Training: Epoch 0001 Iter 0439 / 3136 | Time 0.1s | Iter Loss 0.2384 | Iter Mean Loss 0.4160\n",
      "2024-11-14 04:40:36,550 - root - INFO - KG Training: Epoch 0001 Iter 0440 / 3136 | Time 0.1s | Iter Loss 0.2314 | Iter Mean Loss 0.4156\n",
      "2024-11-14 04:40:36,610 - root - INFO - KG Training: Epoch 0001 Iter 0441 / 3136 | Time 0.1s | Iter Loss 0.2342 | Iter Mean Loss 0.4152\n",
      "2024-11-14 04:40:36,672 - root - INFO - KG Training: Epoch 0001 Iter 0442 / 3136 | Time 0.1s | Iter Loss 0.2403 | Iter Mean Loss 0.4148\n",
      "2024-11-14 04:40:36,738 - root - INFO - KG Training: Epoch 0001 Iter 0443 / 3136 | Time 0.1s | Iter Loss 0.2322 | Iter Mean Loss 0.4144\n",
      "2024-11-14 04:40:36,798 - root - INFO - KG Training: Epoch 0001 Iter 0444 / 3136 | Time 0.1s | Iter Loss 0.2245 | Iter Mean Loss 0.4139\n",
      "2024-11-14 04:40:36,859 - root - INFO - KG Training: Epoch 0001 Iter 0445 / 3136 | Time 0.1s | Iter Loss 0.2395 | Iter Mean Loss 0.4135\n",
      "2024-11-14 04:40:36,918 - root - INFO - KG Training: Epoch 0001 Iter 0446 / 3136 | Time 0.1s | Iter Loss 0.2277 | Iter Mean Loss 0.4131\n",
      "2024-11-14 04:40:36,979 - root - INFO - KG Training: Epoch 0001 Iter 0447 / 3136 | Time 0.1s | Iter Loss 0.2362 | Iter Mean Loss 0.4127\n",
      "2024-11-14 04:40:37,071 - root - INFO - KG Training: Epoch 0001 Iter 0448 / 3136 | Time 0.1s | Iter Loss 0.2321 | Iter Mean Loss 0.4123\n",
      "2024-11-14 04:40:37,132 - root - INFO - KG Training: Epoch 0001 Iter 0449 / 3136 | Time 0.1s | Iter Loss 0.2282 | Iter Mean Loss 0.4119\n",
      "2024-11-14 04:40:37,195 - root - INFO - KG Training: Epoch 0001 Iter 0450 / 3136 | Time 0.1s | Iter Loss 0.2237 | Iter Mean Loss 0.4115\n",
      "2024-11-14 04:40:37,311 - root - INFO - KG Training: Epoch 0001 Iter 0451 / 3136 | Time 0.1s | Iter Loss 0.2227 | Iter Mean Loss 0.4111\n",
      "2024-11-14 04:40:37,372 - root - INFO - KG Training: Epoch 0001 Iter 0452 / 3136 | Time 0.1s | Iter Loss 0.2371 | Iter Mean Loss 0.4107\n",
      "2024-11-14 04:40:37,439 - root - INFO - KG Training: Epoch 0001 Iter 0453 / 3136 | Time 0.1s | Iter Loss 0.2254 | Iter Mean Loss 0.4103\n",
      "2024-11-14 04:40:37,503 - root - INFO - KG Training: Epoch 0001 Iter 0454 / 3136 | Time 0.1s | Iter Loss 0.2298 | Iter Mean Loss 0.4099\n",
      "2024-11-14 04:40:37,568 - root - INFO - KG Training: Epoch 0001 Iter 0455 / 3136 | Time 0.1s | Iter Loss 0.2258 | Iter Mean Loss 0.4095\n",
      "2024-11-14 04:40:37,681 - root - INFO - KG Training: Epoch 0001 Iter 0456 / 3136 | Time 0.1s | Iter Loss 0.2234 | Iter Mean Loss 0.4091\n",
      "2024-11-14 04:40:37,743 - root - INFO - KG Training: Epoch 0001 Iter 0457 / 3136 | Time 0.1s | Iter Loss 0.2249 | Iter Mean Loss 0.4087\n",
      "2024-11-14 04:40:37,803 - root - INFO - KG Training: Epoch 0001 Iter 0458 / 3136 | Time 0.1s | Iter Loss 0.2371 | Iter Mean Loss 0.4083\n",
      "2024-11-14 04:40:37,865 - root - INFO - KG Training: Epoch 0001 Iter 0459 / 3136 | Time 0.1s | Iter Loss 0.2210 | Iter Mean Loss 0.4079\n",
      "2024-11-14 04:40:37,927 - root - INFO - KG Training: Epoch 0001 Iter 0460 / 3136 | Time 0.1s | Iter Loss 0.2287 | Iter Mean Loss 0.4075\n",
      "2024-11-14 04:40:37,991 - root - INFO - KG Training: Epoch 0001 Iter 0461 / 3136 | Time 0.1s | Iter Loss 0.2146 | Iter Mean Loss 0.4071\n",
      "2024-11-14 04:40:38,052 - root - INFO - KG Training: Epoch 0001 Iter 0462 / 3136 | Time 0.1s | Iter Loss 0.2205 | Iter Mean Loss 0.4067\n",
      "2024-11-14 04:40:38,114 - root - INFO - KG Training: Epoch 0001 Iter 0463 / 3136 | Time 0.1s | Iter Loss 0.2221 | Iter Mean Loss 0.4063\n",
      "2024-11-14 04:40:38,177 - root - INFO - KG Training: Epoch 0001 Iter 0464 / 3136 | Time 0.1s | Iter Loss 0.2242 | Iter Mean Loss 0.4059\n",
      "2024-11-14 04:40:38,240 - root - INFO - KG Training: Epoch 0001 Iter 0465 / 3136 | Time 0.1s | Iter Loss 0.2282 | Iter Mean Loss 0.4055\n",
      "2024-11-14 04:40:38,303 - root - INFO - KG Training: Epoch 0001 Iter 0466 / 3136 | Time 0.1s | Iter Loss 0.2208 | Iter Mean Loss 0.4051\n",
      "2024-11-14 04:40:38,366 - root - INFO - KG Training: Epoch 0001 Iter 0467 / 3136 | Time 0.1s | Iter Loss 0.2281 | Iter Mean Loss 0.4047\n",
      "2024-11-14 04:40:38,428 - root - INFO - KG Training: Epoch 0001 Iter 0468 / 3136 | Time 0.1s | Iter Loss 0.2223 | Iter Mean Loss 0.4043\n",
      "2024-11-14 04:40:38,491 - root - INFO - KG Training: Epoch 0001 Iter 0469 / 3136 | Time 0.1s | Iter Loss 0.2184 | Iter Mean Loss 0.4039\n",
      "2024-11-14 04:40:38,552 - root - INFO - KG Training: Epoch 0001 Iter 0470 / 3136 | Time 0.1s | Iter Loss 0.2114 | Iter Mean Loss 0.4035\n",
      "2024-11-14 04:40:38,614 - root - INFO - KG Training: Epoch 0001 Iter 0471 / 3136 | Time 0.1s | Iter Loss 0.2163 | Iter Mean Loss 0.4031\n",
      "2024-11-14 04:40:38,675 - root - INFO - KG Training: Epoch 0001 Iter 0472 / 3136 | Time 0.1s | Iter Loss 0.2203 | Iter Mean Loss 0.4028\n",
      "2024-11-14 04:40:38,739 - root - INFO - KG Training: Epoch 0001 Iter 0473 / 3136 | Time 0.1s | Iter Loss 0.2263 | Iter Mean Loss 0.4024\n",
      "2024-11-14 04:40:38,801 - root - INFO - KG Training: Epoch 0001 Iter 0474 / 3136 | Time 0.1s | Iter Loss 0.2124 | Iter Mean Loss 0.4020\n",
      "2024-11-14 04:40:38,917 - root - INFO - KG Training: Epoch 0001 Iter 0475 / 3136 | Time 0.1s | Iter Loss 0.2179 | Iter Mean Loss 0.4016\n",
      "2024-11-14 04:40:38,987 - root - INFO - KG Training: Epoch 0001 Iter 0476 / 3136 | Time 0.1s | Iter Loss 0.2215 | Iter Mean Loss 0.4012\n",
      "2024-11-14 04:40:39,157 - root - INFO - KG Training: Epoch 0001 Iter 0477 / 3136 | Time 0.2s | Iter Loss 0.2210 | Iter Mean Loss 0.4008\n",
      "2024-11-14 04:40:39,218 - root - INFO - KG Training: Epoch 0001 Iter 0478 / 3136 | Time 0.1s | Iter Loss 0.2163 | Iter Mean Loss 0.4004\n",
      "2024-11-14 04:40:39,281 - root - INFO - KG Training: Epoch 0001 Iter 0479 / 3136 | Time 0.1s | Iter Loss 0.2143 | Iter Mean Loss 0.4001\n",
      "2024-11-14 04:40:39,341 - root - INFO - KG Training: Epoch 0001 Iter 0480 / 3136 | Time 0.1s | Iter Loss 0.2180 | Iter Mean Loss 0.3997\n",
      "2024-11-14 04:40:39,401 - root - INFO - KG Training: Epoch 0001 Iter 0481 / 3136 | Time 0.1s | Iter Loss 0.2091 | Iter Mean Loss 0.3993\n",
      "2024-11-14 04:40:39,461 - root - INFO - KG Training: Epoch 0001 Iter 0482 / 3136 | Time 0.1s | Iter Loss 0.2202 | Iter Mean Loss 0.3989\n",
      "2024-11-14 04:40:39,522 - root - INFO - KG Training: Epoch 0001 Iter 0483 / 3136 | Time 0.1s | Iter Loss 0.2115 | Iter Mean Loss 0.3985\n",
      "2024-11-14 04:40:39,586 - root - INFO - KG Training: Epoch 0001 Iter 0484 / 3136 | Time 0.1s | Iter Loss 0.2196 | Iter Mean Loss 0.3982\n",
      "2024-11-14 04:40:39,644 - root - INFO - KG Training: Epoch 0001 Iter 0485 / 3136 | Time 0.1s | Iter Loss 0.2148 | Iter Mean Loss 0.3978\n",
      "2024-11-14 04:40:39,703 - root - INFO - KG Training: Epoch 0001 Iter 0486 / 3136 | Time 0.1s | Iter Loss 0.2084 | Iter Mean Loss 0.3974\n",
      "2024-11-14 04:40:39,761 - root - INFO - KG Training: Epoch 0001 Iter 0487 / 3136 | Time 0.1s | Iter Loss 0.2151 | Iter Mean Loss 0.3970\n",
      "2024-11-14 04:40:39,824 - root - INFO - KG Training: Epoch 0001 Iter 0488 / 3136 | Time 0.1s | Iter Loss 0.2148 | Iter Mean Loss 0.3966\n",
      "2024-11-14 04:40:39,884 - root - INFO - KG Training: Epoch 0001 Iter 0489 / 3136 | Time 0.1s | Iter Loss 0.2199 | Iter Mean Loss 0.3963\n",
      "2024-11-14 04:40:39,942 - root - INFO - KG Training: Epoch 0001 Iter 0490 / 3136 | Time 0.1s | Iter Loss 0.2094 | Iter Mean Loss 0.3959\n",
      "2024-11-14 04:40:40,002 - root - INFO - KG Training: Epoch 0001 Iter 0491 / 3136 | Time 0.1s | Iter Loss 0.2216 | Iter Mean Loss 0.3955\n",
      "2024-11-14 04:40:40,061 - root - INFO - KG Training: Epoch 0001 Iter 0492 / 3136 | Time 0.1s | Iter Loss 0.2137 | Iter Mean Loss 0.3952\n",
      "2024-11-14 04:40:40,120 - root - INFO - KG Training: Epoch 0001 Iter 0493 / 3136 | Time 0.1s | Iter Loss 0.2113 | Iter Mean Loss 0.3948\n",
      "2024-11-14 04:40:40,181 - root - INFO - KG Training: Epoch 0001 Iter 0494 / 3136 | Time 0.1s | Iter Loss 0.2066 | Iter Mean Loss 0.3944\n",
      "2024-11-14 04:40:40,302 - root - INFO - KG Training: Epoch 0001 Iter 0495 / 3136 | Time 0.1s | Iter Loss 0.2147 | Iter Mean Loss 0.3941\n",
      "2024-11-14 04:40:40,362 - root - INFO - KG Training: Epoch 0001 Iter 0496 / 3136 | Time 0.1s | Iter Loss 0.2109 | Iter Mean Loss 0.3937\n",
      "2024-11-14 04:40:40,425 - root - INFO - KG Training: Epoch 0001 Iter 0497 / 3136 | Time 0.1s | Iter Loss 0.2016 | Iter Mean Loss 0.3933\n",
      "2024-11-14 04:40:40,483 - root - INFO - KG Training: Epoch 0001 Iter 0498 / 3136 | Time 0.1s | Iter Loss 0.2127 | Iter Mean Loss 0.3929\n",
      "2024-11-14 04:40:40,544 - root - INFO - KG Training: Epoch 0001 Iter 0499 / 3136 | Time 0.1s | Iter Loss 0.2117 | Iter Mean Loss 0.3926\n",
      "2024-11-14 04:40:40,606 - root - INFO - KG Training: Epoch 0001 Iter 0500 / 3136 | Time 0.1s | Iter Loss 0.2085 | Iter Mean Loss 0.3922\n",
      "2024-11-14 04:40:40,668 - root - INFO - KG Training: Epoch 0001 Iter 0501 / 3136 | Time 0.1s | Iter Loss 0.2267 | Iter Mean Loss 0.3919\n",
      "2024-11-14 04:40:40,735 - root - INFO - KG Training: Epoch 0001 Iter 0502 / 3136 | Time 0.1s | Iter Loss 0.2148 | Iter Mean Loss 0.3915\n",
      "2024-11-14 04:40:40,793 - root - INFO - KG Training: Epoch 0001 Iter 0503 / 3136 | Time 0.1s | Iter Loss 0.2006 | Iter Mean Loss 0.3911\n",
      "2024-11-14 04:40:40,855 - root - INFO - KG Training: Epoch 0001 Iter 0504 / 3136 | Time 0.1s | Iter Loss 0.2096 | Iter Mean Loss 0.3908\n",
      "2024-11-14 04:40:40,917 - root - INFO - KG Training: Epoch 0001 Iter 0505 / 3136 | Time 0.1s | Iter Loss 0.2055 | Iter Mean Loss 0.3904\n",
      "2024-11-14 04:40:40,985 - root - INFO - KG Training: Epoch 0001 Iter 0506 / 3136 | Time 0.1s | Iter Loss 0.2079 | Iter Mean Loss 0.3901\n",
      "2024-11-14 04:40:41,044 - root - INFO - KG Training: Epoch 0001 Iter 0507 / 3136 | Time 0.1s | Iter Loss 0.2180 | Iter Mean Loss 0.3897\n",
      "2024-11-14 04:40:41,106 - root - INFO - KG Training: Epoch 0001 Iter 0508 / 3136 | Time 0.1s | Iter Loss 0.1982 | Iter Mean Loss 0.3893\n",
      "2024-11-14 04:40:41,168 - root - INFO - KG Training: Epoch 0001 Iter 0509 / 3136 | Time 0.1s | Iter Loss 0.2176 | Iter Mean Loss 0.3890\n",
      "2024-11-14 04:40:41,229 - root - INFO - KG Training: Epoch 0001 Iter 0510 / 3136 | Time 0.1s | Iter Loss 0.2005 | Iter Mean Loss 0.3886\n",
      "2024-11-14 04:40:41,290 - root - INFO - KG Training: Epoch 0001 Iter 0511 / 3136 | Time 0.1s | Iter Loss 0.2032 | Iter Mean Loss 0.3883\n",
      "2024-11-14 04:40:41,351 - root - INFO - KG Training: Epoch 0001 Iter 0512 / 3136 | Time 0.1s | Iter Loss 0.2003 | Iter Mean Loss 0.3879\n",
      "2024-11-14 04:40:41,411 - root - INFO - KG Training: Epoch 0001 Iter 0513 / 3136 | Time 0.1s | Iter Loss 0.2092 | Iter Mean Loss 0.3876\n",
      "2024-11-14 04:40:41,472 - root - INFO - KG Training: Epoch 0001 Iter 0514 / 3136 | Time 0.1s | Iter Loss 0.2044 | Iter Mean Loss 0.3872\n",
      "2024-11-14 04:40:41,532 - root - INFO - KG Training: Epoch 0001 Iter 0515 / 3136 | Time 0.1s | Iter Loss 0.2129 | Iter Mean Loss 0.3869\n",
      "2024-11-14 04:40:41,701 - root - INFO - KG Training: Epoch 0001 Iter 0516 / 3136 | Time 0.2s | Iter Loss 0.2096 | Iter Mean Loss 0.3865\n",
      "2024-11-14 04:40:41,766 - root - INFO - KG Training: Epoch 0001 Iter 0517 / 3136 | Time 0.1s | Iter Loss 0.1969 | Iter Mean Loss 0.3861\n",
      "2024-11-14 04:40:41,822 - root - INFO - KG Training: Epoch 0001 Iter 0518 / 3136 | Time 0.1s | Iter Loss 0.2027 | Iter Mean Loss 0.3858\n",
      "2024-11-14 04:40:41,884 - root - INFO - KG Training: Epoch 0001 Iter 0519 / 3136 | Time 0.1s | Iter Loss 0.2002 | Iter Mean Loss 0.3854\n",
      "2024-11-14 04:40:41,944 - root - INFO - KG Training: Epoch 0001 Iter 0520 / 3136 | Time 0.1s | Iter Loss 0.1986 | Iter Mean Loss 0.3851\n",
      "2024-11-14 04:40:42,006 - root - INFO - KG Training: Epoch 0001 Iter 0521 / 3136 | Time 0.1s | Iter Loss 0.2100 | Iter Mean Loss 0.3847\n",
      "2024-11-14 04:40:42,066 - root - INFO - KG Training: Epoch 0001 Iter 0522 / 3136 | Time 0.1s | Iter Loss 0.1934 | Iter Mean Loss 0.3844\n",
      "2024-11-14 04:40:42,127 - root - INFO - KG Training: Epoch 0001 Iter 0523 / 3136 | Time 0.1s | Iter Loss 0.1929 | Iter Mean Loss 0.3840\n",
      "2024-11-14 04:40:42,190 - root - INFO - KG Training: Epoch 0001 Iter 0524 / 3136 | Time 0.1s | Iter Loss 0.1990 | Iter Mean Loss 0.3837\n",
      "2024-11-14 04:40:42,250 - root - INFO - KG Training: Epoch 0001 Iter 0525 / 3136 | Time 0.1s | Iter Loss 0.2046 | Iter Mean Loss 0.3833\n",
      "2024-11-14 04:40:42,309 - root - INFO - KG Training: Epoch 0001 Iter 0526 / 3136 | Time 0.1s | Iter Loss 0.2002 | Iter Mean Loss 0.3830\n",
      "2024-11-14 04:40:42,368 - root - INFO - KG Training: Epoch 0001 Iter 0527 / 3136 | Time 0.1s | Iter Loss 0.2062 | Iter Mean Loss 0.3826\n",
      "2024-11-14 04:40:42,437 - root - INFO - KG Training: Epoch 0001 Iter 0528 / 3136 | Time 0.1s | Iter Loss 0.2070 | Iter Mean Loss 0.3823\n",
      "2024-11-14 04:40:42,497 - root - INFO - KG Training: Epoch 0001 Iter 0529 / 3136 | Time 0.1s | Iter Loss 0.1962 | Iter Mean Loss 0.3819\n",
      "2024-11-14 04:40:42,558 - root - INFO - KG Training: Epoch 0001 Iter 0530 / 3136 | Time 0.1s | Iter Loss 0.2010 | Iter Mean Loss 0.3816\n",
      "2024-11-14 04:40:42,646 - root - INFO - KG Training: Epoch 0001 Iter 0531 / 3136 | Time 0.1s | Iter Loss 0.1865 | Iter Mean Loss 0.3812\n",
      "2024-11-14 04:40:42,712 - root - INFO - KG Training: Epoch 0001 Iter 0532 / 3136 | Time 0.1s | Iter Loss 0.1960 | Iter Mean Loss 0.3809\n",
      "2024-11-14 04:40:42,778 - root - INFO - KG Training: Epoch 0001 Iter 0533 / 3136 | Time 0.1s | Iter Loss 0.1911 | Iter Mean Loss 0.3805\n",
      "2024-11-14 04:40:42,837 - root - INFO - KG Training: Epoch 0001 Iter 0534 / 3136 | Time 0.1s | Iter Loss 0.1985 | Iter Mean Loss 0.3802\n",
      "2024-11-14 04:40:42,898 - root - INFO - KG Training: Epoch 0001 Iter 0535 / 3136 | Time 0.1s | Iter Loss 0.1969 | Iter Mean Loss 0.3798\n",
      "2024-11-14 04:40:42,956 - root - INFO - KG Training: Epoch 0001 Iter 0536 / 3136 | Time 0.1s | Iter Loss 0.1943 | Iter Mean Loss 0.3795\n",
      "2024-11-14 04:40:43,017 - root - INFO - KG Training: Epoch 0001 Iter 0537 / 3136 | Time 0.1s | Iter Loss 0.2023 | Iter Mean Loss 0.3792\n",
      "2024-11-14 04:40:43,077 - root - INFO - KG Training: Epoch 0001 Iter 0538 / 3136 | Time 0.1s | Iter Loss 0.2006 | Iter Mean Loss 0.3788\n",
      "2024-11-14 04:40:43,135 - root - INFO - KG Training: Epoch 0001 Iter 0539 / 3136 | Time 0.1s | Iter Loss 0.1967 | Iter Mean Loss 0.3785\n",
      "2024-11-14 04:40:43,195 - root - INFO - KG Training: Epoch 0001 Iter 0540 / 3136 | Time 0.1s | Iter Loss 0.2055 | Iter Mean Loss 0.3782\n",
      "2024-11-14 04:40:43,255 - root - INFO - KG Training: Epoch 0001 Iter 0541 / 3136 | Time 0.1s | Iter Loss 0.2088 | Iter Mean Loss 0.3779\n",
      "2024-11-14 04:40:43,313 - root - INFO - KG Training: Epoch 0001 Iter 0542 / 3136 | Time 0.1s | Iter Loss 0.2115 | Iter Mean Loss 0.3776\n",
      "2024-11-14 04:40:43,376 - root - INFO - KG Training: Epoch 0001 Iter 0543 / 3136 | Time 0.1s | Iter Loss 0.2026 | Iter Mean Loss 0.3772\n",
      "2024-11-14 04:40:43,434 - root - INFO - KG Training: Epoch 0001 Iter 0544 / 3136 | Time 0.1s | Iter Loss 0.1931 | Iter Mean Loss 0.3769\n",
      "2024-11-14 04:40:43,502 - root - INFO - KG Training: Epoch 0001 Iter 0545 / 3136 | Time 0.1s | Iter Loss 0.1903 | Iter Mean Loss 0.3766\n",
      "2024-11-14 04:40:43,567 - root - INFO - KG Training: Epoch 0001 Iter 0546 / 3136 | Time 0.1s | Iter Loss 0.1893 | Iter Mean Loss 0.3762\n",
      "2024-11-14 04:40:43,628 - root - INFO - KG Training: Epoch 0001 Iter 0547 / 3136 | Time 0.1s | Iter Loss 0.1960 | Iter Mean Loss 0.3759\n",
      "2024-11-14 04:40:43,690 - root - INFO - KG Training: Epoch 0001 Iter 0548 / 3136 | Time 0.1s | Iter Loss 0.2052 | Iter Mean Loss 0.3756\n",
      "2024-11-14 04:40:43,751 - root - INFO - KG Training: Epoch 0001 Iter 0549 / 3136 | Time 0.1s | Iter Loss 0.2003 | Iter Mean Loss 0.3753\n",
      "2024-11-14 04:40:43,810 - root - INFO - KG Training: Epoch 0001 Iter 0550 / 3136 | Time 0.1s | Iter Loss 0.1930 | Iter Mean Loss 0.3749\n",
      "2024-11-14 04:40:43,871 - root - INFO - KG Training: Epoch 0001 Iter 0551 / 3136 | Time 0.1s | Iter Loss 0.1914 | Iter Mean Loss 0.3746\n",
      "2024-11-14 04:40:43,931 - root - INFO - KG Training: Epoch 0001 Iter 0552 / 3136 | Time 0.1s | Iter Loss 0.1898 | Iter Mean Loss 0.3743\n",
      "2024-11-14 04:40:43,993 - root - INFO - KG Training: Epoch 0001 Iter 0553 / 3136 | Time 0.1s | Iter Loss 0.1942 | Iter Mean Loss 0.3739\n",
      "2024-11-14 04:40:44,055 - root - INFO - KG Training: Epoch 0001 Iter 0554 / 3136 | Time 0.1s | Iter Loss 0.1918 | Iter Mean Loss 0.3736\n",
      "2024-11-14 04:40:44,263 - root - INFO - KG Training: Epoch 0001 Iter 0555 / 3136 | Time 0.2s | Iter Loss 0.1924 | Iter Mean Loss 0.3733\n",
      "2024-11-14 04:40:44,325 - root - INFO - KG Training: Epoch 0001 Iter 0556 / 3136 | Time 0.1s | Iter Loss 0.1937 | Iter Mean Loss 0.3730\n",
      "2024-11-14 04:40:44,388 - root - INFO - KG Training: Epoch 0001 Iter 0557 / 3136 | Time 0.1s | Iter Loss 0.1995 | Iter Mean Loss 0.3726\n",
      "2024-11-14 04:40:44,449 - root - INFO - KG Training: Epoch 0001 Iter 0558 / 3136 | Time 0.1s | Iter Loss 0.1909 | Iter Mean Loss 0.3723\n",
      "2024-11-14 04:40:44,509 - root - INFO - KG Training: Epoch 0001 Iter 0559 / 3136 | Time 0.1s | Iter Loss 0.1964 | Iter Mean Loss 0.3720\n",
      "2024-11-14 04:40:44,569 - root - INFO - KG Training: Epoch 0001 Iter 0560 / 3136 | Time 0.1s | Iter Loss 0.1873 | Iter Mean Loss 0.3717\n",
      "2024-11-14 04:40:44,633 - root - INFO - KG Training: Epoch 0001 Iter 0561 / 3136 | Time 0.1s | Iter Loss 0.2074 | Iter Mean Loss 0.3714\n",
      "2024-11-14 04:40:44,694 - root - INFO - KG Training: Epoch 0001 Iter 0562 / 3136 | Time 0.1s | Iter Loss 0.1788 | Iter Mean Loss 0.3710\n",
      "2024-11-14 04:40:44,754 - root - INFO - KG Training: Epoch 0001 Iter 0563 / 3136 | Time 0.1s | Iter Loss 0.2016 | Iter Mean Loss 0.3707\n",
      "2024-11-14 04:40:44,816 - root - INFO - KG Training: Epoch 0001 Iter 0564 / 3136 | Time 0.1s | Iter Loss 0.1968 | Iter Mean Loss 0.3704\n",
      "2024-11-14 04:40:44,877 - root - INFO - KG Training: Epoch 0001 Iter 0565 / 3136 | Time 0.1s | Iter Loss 0.1920 | Iter Mean Loss 0.3701\n",
      "2024-11-14 04:40:44,936 - root - INFO - KG Training: Epoch 0001 Iter 0566 / 3136 | Time 0.1s | Iter Loss 0.1850 | Iter Mean Loss 0.3698\n",
      "2024-11-14 04:40:44,996 - root - INFO - KG Training: Epoch 0001 Iter 0567 / 3136 | Time 0.1s | Iter Loss 0.1928 | Iter Mean Loss 0.3695\n",
      "2024-11-14 04:40:45,087 - root - INFO - KG Training: Epoch 0001 Iter 0568 / 3136 | Time 0.1s | Iter Loss 0.1857 | Iter Mean Loss 0.3691\n",
      "2024-11-14 04:40:45,145 - root - INFO - KG Training: Epoch 0001 Iter 0569 / 3136 | Time 0.1s | Iter Loss 0.1944 | Iter Mean Loss 0.3688\n",
      "2024-11-14 04:40:45,206 - root - INFO - KG Training: Epoch 0001 Iter 0570 / 3136 | Time 0.1s | Iter Loss 0.1858 | Iter Mean Loss 0.3685\n",
      "2024-11-14 04:40:45,265 - root - INFO - KG Training: Epoch 0001 Iter 0571 / 3136 | Time 0.1s | Iter Loss 0.1802 | Iter Mean Loss 0.3682\n",
      "2024-11-14 04:40:45,324 - root - INFO - KG Training: Epoch 0001 Iter 0572 / 3136 | Time 0.1s | Iter Loss 0.1893 | Iter Mean Loss 0.3679\n",
      "2024-11-14 04:40:45,382 - root - INFO - KG Training: Epoch 0001 Iter 0573 / 3136 | Time 0.1s | Iter Loss 0.1975 | Iter Mean Loss 0.3676\n",
      "2024-11-14 04:40:45,443 - root - INFO - KG Training: Epoch 0001 Iter 0574 / 3136 | Time 0.1s | Iter Loss 0.1888 | Iter Mean Loss 0.3673\n",
      "2024-11-14 04:40:45,502 - root - INFO - KG Training: Epoch 0001 Iter 0575 / 3136 | Time 0.1s | Iter Loss 0.1969 | Iter Mean Loss 0.3670\n",
      "2024-11-14 04:40:45,562 - root - INFO - KG Training: Epoch 0001 Iter 0576 / 3136 | Time 0.1s | Iter Loss 0.1826 | Iter Mean Loss 0.3667\n",
      "2024-11-14 04:40:45,641 - root - INFO - KG Training: Epoch 0001 Iter 0577 / 3136 | Time 0.1s | Iter Loss 0.1859 | Iter Mean Loss 0.3663\n",
      "2024-11-14 04:40:45,705 - root - INFO - KG Training: Epoch 0001 Iter 0578 / 3136 | Time 0.1s | Iter Loss 0.1911 | Iter Mean Loss 0.3660\n",
      "2024-11-14 04:40:45,765 - root - INFO - KG Training: Epoch 0001 Iter 0579 / 3136 | Time 0.1s | Iter Loss 0.1844 | Iter Mean Loss 0.3657\n",
      "2024-11-14 04:40:45,828 - root - INFO - KG Training: Epoch 0001 Iter 0580 / 3136 | Time 0.1s | Iter Loss 0.1922 | Iter Mean Loss 0.3654\n",
      "2024-11-14 04:40:45,886 - root - INFO - KG Training: Epoch 0001 Iter 0581 / 3136 | Time 0.1s | Iter Loss 0.1800 | Iter Mean Loss 0.3651\n",
      "2024-11-14 04:40:45,947 - root - INFO - KG Training: Epoch 0001 Iter 0582 / 3136 | Time 0.1s | Iter Loss 0.1890 | Iter Mean Loss 0.3648\n",
      "2024-11-14 04:40:46,005 - root - INFO - KG Training: Epoch 0001 Iter 0583 / 3136 | Time 0.1s | Iter Loss 0.1829 | Iter Mean Loss 0.3645\n",
      "2024-11-14 04:40:46,061 - root - INFO - KG Training: Epoch 0001 Iter 0584 / 3136 | Time 0.1s | Iter Loss 0.1885 | Iter Mean Loss 0.3642\n",
      "2024-11-14 04:40:46,117 - root - INFO - KG Training: Epoch 0001 Iter 0585 / 3136 | Time 0.1s | Iter Loss 0.1725 | Iter Mean Loss 0.3639\n",
      "2024-11-14 04:40:46,173 - root - INFO - KG Training: Epoch 0001 Iter 0586 / 3136 | Time 0.1s | Iter Loss 0.1840 | Iter Mean Loss 0.3636\n",
      "2024-11-14 04:40:46,233 - root - INFO - KG Training: Epoch 0001 Iter 0587 / 3136 | Time 0.1s | Iter Loss 0.1860 | Iter Mean Loss 0.3633\n",
      "2024-11-14 04:40:46,292 - root - INFO - KG Training: Epoch 0001 Iter 0588 / 3136 | Time 0.1s | Iter Loss 0.1840 | Iter Mean Loss 0.3629\n",
      "2024-11-14 04:40:46,353 - root - INFO - KG Training: Epoch 0001 Iter 0589 / 3136 | Time 0.1s | Iter Loss 0.1779 | Iter Mean Loss 0.3626\n",
      "2024-11-14 04:40:46,415 - root - INFO - KG Training: Epoch 0001 Iter 0590 / 3136 | Time 0.1s | Iter Loss 0.1828 | Iter Mean Loss 0.3623\n",
      "2024-11-14 04:40:46,477 - root - INFO - KG Training: Epoch 0001 Iter 0591 / 3136 | Time 0.1s | Iter Loss 0.1872 | Iter Mean Loss 0.3620\n",
      "2024-11-14 04:40:46,538 - root - INFO - KG Training: Epoch 0001 Iter 0592 / 3136 | Time 0.1s | Iter Loss 0.1872 | Iter Mean Loss 0.3617\n",
      "2024-11-14 04:40:46,597 - root - INFO - KG Training: Epoch 0001 Iter 0593 / 3136 | Time 0.1s | Iter Loss 0.1809 | Iter Mean Loss 0.3614\n",
      "2024-11-14 04:40:46,660 - root - INFO - KG Training: Epoch 0001 Iter 0594 / 3136 | Time 0.1s | Iter Loss 0.1756 | Iter Mean Loss 0.3611\n",
      "2024-11-14 04:40:46,719 - root - INFO - KG Training: Epoch 0001 Iter 0595 / 3136 | Time 0.1s | Iter Loss 0.1854 | Iter Mean Loss 0.3608\n",
      "2024-11-14 04:40:46,946 - root - INFO - KG Training: Epoch 0001 Iter 0596 / 3136 | Time 0.2s | Iter Loss 0.1843 | Iter Mean Loss 0.3605\n",
      "2024-11-14 04:40:47,006 - root - INFO - KG Training: Epoch 0001 Iter 0597 / 3136 | Time 0.1s | Iter Loss 0.1761 | Iter Mean Loss 0.3602\n",
      "2024-11-14 04:40:47,064 - root - INFO - KG Training: Epoch 0001 Iter 0598 / 3136 | Time 0.1s | Iter Loss 0.1733 | Iter Mean Loss 0.3599\n",
      "2024-11-14 04:40:47,123 - root - INFO - KG Training: Epoch 0001 Iter 0599 / 3136 | Time 0.1s | Iter Loss 0.1882 | Iter Mean Loss 0.3596\n",
      "2024-11-14 04:40:47,182 - root - INFO - KG Training: Epoch 0001 Iter 0600 / 3136 | Time 0.1s | Iter Loss 0.1814 | Iter Mean Loss 0.3593\n",
      "2024-11-14 04:40:47,243 - root - INFO - KG Training: Epoch 0001 Iter 0601 / 3136 | Time 0.1s | Iter Loss 0.1831 | Iter Mean Loss 0.3590\n",
      "2024-11-14 04:40:47,302 - root - INFO - KG Training: Epoch 0001 Iter 0602 / 3136 | Time 0.1s | Iter Loss 0.1822 | Iter Mean Loss 0.3587\n",
      "2024-11-14 04:40:47,359 - root - INFO - KG Training: Epoch 0001 Iter 0603 / 3136 | Time 0.1s | Iter Loss 0.1935 | Iter Mean Loss 0.3585\n",
      "2024-11-14 04:40:47,419 - root - INFO - KG Training: Epoch 0001 Iter 0604 / 3136 | Time 0.1s | Iter Loss 0.1816 | Iter Mean Loss 0.3582\n",
      "2024-11-14 04:40:47,477 - root - INFO - KG Training: Epoch 0001 Iter 0605 / 3136 | Time 0.1s | Iter Loss 0.1903 | Iter Mean Loss 0.3579\n",
      "2024-11-14 04:40:47,536 - root - INFO - KG Training: Epoch 0001 Iter 0606 / 3136 | Time 0.1s | Iter Loss 0.1743 | Iter Mean Loss 0.3576\n",
      "2024-11-14 04:40:47,595 - root - INFO - KG Training: Epoch 0001 Iter 0607 / 3136 | Time 0.1s | Iter Loss 0.1837 | Iter Mean Loss 0.3573\n",
      "2024-11-14 04:40:47,656 - root - INFO - KG Training: Epoch 0001 Iter 0608 / 3136 | Time 0.1s | Iter Loss 0.1791 | Iter Mean Loss 0.3570\n",
      "2024-11-14 04:40:47,714 - root - INFO - KG Training: Epoch 0001 Iter 0609 / 3136 | Time 0.1s | Iter Loss 0.1771 | Iter Mean Loss 0.3567\n",
      "2024-11-14 04:40:47,784 - root - INFO - KG Training: Epoch 0001 Iter 0610 / 3136 | Time 0.1s | Iter Loss 0.1885 | Iter Mean Loss 0.3564\n",
      "2024-11-14 04:40:47,844 - root - INFO - KG Training: Epoch 0001 Iter 0611 / 3136 | Time 0.1s | Iter Loss 0.1772 | Iter Mean Loss 0.3561\n",
      "2024-11-14 04:40:47,907 - root - INFO - KG Training: Epoch 0001 Iter 0612 / 3136 | Time 0.1s | Iter Loss 0.1746 | Iter Mean Loss 0.3558\n",
      "2024-11-14 04:40:47,968 - root - INFO - KG Training: Epoch 0001 Iter 0613 / 3136 | Time 0.1s | Iter Loss 0.1679 | Iter Mean Loss 0.3555\n",
      "2024-11-14 04:40:48,037 - root - INFO - KG Training: Epoch 0001 Iter 0614 / 3136 | Time 0.1s | Iter Loss 0.1804 | Iter Mean Loss 0.3553\n",
      "2024-11-14 04:40:48,124 - root - INFO - KG Training: Epoch 0001 Iter 0615 / 3136 | Time 0.1s | Iter Loss 0.1734 | Iter Mean Loss 0.3550\n",
      "2024-11-14 04:40:48,187 - root - INFO - KG Training: Epoch 0001 Iter 0616 / 3136 | Time 0.1s | Iter Loss 0.1795 | Iter Mean Loss 0.3547\n",
      "2024-11-14 04:40:48,367 - root - INFO - KG Training: Epoch 0001 Iter 0617 / 3136 | Time 0.2s | Iter Loss 0.1831 | Iter Mean Loss 0.3544\n",
      "2024-11-14 04:40:48,430 - root - INFO - KG Training: Epoch 0001 Iter 0618 / 3136 | Time 0.1s | Iter Loss 0.1725 | Iter Mean Loss 0.3541\n",
      "2024-11-14 04:40:48,495 - root - INFO - KG Training: Epoch 0001 Iter 0619 / 3136 | Time 0.1s | Iter Loss 0.1711 | Iter Mean Loss 0.3538\n",
      "2024-11-14 04:40:48,552 - root - INFO - KG Training: Epoch 0001 Iter 0620 / 3136 | Time 0.1s | Iter Loss 0.1687 | Iter Mean Loss 0.3535\n",
      "2024-11-14 04:40:48,682 - root - INFO - KG Training: Epoch 0001 Iter 0621 / 3136 | Time 0.1s | Iter Loss 0.1719 | Iter Mean Loss 0.3532\n",
      "2024-11-14 04:40:48,746 - root - INFO - KG Training: Epoch 0001 Iter 0622 / 3136 | Time 0.1s | Iter Loss 0.1676 | Iter Mean Loss 0.3529\n",
      "2024-11-14 04:40:48,807 - root - INFO - KG Training: Epoch 0001 Iter 0623 / 3136 | Time 0.1s | Iter Loss 0.1831 | Iter Mean Loss 0.3526\n",
      "2024-11-14 04:40:48,869 - root - INFO - KG Training: Epoch 0001 Iter 0624 / 3136 | Time 0.1s | Iter Loss 0.1779 | Iter Mean Loss 0.3524\n",
      "2024-11-14 04:40:48,931 - root - INFO - KG Training: Epoch 0001 Iter 0625 / 3136 | Time 0.1s | Iter Loss 0.1858 | Iter Mean Loss 0.3521\n",
      "2024-11-14 04:40:49,127 - root - INFO - KG Training: Epoch 0001 Iter 0626 / 3136 | Time 0.2s | Iter Loss 0.1685 | Iter Mean Loss 0.3518\n",
      "2024-11-14 04:40:49,244 - root - INFO - KG Training: Epoch 0001 Iter 0627 / 3136 | Time 0.1s | Iter Loss 0.1732 | Iter Mean Loss 0.3515\n",
      "2024-11-14 04:40:49,302 - root - INFO - KG Training: Epoch 0001 Iter 0628 / 3136 | Time 0.1s | Iter Loss 0.1704 | Iter Mean Loss 0.3512\n",
      "2024-11-14 04:40:49,361 - root - INFO - KG Training: Epoch 0001 Iter 0629 / 3136 | Time 0.1s | Iter Loss 0.1657 | Iter Mean Loss 0.3509\n",
      "2024-11-14 04:40:49,418 - root - INFO - KG Training: Epoch 0001 Iter 0630 / 3136 | Time 0.1s | Iter Loss 0.1741 | Iter Mean Loss 0.3507\n",
      "2024-11-14 04:40:49,478 - root - INFO - KG Training: Epoch 0001 Iter 0631 / 3136 | Time 0.1s | Iter Loss 0.1689 | Iter Mean Loss 0.3504\n",
      "2024-11-14 04:40:49,537 - root - INFO - KG Training: Epoch 0001 Iter 0632 / 3136 | Time 0.1s | Iter Loss 0.1707 | Iter Mean Loss 0.3501\n",
      "2024-11-14 04:40:49,597 - root - INFO - KG Training: Epoch 0001 Iter 0633 / 3136 | Time 0.1s | Iter Loss 0.1739 | Iter Mean Loss 0.3498\n",
      "2024-11-14 04:40:49,655 - root - INFO - KG Training: Epoch 0001 Iter 0634 / 3136 | Time 0.1s | Iter Loss 0.1838 | Iter Mean Loss 0.3495\n",
      "2024-11-14 04:40:49,764 - root - INFO - KG Training: Epoch 0001 Iter 0635 / 3136 | Time 0.1s | Iter Loss 0.1675 | Iter Mean Loss 0.3493\n",
      "2024-11-14 04:40:49,823 - root - INFO - KG Training: Epoch 0001 Iter 0636 / 3136 | Time 0.1s | Iter Loss 0.1755 | Iter Mean Loss 0.3490\n",
      "2024-11-14 04:40:49,881 - root - INFO - KG Training: Epoch 0001 Iter 0637 / 3136 | Time 0.1s | Iter Loss 0.1645 | Iter Mean Loss 0.3487\n",
      "2024-11-14 04:40:49,942 - root - INFO - KG Training: Epoch 0001 Iter 0638 / 3136 | Time 0.1s | Iter Loss 0.1733 | Iter Mean Loss 0.3484\n",
      "2024-11-14 04:40:50,012 - root - INFO - KG Training: Epoch 0001 Iter 0639 / 3136 | Time 0.1s | Iter Loss 0.1704 | Iter Mean Loss 0.3481\n",
      "2024-11-14 04:40:50,072 - root - INFO - KG Training: Epoch 0001 Iter 0640 / 3136 | Time 0.1s | Iter Loss 0.1796 | Iter Mean Loss 0.3479\n",
      "2024-11-14 04:40:50,295 - root - INFO - KG Training: Epoch 0001 Iter 0641 / 3136 | Time 0.2s | Iter Loss 0.1624 | Iter Mean Loss 0.3476\n",
      "2024-11-14 04:40:50,356 - root - INFO - KG Training: Epoch 0001 Iter 0642 / 3136 | Time 0.1s | Iter Loss 0.1658 | Iter Mean Loss 0.3473\n",
      "2024-11-14 04:40:50,417 - root - INFO - KG Training: Epoch 0001 Iter 0643 / 3136 | Time 0.1s | Iter Loss 0.1698 | Iter Mean Loss 0.3470\n",
      "2024-11-14 04:40:50,477 - root - INFO - KG Training: Epoch 0001 Iter 0644 / 3136 | Time 0.1s | Iter Loss 0.1736 | Iter Mean Loss 0.3468\n",
      "2024-11-14 04:40:50,536 - root - INFO - KG Training: Epoch 0001 Iter 0645 / 3136 | Time 0.1s | Iter Loss 0.1712 | Iter Mean Loss 0.3465\n",
      "2024-11-14 04:40:50,596 - root - INFO - KG Training: Epoch 0001 Iter 0646 / 3136 | Time 0.1s | Iter Loss 0.1746 | Iter Mean Loss 0.3462\n",
      "2024-11-14 04:40:50,656 - root - INFO - KG Training: Epoch 0001 Iter 0647 / 3136 | Time 0.1s | Iter Loss 0.1703 | Iter Mean Loss 0.3459\n",
      "2024-11-14 04:40:50,719 - root - INFO - KG Training: Epoch 0001 Iter 0648 / 3136 | Time 0.1s | Iter Loss 0.1700 | Iter Mean Loss 0.3457\n",
      "2024-11-14 04:40:50,791 - root - INFO - KG Training: Epoch 0001 Iter 0649 / 3136 | Time 0.1s | Iter Loss 0.1632 | Iter Mean Loss 0.3454\n",
      "2024-11-14 04:40:50,853 - root - INFO - KG Training: Epoch 0001 Iter 0650 / 3136 | Time 0.1s | Iter Loss 0.1671 | Iter Mean Loss 0.3451\n",
      "2024-11-14 04:40:50,914 - root - INFO - KG Training: Epoch 0001 Iter 0651 / 3136 | Time 0.1s | Iter Loss 0.1680 | Iter Mean Loss 0.3448\n",
      "2024-11-14 04:40:50,981 - root - INFO - KG Training: Epoch 0001 Iter 0652 / 3136 | Time 0.1s | Iter Loss 0.1659 | Iter Mean Loss 0.3446\n",
      "2024-11-14 04:40:51,044 - root - INFO - KG Training: Epoch 0001 Iter 0653 / 3136 | Time 0.1s | Iter Loss 0.1723 | Iter Mean Loss 0.3443\n",
      "2024-11-14 04:40:51,103 - root - INFO - KG Training: Epoch 0001 Iter 0654 / 3136 | Time 0.1s | Iter Loss 0.1725 | Iter Mean Loss 0.3440\n",
      "2024-11-14 04:40:51,164 - root - INFO - KG Training: Epoch 0001 Iter 0655 / 3136 | Time 0.1s | Iter Loss 0.1679 | Iter Mean Loss 0.3438\n",
      "2024-11-14 04:40:51,224 - root - INFO - KG Training: Epoch 0001 Iter 0656 / 3136 | Time 0.1s | Iter Loss 0.1672 | Iter Mean Loss 0.3435\n",
      "2024-11-14 04:40:51,295 - root - INFO - KG Training: Epoch 0001 Iter 0657 / 3136 | Time 0.1s | Iter Loss 0.1696 | Iter Mean Loss 0.3432\n",
      "2024-11-14 04:40:51,377 - root - INFO - KG Training: Epoch 0001 Iter 0658 / 3136 | Time 0.1s | Iter Loss 0.1776 | Iter Mean Loss 0.3430\n",
      "2024-11-14 04:40:51,442 - root - INFO - KG Training: Epoch 0001 Iter 0659 / 3136 | Time 0.1s | Iter Loss 0.1707 | Iter Mean Loss 0.3427\n",
      "2024-11-14 04:40:51,504 - root - INFO - KG Training: Epoch 0001 Iter 0660 / 3136 | Time 0.1s | Iter Loss 0.1705 | Iter Mean Loss 0.3425\n",
      "2024-11-14 04:40:51,565 - root - INFO - KG Training: Epoch 0001 Iter 0661 / 3136 | Time 0.1s | Iter Loss 0.1692 | Iter Mean Loss 0.3422\n",
      "2024-11-14 04:40:51,627 - root - INFO - KG Training: Epoch 0001 Iter 0662 / 3136 | Time 0.1s | Iter Loss 0.1764 | Iter Mean Loss 0.3420\n",
      "2024-11-14 04:40:51,692 - root - INFO - KG Training: Epoch 0001 Iter 0663 / 3136 | Time 0.1s | Iter Loss 0.1672 | Iter Mean Loss 0.3417\n",
      "2024-11-14 04:40:51,755 - root - INFO - KG Training: Epoch 0001 Iter 0664 / 3136 | Time 0.1s | Iter Loss 0.1698 | Iter Mean Loss 0.3414\n",
      "2024-11-14 04:40:51,815 - root - INFO - KG Training: Epoch 0001 Iter 0665 / 3136 | Time 0.1s | Iter Loss 0.1715 | Iter Mean Loss 0.3412\n",
      "2024-11-14 04:40:51,875 - root - INFO - KG Training: Epoch 0001 Iter 0666 / 3136 | Time 0.1s | Iter Loss 0.1602 | Iter Mean Loss 0.3409\n",
      "2024-11-14 04:40:51,933 - root - INFO - KG Training: Epoch 0001 Iter 0667 / 3136 | Time 0.1s | Iter Loss 0.1723 | Iter Mean Loss 0.3407\n",
      "2024-11-14 04:40:51,998 - root - INFO - KG Training: Epoch 0001 Iter 0668 / 3136 | Time 0.1s | Iter Loss 0.1663 | Iter Mean Loss 0.3404\n",
      "2024-11-14 04:40:52,056 - root - INFO - KG Training: Epoch 0001 Iter 0669 / 3136 | Time 0.1s | Iter Loss 0.1723 | Iter Mean Loss 0.3401\n",
      "2024-11-14 04:40:52,115 - root - INFO - KG Training: Epoch 0001 Iter 0670 / 3136 | Time 0.1s | Iter Loss 0.1645 | Iter Mean Loss 0.3399\n",
      "2024-11-14 04:40:52,174 - root - INFO - KG Training: Epoch 0001 Iter 0671 / 3136 | Time 0.1s | Iter Loss 0.1690 | Iter Mean Loss 0.3396\n",
      "2024-11-14 04:40:52,232 - root - INFO - KG Training: Epoch 0001 Iter 0672 / 3136 | Time 0.1s | Iter Loss 0.1683 | Iter Mean Loss 0.3394\n",
      "2024-11-14 04:40:52,291 - root - INFO - KG Training: Epoch 0001 Iter 0673 / 3136 | Time 0.1s | Iter Loss 0.1611 | Iter Mean Loss 0.3391\n",
      "2024-11-14 04:40:52,349 - root - INFO - KG Training: Epoch 0001 Iter 0674 / 3136 | Time 0.1s | Iter Loss 0.1687 | Iter Mean Loss 0.3389\n",
      "2024-11-14 04:40:52,411 - root - INFO - KG Training: Epoch 0001 Iter 0675 / 3136 | Time 0.1s | Iter Loss 0.1742 | Iter Mean Loss 0.3386\n",
      "2024-11-14 04:40:52,475 - root - INFO - KG Training: Epoch 0001 Iter 0676 / 3136 | Time 0.1s | Iter Loss 0.1654 | Iter Mean Loss 0.3384\n",
      "2024-11-14 04:40:52,536 - root - INFO - KG Training: Epoch 0001 Iter 0677 / 3136 | Time 0.1s | Iter Loss 0.1553 | Iter Mean Loss 0.3381\n",
      "2024-11-14 04:40:52,596 - root - INFO - KG Training: Epoch 0001 Iter 0678 / 3136 | Time 0.1s | Iter Loss 0.1627 | Iter Mean Loss 0.3378\n",
      "2024-11-14 04:40:52,656 - root - INFO - KG Training: Epoch 0001 Iter 0679 / 3136 | Time 0.1s | Iter Loss 0.1630 | Iter Mean Loss 0.3376\n",
      "2024-11-14 04:40:52,716 - root - INFO - KG Training: Epoch 0001 Iter 0680 / 3136 | Time 0.1s | Iter Loss 0.1633 | Iter Mean Loss 0.3373\n",
      "2024-11-14 04:40:52,778 - root - INFO - KG Training: Epoch 0001 Iter 0681 / 3136 | Time 0.1s | Iter Loss 0.1594 | Iter Mean Loss 0.3370\n",
      "2024-11-14 04:40:52,839 - root - INFO - KG Training: Epoch 0001 Iter 0682 / 3136 | Time 0.1s | Iter Loss 0.1630 | Iter Mean Loss 0.3368\n",
      "2024-11-14 04:40:52,899 - root - INFO - KG Training: Epoch 0001 Iter 0683 / 3136 | Time 0.1s | Iter Loss 0.1619 | Iter Mean Loss 0.3365\n",
      "2024-11-14 04:40:52,960 - root - INFO - KG Training: Epoch 0001 Iter 0684 / 3136 | Time 0.1s | Iter Loss 0.1638 | Iter Mean Loss 0.3363\n",
      "2024-11-14 04:40:53,268 - root - INFO - KG Training: Epoch 0001 Iter 0685 / 3136 | Time 0.3s | Iter Loss 0.1529 | Iter Mean Loss 0.3360\n",
      "2024-11-14 04:40:53,329 - root - INFO - KG Training: Epoch 0001 Iter 0686 / 3136 | Time 0.1s | Iter Loss 0.1679 | Iter Mean Loss 0.3358\n",
      "2024-11-14 04:40:53,391 - root - INFO - KG Training: Epoch 0001 Iter 0687 / 3136 | Time 0.1s | Iter Loss 0.1733 | Iter Mean Loss 0.3355\n",
      "2024-11-14 04:40:53,450 - root - INFO - KG Training: Epoch 0001 Iter 0688 / 3136 | Time 0.1s | Iter Loss 0.1681 | Iter Mean Loss 0.3353\n",
      "2024-11-14 04:40:53,510 - root - INFO - KG Training: Epoch 0001 Iter 0689 / 3136 | Time 0.1s | Iter Loss 0.1760 | Iter Mean Loss 0.3351\n",
      "2024-11-14 04:40:53,570 - root - INFO - KG Training: Epoch 0001 Iter 0690 / 3136 | Time 0.1s | Iter Loss 0.1601 | Iter Mean Loss 0.3348\n",
      "2024-11-14 04:40:53,681 - root - INFO - KG Training: Epoch 0001 Iter 0691 / 3136 | Time 0.1s | Iter Loss 0.1640 | Iter Mean Loss 0.3346\n",
      "2024-11-14 04:40:53,743 - root - INFO - KG Training: Epoch 0001 Iter 0692 / 3136 | Time 0.1s | Iter Loss 0.1713 | Iter Mean Loss 0.3343\n",
      "2024-11-14 04:40:53,805 - root - INFO - KG Training: Epoch 0001 Iter 0693 / 3136 | Time 0.1s | Iter Loss 0.1733 | Iter Mean Loss 0.3341\n",
      "2024-11-14 04:40:53,913 - root - INFO - KG Training: Epoch 0001 Iter 0694 / 3136 | Time 0.1s | Iter Loss 0.1575 | Iter Mean Loss 0.3338\n",
      "2024-11-14 04:40:53,996 - root - INFO - KG Training: Epoch 0001 Iter 0695 / 3136 | Time 0.1s | Iter Loss 0.1713 | Iter Mean Loss 0.3336\n",
      "2024-11-14 04:40:54,059 - root - INFO - KG Training: Epoch 0001 Iter 0696 / 3136 | Time 0.1s | Iter Loss 0.1649 | Iter Mean Loss 0.3334\n",
      "2024-11-14 04:40:54,270 - root - INFO - KG Training: Epoch 0001 Iter 0697 / 3136 | Time 0.2s | Iter Loss 0.1732 | Iter Mean Loss 0.3331\n",
      "2024-11-14 04:40:54,330 - root - INFO - KG Training: Epoch 0001 Iter 0698 / 3136 | Time 0.1s | Iter Loss 0.1582 | Iter Mean Loss 0.3329\n",
      "2024-11-14 04:40:54,393 - root - INFO - KG Training: Epoch 0001 Iter 0699 / 3136 | Time 0.1s | Iter Loss 0.1691 | Iter Mean Loss 0.3326\n",
      "2024-11-14 04:40:54,455 - root - INFO - KG Training: Epoch 0001 Iter 0700 / 3136 | Time 0.1s | Iter Loss 0.1584 | Iter Mean Loss 0.3324\n",
      "2024-11-14 04:40:54,516 - root - INFO - KG Training: Epoch 0001 Iter 0701 / 3136 | Time 0.1s | Iter Loss 0.1533 | Iter Mean Loss 0.3321\n",
      "2024-11-14 04:40:54,577 - root - INFO - KG Training: Epoch 0001 Iter 0702 / 3136 | Time 0.1s | Iter Loss 0.1520 | Iter Mean Loss 0.3319\n",
      "2024-11-14 04:40:54,638 - root - INFO - KG Training: Epoch 0001 Iter 0703 / 3136 | Time 0.1s | Iter Loss 0.1506 | Iter Mean Loss 0.3316\n",
      "2024-11-14 04:40:54,704 - root - INFO - KG Training: Epoch 0001 Iter 0704 / 3136 | Time 0.1s | Iter Loss 0.1612 | Iter Mean Loss 0.3314\n",
      "2024-11-14 04:40:54,765 - root - INFO - KG Training: Epoch 0001 Iter 0705 / 3136 | Time 0.1s | Iter Loss 0.1568 | Iter Mean Loss 0.3311\n",
      "2024-11-14 04:40:54,824 - root - INFO - KG Training: Epoch 0001 Iter 0706 / 3136 | Time 0.1s | Iter Loss 0.1535 | Iter Mean Loss 0.3309\n",
      "2024-11-14 04:40:54,885 - root - INFO - KG Training: Epoch 0001 Iter 0707 / 3136 | Time 0.1s | Iter Loss 0.1572 | Iter Mean Loss 0.3306\n",
      "2024-11-14 04:40:54,947 - root - INFO - KG Training: Epoch 0001 Iter 0708 / 3136 | Time 0.1s | Iter Loss 0.1486 | Iter Mean Loss 0.3304\n",
      "2024-11-14 04:40:55,010 - root - INFO - KG Training: Epoch 0001 Iter 0709 / 3136 | Time 0.1s | Iter Loss 0.1709 | Iter Mean Loss 0.3302\n",
      "2024-11-14 04:40:55,083 - root - INFO - KG Training: Epoch 0001 Iter 0710 / 3136 | Time 0.1s | Iter Loss 0.1540 | Iter Mean Loss 0.3299\n",
      "2024-11-14 04:40:55,155 - root - INFO - KG Training: Epoch 0001 Iter 0711 / 3136 | Time 0.1s | Iter Loss 0.1568 | Iter Mean Loss 0.3297\n",
      "2024-11-14 04:40:55,213 - root - INFO - KG Training: Epoch 0001 Iter 0712 / 3136 | Time 0.1s | Iter Loss 0.1585 | Iter Mean Loss 0.3294\n",
      "2024-11-14 04:40:55,361 - root - INFO - KG Training: Epoch 0001 Iter 0713 / 3136 | Time 0.1s | Iter Loss 0.1607 | Iter Mean Loss 0.3292\n",
      "2024-11-14 04:40:55,420 - root - INFO - KG Training: Epoch 0001 Iter 0714 / 3136 | Time 0.1s | Iter Loss 0.1630 | Iter Mean Loss 0.3290\n",
      "2024-11-14 04:40:55,481 - root - INFO - KG Training: Epoch 0001 Iter 0715 / 3136 | Time 0.1s | Iter Loss 0.1561 | Iter Mean Loss 0.3287\n",
      "2024-11-14 04:40:55,540 - root - INFO - KG Training: Epoch 0001 Iter 0716 / 3136 | Time 0.1s | Iter Loss 0.1602 | Iter Mean Loss 0.3285\n",
      "2024-11-14 04:40:55,601 - root - INFO - KG Training: Epoch 0001 Iter 0717 / 3136 | Time 0.1s | Iter Loss 0.1596 | Iter Mean Loss 0.3282\n",
      "2024-11-14 04:40:55,660 - root - INFO - KG Training: Epoch 0001 Iter 0718 / 3136 | Time 0.1s | Iter Loss 0.1572 | Iter Mean Loss 0.3280\n",
      "2024-11-14 04:40:55,718 - root - INFO - KG Training: Epoch 0001 Iter 0719 / 3136 | Time 0.1s | Iter Loss 0.1593 | Iter Mean Loss 0.3278\n",
      "2024-11-14 04:40:55,778 - root - INFO - KG Training: Epoch 0001 Iter 0720 / 3136 | Time 0.1s | Iter Loss 0.1613 | Iter Mean Loss 0.3275\n",
      "2024-11-14 04:40:55,841 - root - INFO - KG Training: Epoch 0001 Iter 0721 / 3136 | Time 0.1s | Iter Loss 0.1563 | Iter Mean Loss 0.3273\n",
      "2024-11-14 04:40:55,899 - root - INFO - KG Training: Epoch 0001 Iter 0722 / 3136 | Time 0.1s | Iter Loss 0.1549 | Iter Mean Loss 0.3271\n",
      "2024-11-14 04:40:56,058 - root - INFO - KG Training: Epoch 0001 Iter 0723 / 3136 | Time 0.2s | Iter Loss 0.1435 | Iter Mean Loss 0.3268\n",
      "2024-11-14 04:40:56,118 - root - INFO - KG Training: Epoch 0001 Iter 0724 / 3136 | Time 0.1s | Iter Loss 0.1541 | Iter Mean Loss 0.3266\n",
      "2024-11-14 04:40:56,179 - root - INFO - KG Training: Epoch 0001 Iter 0725 / 3136 | Time 0.1s | Iter Loss 0.1456 | Iter Mean Loss 0.3263\n",
      "2024-11-14 04:40:56,240 - root - INFO - KG Training: Epoch 0001 Iter 0726 / 3136 | Time 0.1s | Iter Loss 0.1558 | Iter Mean Loss 0.3261\n",
      "2024-11-14 04:40:56,299 - root - INFO - KG Training: Epoch 0001 Iter 0727 / 3136 | Time 0.1s | Iter Loss 0.1477 | Iter Mean Loss 0.3258\n",
      "2024-11-14 04:40:56,363 - root - INFO - KG Training: Epoch 0001 Iter 0728 / 3136 | Time 0.1s | Iter Loss 0.1471 | Iter Mean Loss 0.3256\n",
      "2024-11-14 04:40:56,429 - root - INFO - KG Training: Epoch 0001 Iter 0729 / 3136 | Time 0.1s | Iter Loss 0.1556 | Iter Mean Loss 0.3254\n",
      "2024-11-14 04:40:56,486 - root - INFO - KG Training: Epoch 0001 Iter 0730 / 3136 | Time 0.1s | Iter Loss 0.1463 | Iter Mean Loss 0.3251\n",
      "2024-11-14 04:40:56,542 - root - INFO - KG Training: Epoch 0001 Iter 0731 / 3136 | Time 0.1s | Iter Loss 0.1588 | Iter Mean Loss 0.3249\n",
      "2024-11-14 04:40:56,602 - root - INFO - KG Training: Epoch 0001 Iter 0732 / 3136 | Time 0.1s | Iter Loss 0.1556 | Iter Mean Loss 0.3247\n",
      "2024-11-14 04:40:56,663 - root - INFO - KG Training: Epoch 0001 Iter 0733 / 3136 | Time 0.1s | Iter Loss 0.1492 | Iter Mean Loss 0.3244\n",
      "2024-11-14 04:40:56,723 - root - INFO - KG Training: Epoch 0001 Iter 0734 / 3136 | Time 0.1s | Iter Loss 0.1642 | Iter Mean Loss 0.3242\n",
      "2024-11-14 04:40:56,785 - root - INFO - KG Training: Epoch 0001 Iter 0735 / 3136 | Time 0.1s | Iter Loss 0.1574 | Iter Mean Loss 0.3240\n",
      "2024-11-14 04:40:56,922 - root - INFO - KG Training: Epoch 0001 Iter 0736 / 3136 | Time 0.1s | Iter Loss 0.1535 | Iter Mean Loss 0.3237\n",
      "2024-11-14 04:40:57,012 - root - INFO - KG Training: Epoch 0001 Iter 0737 / 3136 | Time 0.1s | Iter Loss 0.1564 | Iter Mean Loss 0.3235\n",
      "2024-11-14 04:40:57,073 - root - INFO - KG Training: Epoch 0001 Iter 0738 / 3136 | Time 0.1s | Iter Loss 0.1511 | Iter Mean Loss 0.3233\n",
      "2024-11-14 04:40:57,131 - root - INFO - KG Training: Epoch 0001 Iter 0739 / 3136 | Time 0.1s | Iter Loss 0.1623 | Iter Mean Loss 0.3231\n",
      "2024-11-14 04:40:57,191 - root - INFO - KG Training: Epoch 0001 Iter 0740 / 3136 | Time 0.1s | Iter Loss 0.1461 | Iter Mean Loss 0.3228\n",
      "2024-11-14 04:40:57,250 - root - INFO - KG Training: Epoch 0001 Iter 0741 / 3136 | Time 0.1s | Iter Loss 0.1569 | Iter Mean Loss 0.3226\n",
      "2024-11-14 04:40:57,311 - root - INFO - KG Training: Epoch 0001 Iter 0742 / 3136 | Time 0.1s | Iter Loss 0.1518 | Iter Mean Loss 0.3224\n",
      "2024-11-14 04:40:57,370 - root - INFO - KG Training: Epoch 0001 Iter 0743 / 3136 | Time 0.1s | Iter Loss 0.1635 | Iter Mean Loss 0.3222\n",
      "2024-11-14 04:40:57,433 - root - INFO - KG Training: Epoch 0001 Iter 0744 / 3136 | Time 0.1s | Iter Loss 0.1538 | Iter Mean Loss 0.3219\n",
      "2024-11-14 04:40:57,493 - root - INFO - KG Training: Epoch 0001 Iter 0745 / 3136 | Time 0.1s | Iter Loss 0.1546 | Iter Mean Loss 0.3217\n",
      "2024-11-14 04:40:57,611 - root - INFO - KG Training: Epoch 0001 Iter 0746 / 3136 | Time 0.1s | Iter Loss 0.1551 | Iter Mean Loss 0.3215\n",
      "2024-11-14 04:40:57,669 - root - INFO - KG Training: Epoch 0001 Iter 0747 / 3136 | Time 0.1s | Iter Loss 0.1658 | Iter Mean Loss 0.3213\n",
      "2024-11-14 04:40:57,728 - root - INFO - KG Training: Epoch 0001 Iter 0748 / 3136 | Time 0.1s | Iter Loss 0.1582 | Iter Mean Loss 0.3211\n",
      "2024-11-14 04:40:57,783 - root - INFO - KG Training: Epoch 0001 Iter 0749 / 3136 | Time 0.1s | Iter Loss 0.1429 | Iter Mean Loss 0.3208\n",
      "2024-11-14 04:40:57,846 - root - INFO - KG Training: Epoch 0001 Iter 0750 / 3136 | Time 0.1s | Iter Loss 0.1492 | Iter Mean Loss 0.3206\n",
      "2024-11-14 04:40:57,906 - root - INFO - KG Training: Epoch 0001 Iter 0751 / 3136 | Time 0.1s | Iter Loss 0.1508 | Iter Mean Loss 0.3204\n",
      "2024-11-14 04:40:57,970 - root - INFO - KG Training: Epoch 0001 Iter 0752 / 3136 | Time 0.1s | Iter Loss 0.1592 | Iter Mean Loss 0.3202\n",
      "2024-11-14 04:40:58,030 - root - INFO - KG Training: Epoch 0001 Iter 0753 / 3136 | Time 0.1s | Iter Loss 0.1533 | Iter Mean Loss 0.3199\n",
      "2024-11-14 04:40:58,091 - root - INFO - KG Training: Epoch 0001 Iter 0754 / 3136 | Time 0.1s | Iter Loss 0.1476 | Iter Mean Loss 0.3197\n",
      "2024-11-14 04:40:58,151 - root - INFO - KG Training: Epoch 0001 Iter 0755 / 3136 | Time 0.1s | Iter Loss 0.1463 | Iter Mean Loss 0.3195\n",
      "2024-11-14 04:40:58,215 - root - INFO - KG Training: Epoch 0001 Iter 0756 / 3136 | Time 0.1s | Iter Loss 0.1595 | Iter Mean Loss 0.3193\n",
      "2024-11-14 04:40:58,274 - root - INFO - KG Training: Epoch 0001 Iter 0757 / 3136 | Time 0.1s | Iter Loss 0.1528 | Iter Mean Loss 0.3190\n",
      "2024-11-14 04:40:58,334 - root - INFO - KG Training: Epoch 0001 Iter 0758 / 3136 | Time 0.1s | Iter Loss 0.1571 | Iter Mean Loss 0.3188\n",
      "2024-11-14 04:40:58,394 - root - INFO - KG Training: Epoch 0001 Iter 0759 / 3136 | Time 0.1s | Iter Loss 0.1523 | Iter Mean Loss 0.3186\n",
      "2024-11-14 04:40:58,453 - root - INFO - KG Training: Epoch 0001 Iter 0760 / 3136 | Time 0.1s | Iter Loss 0.1467 | Iter Mean Loss 0.3184\n",
      "2024-11-14 04:40:58,510 - root - INFO - KG Training: Epoch 0001 Iter 0761 / 3136 | Time 0.1s | Iter Loss 0.1431 | Iter Mean Loss 0.3182\n",
      "2024-11-14 04:40:58,570 - root - INFO - KG Training: Epoch 0001 Iter 0762 / 3136 | Time 0.1s | Iter Loss 0.1466 | Iter Mean Loss 0.3179\n",
      "2024-11-14 04:40:58,631 - root - INFO - KG Training: Epoch 0001 Iter 0763 / 3136 | Time 0.1s | Iter Loss 0.1586 | Iter Mean Loss 0.3177\n",
      "2024-11-14 04:40:58,692 - root - INFO - KG Training: Epoch 0001 Iter 0764 / 3136 | Time 0.1s | Iter Loss 0.1479 | Iter Mean Loss 0.3175\n",
      "2024-11-14 04:40:58,752 - root - INFO - KG Training: Epoch 0001 Iter 0765 / 3136 | Time 0.1s | Iter Loss 0.1556 | Iter Mean Loss 0.3173\n",
      "2024-11-14 04:40:58,810 - root - INFO - KG Training: Epoch 0001 Iter 0766 / 3136 | Time 0.1s | Iter Loss 0.1473 | Iter Mean Loss 0.3171\n",
      "2024-11-14 04:40:58,873 - root - INFO - KG Training: Epoch 0001 Iter 0767 / 3136 | Time 0.1s | Iter Loss 0.1547 | Iter Mean Loss 0.3168\n",
      "2024-11-14 04:40:58,933 - root - INFO - KG Training: Epoch 0001 Iter 0768 / 3136 | Time 0.1s | Iter Loss 0.1478 | Iter Mean Loss 0.3166\n",
      "2024-11-14 04:40:58,992 - root - INFO - KG Training: Epoch 0001 Iter 0769 / 3136 | Time 0.1s | Iter Loss 0.1465 | Iter Mean Loss 0.3164\n",
      "2024-11-14 04:40:59,057 - root - INFO - KG Training: Epoch 0001 Iter 0770 / 3136 | Time 0.1s | Iter Loss 0.1452 | Iter Mean Loss 0.3162\n",
      "2024-11-14 04:40:59,115 - root - INFO - KG Training: Epoch 0001 Iter 0771 / 3136 | Time 0.1s | Iter Loss 0.1579 | Iter Mean Loss 0.3160\n",
      "2024-11-14 04:40:59,176 - root - INFO - KG Training: Epoch 0001 Iter 0772 / 3136 | Time 0.1s | Iter Loss 0.1475 | Iter Mean Loss 0.3158\n",
      "2024-11-14 04:40:59,254 - root - INFO - KG Training: Epoch 0001 Iter 0773 / 3136 | Time 0.1s | Iter Loss 0.1473 | Iter Mean Loss 0.3155\n",
      "2024-11-14 04:40:59,314 - root - INFO - KG Training: Epoch 0001 Iter 0774 / 3136 | Time 0.1s | Iter Loss 0.1507 | Iter Mean Loss 0.3153\n",
      "2024-11-14 04:40:59,372 - root - INFO - KG Training: Epoch 0001 Iter 0775 / 3136 | Time 0.1s | Iter Loss 0.1520 | Iter Mean Loss 0.3151\n",
      "2024-11-14 04:40:59,432 - root - INFO - KG Training: Epoch 0001 Iter 0776 / 3136 | Time 0.1s | Iter Loss 0.1472 | Iter Mean Loss 0.3149\n",
      "2024-11-14 04:40:59,499 - root - INFO - KG Training: Epoch 0001 Iter 0777 / 3136 | Time 0.1s | Iter Loss 0.1502 | Iter Mean Loss 0.3147\n",
      "2024-11-14 04:40:59,559 - root - INFO - KG Training: Epoch 0001 Iter 0778 / 3136 | Time 0.1s | Iter Loss 0.1490 | Iter Mean Loss 0.3145\n",
      "2024-11-14 04:40:59,620 - root - INFO - KG Training: Epoch 0001 Iter 0779 / 3136 | Time 0.1s | Iter Loss 0.1524 | Iter Mean Loss 0.3143\n",
      "2024-11-14 04:40:59,681 - root - INFO - KG Training: Epoch 0001 Iter 0780 / 3136 | Time 0.1s | Iter Loss 0.1476 | Iter Mean Loss 0.3141\n",
      "2024-11-14 04:40:59,796 - root - INFO - KG Training: Epoch 0001 Iter 0781 / 3136 | Time 0.1s | Iter Loss 0.1343 | Iter Mean Loss 0.3138\n",
      "2024-11-14 04:40:59,858 - root - INFO - KG Training: Epoch 0001 Iter 0782 / 3136 | Time 0.1s | Iter Loss 0.1477 | Iter Mean Loss 0.3136\n",
      "2024-11-14 04:40:59,917 - root - INFO - KG Training: Epoch 0001 Iter 0783 / 3136 | Time 0.1s | Iter Loss 0.1568 | Iter Mean Loss 0.3134\n",
      "2024-11-14 04:40:59,980 - root - INFO - KG Training: Epoch 0001 Iter 0784 / 3136 | Time 0.1s | Iter Loss 0.1489 | Iter Mean Loss 0.3132\n",
      "2024-11-14 04:41:00,042 - root - INFO - KG Training: Epoch 0001 Iter 0785 / 3136 | Time 0.1s | Iter Loss 0.1508 | Iter Mean Loss 0.3130\n",
      "2024-11-14 04:41:00,102 - root - INFO - KG Training: Epoch 0001 Iter 0786 / 3136 | Time 0.1s | Iter Loss 0.1461 | Iter Mean Loss 0.3128\n",
      "2024-11-14 04:41:00,164 - root - INFO - KG Training: Epoch 0001 Iter 0787 / 3136 | Time 0.1s | Iter Loss 0.1456 | Iter Mean Loss 0.3126\n",
      "2024-11-14 04:41:00,233 - root - INFO - KG Training: Epoch 0001 Iter 0788 / 3136 | Time 0.1s | Iter Loss 0.1500 | Iter Mean Loss 0.3124\n",
      "2024-11-14 04:41:00,296 - root - INFO - KG Training: Epoch 0001 Iter 0789 / 3136 | Time 0.1s | Iter Loss 0.1519 | Iter Mean Loss 0.3122\n",
      "2024-11-14 04:41:00,359 - root - INFO - KG Training: Epoch 0001 Iter 0790 / 3136 | Time 0.1s | Iter Loss 0.1375 | Iter Mean Loss 0.3119\n",
      "2024-11-14 04:41:00,420 - root - INFO - KG Training: Epoch 0001 Iter 0791 / 3136 | Time 0.1s | Iter Loss 0.1370 | Iter Mean Loss 0.3117\n",
      "2024-11-14 04:41:00,481 - root - INFO - KG Training: Epoch 0001 Iter 0792 / 3136 | Time 0.1s | Iter Loss 0.1497 | Iter Mean Loss 0.3115\n",
      "2024-11-14 04:41:00,548 - root - INFO - KG Training: Epoch 0001 Iter 0793 / 3136 | Time 0.1s | Iter Loss 0.1524 | Iter Mean Loss 0.3113\n",
      "2024-11-14 04:41:00,613 - root - INFO - KG Training: Epoch 0001 Iter 0794 / 3136 | Time 0.1s | Iter Loss 0.1453 | Iter Mean Loss 0.3111\n",
      "2024-11-14 04:41:00,675 - root - INFO - KG Training: Epoch 0001 Iter 0795 / 3136 | Time 0.1s | Iter Loss 0.1413 | Iter Mean Loss 0.3109\n",
      "2024-11-14 04:41:00,736 - root - INFO - KG Training: Epoch 0001 Iter 0796 / 3136 | Time 0.1s | Iter Loss 0.1460 | Iter Mean Loss 0.3107\n",
      "2024-11-14 04:41:00,801 - root - INFO - KG Training: Epoch 0001 Iter 0797 / 3136 | Time 0.1s | Iter Loss 0.1621 | Iter Mean Loss 0.3105\n",
      "2024-11-14 04:41:00,864 - root - INFO - KG Training: Epoch 0001 Iter 0798 / 3136 | Time 0.1s | Iter Loss 0.1448 | Iter Mean Loss 0.3103\n",
      "2024-11-14 04:41:00,933 - root - INFO - KG Training: Epoch 0001 Iter 0799 / 3136 | Time 0.1s | Iter Loss 0.1451 | Iter Mean Loss 0.3101\n",
      "2024-11-14 04:41:00,998 - root - INFO - KG Training: Epoch 0001 Iter 0800 / 3136 | Time 0.1s | Iter Loss 0.1518 | Iter Mean Loss 0.3099\n",
      "2024-11-14 04:41:01,060 - root - INFO - KG Training: Epoch 0001 Iter 0801 / 3136 | Time 0.1s | Iter Loss 0.1421 | Iter Mean Loss 0.3097\n",
      "2024-11-14 04:41:01,122 - root - INFO - KG Training: Epoch 0001 Iter 0802 / 3136 | Time 0.1s | Iter Loss 0.1404 | Iter Mean Loss 0.3095\n",
      "2024-11-14 04:41:01,184 - root - INFO - KG Training: Epoch 0001 Iter 0803 / 3136 | Time 0.1s | Iter Loss 0.1240 | Iter Mean Loss 0.3092\n",
      "2024-11-14 04:41:01,247 - root - INFO - KG Training: Epoch 0001 Iter 0804 / 3136 | Time 0.1s | Iter Loss 0.1568 | Iter Mean Loss 0.3090\n",
      "2024-11-14 04:41:01,308 - root - INFO - KG Training: Epoch 0001 Iter 0805 / 3136 | Time 0.1s | Iter Loss 0.1349 | Iter Mean Loss 0.3088\n",
      "2024-11-14 04:41:01,369 - root - INFO - KG Training: Epoch 0001 Iter 0806 / 3136 | Time 0.1s | Iter Loss 0.1437 | Iter Mean Loss 0.3086\n",
      "2024-11-14 04:41:01,430 - root - INFO - KG Training: Epoch 0001 Iter 0807 / 3136 | Time 0.1s | Iter Loss 0.1440 | Iter Mean Loss 0.3084\n",
      "2024-11-14 04:41:01,486 - root - INFO - KG Training: Epoch 0001 Iter 0808 / 3136 | Time 0.1s | Iter Loss 0.1434 | Iter Mean Loss 0.3082\n",
      "2024-11-14 04:41:01,545 - root - INFO - KG Training: Epoch 0001 Iter 0809 / 3136 | Time 0.1s | Iter Loss 0.1466 | Iter Mean Loss 0.3080\n",
      "2024-11-14 04:41:01,609 - root - INFO - KG Training: Epoch 0001 Iter 0810 / 3136 | Time 0.1s | Iter Loss 0.1418 | Iter Mean Loss 0.3078\n",
      "2024-11-14 04:41:01,675 - root - INFO - KG Training: Epoch 0001 Iter 0811 / 3136 | Time 0.1s | Iter Loss 0.1415 | Iter Mean Loss 0.3076\n",
      "2024-11-14 04:41:01,735 - root - INFO - KG Training: Epoch 0001 Iter 0812 / 3136 | Time 0.1s | Iter Loss 0.1485 | Iter Mean Loss 0.3074\n",
      "2024-11-14 04:41:01,794 - root - INFO - KG Training: Epoch 0001 Iter 0813 / 3136 | Time 0.1s | Iter Loss 0.1444 | Iter Mean Loss 0.3072\n",
      "2024-11-14 04:41:01,853 - root - INFO - KG Training: Epoch 0001 Iter 0814 / 3136 | Time 0.1s | Iter Loss 0.1457 | Iter Mean Loss 0.3070\n",
      "2024-11-14 04:41:01,912 - root - INFO - KG Training: Epoch 0001 Iter 0815 / 3136 | Time 0.1s | Iter Loss 0.1421 | Iter Mean Loss 0.3068\n",
      "2024-11-14 04:41:01,970 - root - INFO - KG Training: Epoch 0001 Iter 0816 / 3136 | Time 0.1s | Iter Loss 0.1368 | Iter Mean Loss 0.3066\n",
      "2024-11-14 04:41:02,032 - root - INFO - KG Training: Epoch 0001 Iter 0817 / 3136 | Time 0.1s | Iter Loss 0.1417 | Iter Mean Loss 0.3064\n",
      "2024-11-14 04:41:02,091 - root - INFO - KG Training: Epoch 0001 Iter 0818 / 3136 | Time 0.1s | Iter Loss 0.1490 | Iter Mean Loss 0.3062\n",
      "2024-11-14 04:41:02,151 - root - INFO - KG Training: Epoch 0001 Iter 0819 / 3136 | Time 0.1s | Iter Loss 0.1408 | Iter Mean Loss 0.3060\n",
      "2024-11-14 04:41:02,210 - root - INFO - KG Training: Epoch 0001 Iter 0820 / 3136 | Time 0.1s | Iter Loss 0.1465 | Iter Mean Loss 0.3058\n",
      "2024-11-14 04:41:02,268 - root - INFO - KG Training: Epoch 0001 Iter 0821 / 3136 | Time 0.1s | Iter Loss 0.1479 | Iter Mean Loss 0.3056\n",
      "2024-11-14 04:41:02,330 - root - INFO - KG Training: Epoch 0001 Iter 0822 / 3136 | Time 0.1s | Iter Loss 0.1472 | Iter Mean Loss 0.3054\n",
      "2024-11-14 04:41:02,392 - root - INFO - KG Training: Epoch 0001 Iter 0823 / 3136 | Time 0.1s | Iter Loss 0.1347 | Iter Mean Loss 0.3052\n",
      "2024-11-14 04:41:02,451 - root - INFO - KG Training: Epoch 0001 Iter 0824 / 3136 | Time 0.1s | Iter Loss 0.1479 | Iter Mean Loss 0.3050\n",
      "2024-11-14 04:41:02,510 - root - INFO - KG Training: Epoch 0001 Iter 0825 / 3136 | Time 0.1s | Iter Loss 0.1490 | Iter Mean Loss 0.3048\n",
      "2024-11-14 04:41:02,568 - root - INFO - KG Training: Epoch 0001 Iter 0826 / 3136 | Time 0.1s | Iter Loss 0.1416 | Iter Mean Loss 0.3046\n",
      "2024-11-14 04:41:02,627 - root - INFO - KG Training: Epoch 0001 Iter 0827 / 3136 | Time 0.1s | Iter Loss 0.1381 | Iter Mean Loss 0.3044\n",
      "2024-11-14 04:41:02,800 - root - INFO - KG Training: Epoch 0001 Iter 0828 / 3136 | Time 0.2s | Iter Loss 0.1479 | Iter Mean Loss 0.3042\n",
      "2024-11-14 04:41:02,859 - root - INFO - KG Training: Epoch 0001 Iter 0829 / 3136 | Time 0.1s | Iter Loss 0.1490 | Iter Mean Loss 0.3041\n",
      "2024-11-14 04:41:02,915 - root - INFO - KG Training: Epoch 0001 Iter 0830 / 3136 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.3039\n",
      "2024-11-14 04:41:02,974 - root - INFO - KG Training: Epoch 0001 Iter 0831 / 3136 | Time 0.1s | Iter Loss 0.1444 | Iter Mean Loss 0.3037\n",
      "2024-11-14 04:41:03,034 - root - INFO - KG Training: Epoch 0001 Iter 0832 / 3136 | Time 0.1s | Iter Loss 0.1511 | Iter Mean Loss 0.3035\n",
      "2024-11-14 04:41:03,094 - root - INFO - KG Training: Epoch 0001 Iter 0833 / 3136 | Time 0.1s | Iter Loss 0.1557 | Iter Mean Loss 0.3033\n",
      "2024-11-14 04:41:03,155 - root - INFO - KG Training: Epoch 0001 Iter 0834 / 3136 | Time 0.1s | Iter Loss 0.1329 | Iter Mean Loss 0.3031\n",
      "2024-11-14 04:41:03,215 - root - INFO - KG Training: Epoch 0001 Iter 0835 / 3136 | Time 0.1s | Iter Loss 0.1353 | Iter Mean Loss 0.3029\n",
      "2024-11-14 04:41:03,274 - root - INFO - KG Training: Epoch 0001 Iter 0836 / 3136 | Time 0.1s | Iter Loss 0.1351 | Iter Mean Loss 0.3027\n",
      "2024-11-14 04:41:03,335 - root - INFO - KG Training: Epoch 0001 Iter 0837 / 3136 | Time 0.1s | Iter Loss 0.1415 | Iter Mean Loss 0.3025\n",
      "2024-11-14 04:41:03,443 - root - INFO - KG Training: Epoch 0001 Iter 0838 / 3136 | Time 0.1s | Iter Loss 0.1412 | Iter Mean Loss 0.3023\n",
      "2024-11-14 04:41:03,500 - root - INFO - KG Training: Epoch 0001 Iter 0839 / 3136 | Time 0.1s | Iter Loss 0.1393 | Iter Mean Loss 0.3021\n",
      "2024-11-14 04:41:03,562 - root - INFO - KG Training: Epoch 0001 Iter 0840 / 3136 | Time 0.1s | Iter Loss 0.1385 | Iter Mean Loss 0.3019\n",
      "2024-11-14 04:41:03,624 - root - INFO - KG Training: Epoch 0001 Iter 0841 / 3136 | Time 0.1s | Iter Loss 0.1389 | Iter Mean Loss 0.3017\n",
      "2024-11-14 04:41:03,683 - root - INFO - KG Training: Epoch 0001 Iter 0842 / 3136 | Time 0.1s | Iter Loss 0.1408 | Iter Mean Loss 0.3015\n",
      "2024-11-14 04:41:03,739 - root - INFO - KG Training: Epoch 0001 Iter 0843 / 3136 | Time 0.1s | Iter Loss 0.1507 | Iter Mean Loss 0.3014\n",
      "2024-11-14 04:41:03,797 - root - INFO - KG Training: Epoch 0001 Iter 0844 / 3136 | Time 0.1s | Iter Loss 0.1407 | Iter Mean Loss 0.3012\n",
      "2024-11-14 04:41:04,060 - root - INFO - KG Training: Epoch 0001 Iter 0845 / 3136 | Time 0.3s | Iter Loss 0.1346 | Iter Mean Loss 0.3010\n",
      "2024-11-14 04:41:04,120 - root - INFO - KG Training: Epoch 0001 Iter 0846 / 3136 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.3008\n",
      "2024-11-14 04:41:04,180 - root - INFO - KG Training: Epoch 0001 Iter 0847 / 3136 | Time 0.1s | Iter Loss 0.1332 | Iter Mean Loss 0.3006\n",
      "2024-11-14 04:41:04,238 - root - INFO - KG Training: Epoch 0001 Iter 0848 / 3136 | Time 0.1s | Iter Loss 0.1291 | Iter Mean Loss 0.3004\n",
      "2024-11-14 04:41:04,297 - root - INFO - KG Training: Epoch 0001 Iter 0849 / 3136 | Time 0.1s | Iter Loss 0.1246 | Iter Mean Loss 0.3002\n",
      "2024-11-14 04:41:04,358 - root - INFO - KG Training: Epoch 0001 Iter 0850 / 3136 | Time 0.1s | Iter Loss 0.1356 | Iter Mean Loss 0.3000\n",
      "2024-11-14 04:41:04,425 - root - INFO - KG Training: Epoch 0001 Iter 0851 / 3136 | Time 0.1s | Iter Loss 0.1449 | Iter Mean Loss 0.2998\n",
      "2024-11-14 04:41:04,487 - root - INFO - KG Training: Epoch 0001 Iter 0852 / 3136 | Time 0.1s | Iter Loss 0.1427 | Iter Mean Loss 0.2996\n",
      "2024-11-14 04:41:04,545 - root - INFO - KG Training: Epoch 0001 Iter 0853 / 3136 | Time 0.1s | Iter Loss 0.1364 | Iter Mean Loss 0.2994\n",
      "2024-11-14 04:41:04,602 - root - INFO - KG Training: Epoch 0001 Iter 0854 / 3136 | Time 0.1s | Iter Loss 0.1444 | Iter Mean Loss 0.2992\n",
      "2024-11-14 04:41:04,662 - root - INFO - KG Training: Epoch 0001 Iter 0855 / 3136 | Time 0.1s | Iter Loss 0.1377 | Iter Mean Loss 0.2990\n",
      "2024-11-14 04:41:04,724 - root - INFO - KG Training: Epoch 0001 Iter 0856 / 3136 | Time 0.1s | Iter Loss 0.1417 | Iter Mean Loss 0.2989\n",
      "2024-11-14 04:41:04,783 - root - INFO - KG Training: Epoch 0001 Iter 0857 / 3136 | Time 0.1s | Iter Loss 0.1503 | Iter Mean Loss 0.2987\n",
      "2024-11-14 04:41:04,843 - root - INFO - KG Training: Epoch 0001 Iter 0858 / 3136 | Time 0.1s | Iter Loss 0.1350 | Iter Mean Loss 0.2985\n",
      "2024-11-14 04:41:04,908 - root - INFO - KG Training: Epoch 0001 Iter 0859 / 3136 | Time 0.1s | Iter Loss 0.1379 | Iter Mean Loss 0.2983\n",
      "2024-11-14 04:41:04,968 - root - INFO - KG Training: Epoch 0001 Iter 0860 / 3136 | Time 0.1s | Iter Loss 0.1353 | Iter Mean Loss 0.2981\n",
      "2024-11-14 04:41:05,030 - root - INFO - KG Training: Epoch 0001 Iter 0861 / 3136 | Time 0.1s | Iter Loss 0.1393 | Iter Mean Loss 0.2979\n",
      "2024-11-14 04:41:05,092 - root - INFO - KG Training: Epoch 0001 Iter 0862 / 3136 | Time 0.1s | Iter Loss 0.1489 | Iter Mean Loss 0.2978\n",
      "2024-11-14 04:41:05,159 - root - INFO - KG Training: Epoch 0001 Iter 0863 / 3136 | Time 0.1s | Iter Loss 0.1356 | Iter Mean Loss 0.2976\n",
      "2024-11-14 04:41:05,218 - root - INFO - KG Training: Epoch 0001 Iter 0864 / 3136 | Time 0.1s | Iter Loss 0.1486 | Iter Mean Loss 0.2974\n",
      "2024-11-14 04:41:05,278 - root - INFO - KG Training: Epoch 0001 Iter 0865 / 3136 | Time 0.1s | Iter Loss 0.1439 | Iter Mean Loss 0.2972\n",
      "2024-11-14 04:41:05,336 - root - INFO - KG Training: Epoch 0001 Iter 0866 / 3136 | Time 0.1s | Iter Loss 0.1191 | Iter Mean Loss 0.2970\n",
      "2024-11-14 04:41:05,395 - root - INFO - KG Training: Epoch 0001 Iter 0867 / 3136 | Time 0.1s | Iter Loss 0.1399 | Iter Mean Loss 0.2968\n",
      "2024-11-14 04:41:05,454 - root - INFO - KG Training: Epoch 0001 Iter 0868 / 3136 | Time 0.1s | Iter Loss 0.1325 | Iter Mean Loss 0.2967\n",
      "2024-11-14 04:41:05,533 - root - INFO - KG Training: Epoch 0001 Iter 0869 / 3136 | Time 0.1s | Iter Loss 0.1308 | Iter Mean Loss 0.2965\n",
      "2024-11-14 04:41:05,593 - root - INFO - KG Training: Epoch 0001 Iter 0870 / 3136 | Time 0.1s | Iter Loss 0.1326 | Iter Mean Loss 0.2963\n",
      "2024-11-14 04:41:05,652 - root - INFO - KG Training: Epoch 0001 Iter 0871 / 3136 | Time 0.1s | Iter Loss 0.1346 | Iter Mean Loss 0.2961\n",
      "2024-11-14 04:41:05,712 - root - INFO - KG Training: Epoch 0001 Iter 0872 / 3136 | Time 0.1s | Iter Loss 0.1434 | Iter Mean Loss 0.2959\n",
      "2024-11-14 04:41:05,834 - root - INFO - KG Training: Epoch 0001 Iter 0873 / 3136 | Time 0.1s | Iter Loss 0.1252 | Iter Mean Loss 0.2957\n",
      "2024-11-14 04:41:05,894 - root - INFO - KG Training: Epoch 0001 Iter 0874 / 3136 | Time 0.1s | Iter Loss 0.1301 | Iter Mean Loss 0.2955\n",
      "2024-11-14 04:41:05,953 - root - INFO - KG Training: Epoch 0001 Iter 0875 / 3136 | Time 0.1s | Iter Loss 0.1372 | Iter Mean Loss 0.2953\n",
      "2024-11-14 04:41:06,013 - root - INFO - KG Training: Epoch 0001 Iter 0876 / 3136 | Time 0.1s | Iter Loss 0.1412 | Iter Mean Loss 0.2952\n",
      "2024-11-14 04:41:06,071 - root - INFO - KG Training: Epoch 0001 Iter 0877 / 3136 | Time 0.1s | Iter Loss 0.1364 | Iter Mean Loss 0.2950\n",
      "2024-11-14 04:41:06,132 - root - INFO - KG Training: Epoch 0001 Iter 0878 / 3136 | Time 0.1s | Iter Loss 0.1313 | Iter Mean Loss 0.2948\n",
      "2024-11-14 04:41:06,193 - root - INFO - KG Training: Epoch 0001 Iter 0879 / 3136 | Time 0.1s | Iter Loss 0.1370 | Iter Mean Loss 0.2946\n",
      "2024-11-14 04:41:06,253 - root - INFO - KG Training: Epoch 0001 Iter 0880 / 3136 | Time 0.1s | Iter Loss 0.1434 | Iter Mean Loss 0.2945\n",
      "2024-11-14 04:41:06,312 - root - INFO - KG Training: Epoch 0001 Iter 0881 / 3136 | Time 0.1s | Iter Loss 0.1352 | Iter Mean Loss 0.2943\n",
      "2024-11-14 04:41:06,372 - root - INFO - KG Training: Epoch 0001 Iter 0882 / 3136 | Time 0.1s | Iter Loss 0.1304 | Iter Mean Loss 0.2941\n",
      "2024-11-14 04:41:06,430 - root - INFO - KG Training: Epoch 0001 Iter 0883 / 3136 | Time 0.1s | Iter Loss 0.1305 | Iter Mean Loss 0.2939\n",
      "2024-11-14 04:41:06,490 - root - INFO - KG Training: Epoch 0001 Iter 0884 / 3136 | Time 0.1s | Iter Loss 0.1497 | Iter Mean Loss 0.2937\n",
      "2024-11-14 04:41:06,716 - root - INFO - KG Training: Epoch 0001 Iter 0885 / 3136 | Time 0.2s | Iter Loss 0.1343 | Iter Mean Loss 0.2936\n",
      "2024-11-14 04:41:06,778 - root - INFO - KG Training: Epoch 0001 Iter 0886 / 3136 | Time 0.1s | Iter Loss 0.1326 | Iter Mean Loss 0.2934\n",
      "2024-11-14 04:41:06,836 - root - INFO - KG Training: Epoch 0001 Iter 0887 / 3136 | Time 0.1s | Iter Loss 0.1385 | Iter Mean Loss 0.2932\n",
      "2024-11-14 04:41:06,893 - root - INFO - KG Training: Epoch 0001 Iter 0888 / 3136 | Time 0.1s | Iter Loss 0.1346 | Iter Mean Loss 0.2930\n",
      "2024-11-14 04:41:06,952 - root - INFO - KG Training: Epoch 0001 Iter 0889 / 3136 | Time 0.1s | Iter Loss 0.1289 | Iter Mean Loss 0.2928\n",
      "2024-11-14 04:41:07,012 - root - INFO - KG Training: Epoch 0001 Iter 0890 / 3136 | Time 0.1s | Iter Loss 0.1365 | Iter Mean Loss 0.2927\n",
      "2024-11-14 04:41:07,275 - root - INFO - KG Training: Epoch 0001 Iter 0891 / 3136 | Time 0.3s | Iter Loss 0.1455 | Iter Mean Loss 0.2925\n",
      "2024-11-14 04:41:07,335 - root - INFO - KG Training: Epoch 0001 Iter 0892 / 3136 | Time 0.1s | Iter Loss 0.1411 | Iter Mean Loss 0.2923\n",
      "2024-11-14 04:41:07,393 - root - INFO - KG Training: Epoch 0001 Iter 0893 / 3136 | Time 0.1s | Iter Loss 0.1255 | Iter Mean Loss 0.2921\n",
      "2024-11-14 04:41:07,450 - root - INFO - KG Training: Epoch 0001 Iter 0894 / 3136 | Time 0.1s | Iter Loss 0.1395 | Iter Mean Loss 0.2920\n",
      "2024-11-14 04:41:07,510 - root - INFO - KG Training: Epoch 0001 Iter 0895 / 3136 | Time 0.1s | Iter Loss 0.1252 | Iter Mean Loss 0.2918\n",
      "2024-11-14 04:41:07,572 - root - INFO - KG Training: Epoch 0001 Iter 0896 / 3136 | Time 0.1s | Iter Loss 0.1297 | Iter Mean Loss 0.2916\n",
      "2024-11-14 04:41:07,633 - root - INFO - KG Training: Epoch 0001 Iter 0897 / 3136 | Time 0.1s | Iter Loss 0.1332 | Iter Mean Loss 0.2914\n",
      "2024-11-14 04:41:07,694 - root - INFO - KG Training: Epoch 0001 Iter 0898 / 3136 | Time 0.1s | Iter Loss 0.1321 | Iter Mean Loss 0.2912\n",
      "2024-11-14 04:41:07,754 - root - INFO - KG Training: Epoch 0001 Iter 0899 / 3136 | Time 0.1s | Iter Loss 0.1348 | Iter Mean Loss 0.2911\n",
      "2024-11-14 04:41:07,813 - root - INFO - KG Training: Epoch 0001 Iter 0900 / 3136 | Time 0.1s | Iter Loss 0.1312 | Iter Mean Loss 0.2909\n",
      "2024-11-14 04:41:07,873 - root - INFO - KG Training: Epoch 0001 Iter 0901 / 3136 | Time 0.1s | Iter Loss 0.1291 | Iter Mean Loss 0.2907\n",
      "2024-11-14 04:41:07,933 - root - INFO - KG Training: Epoch 0001 Iter 0902 / 3136 | Time 0.1s | Iter Loss 0.1264 | Iter Mean Loss 0.2905\n",
      "2024-11-14 04:41:07,993 - root - INFO - KG Training: Epoch 0001 Iter 0903 / 3136 | Time 0.1s | Iter Loss 0.1394 | Iter Mean Loss 0.2904\n",
      "2024-11-14 04:41:08,053 - root - INFO - KG Training: Epoch 0001 Iter 0904 / 3136 | Time 0.1s | Iter Loss 0.1358 | Iter Mean Loss 0.2902\n",
      "2024-11-14 04:41:08,112 - root - INFO - KG Training: Epoch 0001 Iter 0905 / 3136 | Time 0.1s | Iter Loss 0.1338 | Iter Mean Loss 0.2900\n",
      "2024-11-14 04:41:08,172 - root - INFO - KG Training: Epoch 0001 Iter 0906 / 3136 | Time 0.1s | Iter Loss 0.1267 | Iter Mean Loss 0.2898\n",
      "2024-11-14 04:41:08,234 - root - INFO - KG Training: Epoch 0001 Iter 0907 / 3136 | Time 0.1s | Iter Loss 0.1273 | Iter Mean Loss 0.2897\n",
      "2024-11-14 04:41:08,293 - root - INFO - KG Training: Epoch 0001 Iter 0908 / 3136 | Time 0.1s | Iter Loss 0.1280 | Iter Mean Loss 0.2895\n",
      "2024-11-14 04:41:08,351 - root - INFO - KG Training: Epoch 0001 Iter 0909 / 3136 | Time 0.1s | Iter Loss 0.1350 | Iter Mean Loss 0.2893\n",
      "2024-11-14 04:41:08,410 - root - INFO - KG Training: Epoch 0001 Iter 0910 / 3136 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.2891\n",
      "2024-11-14 04:41:08,470 - root - INFO - KG Training: Epoch 0001 Iter 0911 / 3136 | Time 0.1s | Iter Loss 0.1270 | Iter Mean Loss 0.2890\n",
      "2024-11-14 04:41:08,530 - root - INFO - KG Training: Epoch 0001 Iter 0912 / 3136 | Time 0.1s | Iter Loss 0.1214 | Iter Mean Loss 0.2888\n",
      "2024-11-14 04:41:08,590 - root - INFO - KG Training: Epoch 0001 Iter 0913 / 3136 | Time 0.1s | Iter Loss 0.1263 | Iter Mean Loss 0.2886\n",
      "2024-11-14 04:41:08,651 - root - INFO - KG Training: Epoch 0001 Iter 0914 / 3136 | Time 0.1s | Iter Loss 0.1281 | Iter Mean Loss 0.2884\n",
      "2024-11-14 04:41:08,711 - root - INFO - KG Training: Epoch 0001 Iter 0915 / 3136 | Time 0.1s | Iter Loss 0.1361 | Iter Mean Loss 0.2883\n",
      "2024-11-14 04:41:08,774 - root - INFO - KG Training: Epoch 0001 Iter 0916 / 3136 | Time 0.1s | Iter Loss 0.1296 | Iter Mean Loss 0.2881\n",
      "2024-11-14 04:41:08,831 - root - INFO - KG Training: Epoch 0001 Iter 0917 / 3136 | Time 0.1s | Iter Loss 0.1218 | Iter Mean Loss 0.2879\n",
      "2024-11-14 04:41:08,891 - root - INFO - KG Training: Epoch 0001 Iter 0918 / 3136 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.2877\n",
      "2024-11-14 04:41:08,949 - root - INFO - KG Training: Epoch 0001 Iter 0919 / 3136 | Time 0.1s | Iter Loss 0.1234 | Iter Mean Loss 0.2876\n",
      "2024-11-14 04:41:09,009 - root - INFO - KG Training: Epoch 0001 Iter 0920 / 3136 | Time 0.1s | Iter Loss 0.1298 | Iter Mean Loss 0.2874\n",
      "2024-11-14 04:41:09,074 - root - INFO - KG Training: Epoch 0001 Iter 0921 / 3136 | Time 0.1s | Iter Loss 0.1389 | Iter Mean Loss 0.2872\n",
      "2024-11-14 04:41:09,136 - root - INFO - KG Training: Epoch 0001 Iter 0922 / 3136 | Time 0.1s | Iter Loss 0.1286 | Iter Mean Loss 0.2871\n",
      "2024-11-14 04:41:09,196 - root - INFO - KG Training: Epoch 0001 Iter 0923 / 3136 | Time 0.1s | Iter Loss 0.1262 | Iter Mean Loss 0.2869\n",
      "2024-11-14 04:41:09,254 - root - INFO - KG Training: Epoch 0001 Iter 0924 / 3136 | Time 0.1s | Iter Loss 0.1308 | Iter Mean Loss 0.2867\n",
      "2024-11-14 04:41:09,312 - root - INFO - KG Training: Epoch 0001 Iter 0925 / 3136 | Time 0.1s | Iter Loss 0.1323 | Iter Mean Loss 0.2865\n",
      "2024-11-14 04:41:09,369 - root - INFO - KG Training: Epoch 0001 Iter 0926 / 3136 | Time 0.1s | Iter Loss 0.1314 | Iter Mean Loss 0.2864\n",
      "2024-11-14 04:41:09,426 - root - INFO - KG Training: Epoch 0001 Iter 0927 / 3136 | Time 0.1s | Iter Loss 0.1458 | Iter Mean Loss 0.2862\n",
      "2024-11-14 04:41:09,496 - root - INFO - KG Training: Epoch 0001 Iter 0928 / 3136 | Time 0.1s | Iter Loss 0.1256 | Iter Mean Loss 0.2861\n",
      "2024-11-14 04:41:09,553 - root - INFO - KG Training: Epoch 0001 Iter 0929 / 3136 | Time 0.1s | Iter Loss 0.1365 | Iter Mean Loss 0.2859\n",
      "2024-11-14 04:41:09,628 - root - INFO - KG Training: Epoch 0001 Iter 0930 / 3136 | Time 0.1s | Iter Loss 0.1168 | Iter Mean Loss 0.2857\n",
      "2024-11-14 04:41:09,687 - root - INFO - KG Training: Epoch 0001 Iter 0931 / 3136 | Time 0.1s | Iter Loss 0.1381 | Iter Mean Loss 0.2856\n",
      "2024-11-14 04:41:09,761 - root - INFO - KG Training: Epoch 0001 Iter 0932 / 3136 | Time 0.1s | Iter Loss 0.1255 | Iter Mean Loss 0.2854\n",
      "2024-11-14 04:41:09,821 - root - INFO - KG Training: Epoch 0001 Iter 0933 / 3136 | Time 0.1s | Iter Loss 0.1278 | Iter Mean Loss 0.2852\n",
      "2024-11-14 04:41:09,895 - root - INFO - KG Training: Epoch 0001 Iter 0934 / 3136 | Time 0.1s | Iter Loss 0.1289 | Iter Mean Loss 0.2850\n",
      "2024-11-14 04:41:09,969 - root - INFO - KG Training: Epoch 0001 Iter 0935 / 3136 | Time 0.1s | Iter Loss 0.1306 | Iter Mean Loss 0.2849\n",
      "2024-11-14 04:41:10,028 - root - INFO - KG Training: Epoch 0001 Iter 0936 / 3136 | Time 0.1s | Iter Loss 0.1282 | Iter Mean Loss 0.2847\n",
      "2024-11-14 04:41:10,086 - root - INFO - KG Training: Epoch 0001 Iter 0937 / 3136 | Time 0.1s | Iter Loss 0.1261 | Iter Mean Loss 0.2845\n",
      "2024-11-14 04:41:10,160 - root - INFO - KG Training: Epoch 0001 Iter 0938 / 3136 | Time 0.1s | Iter Loss 0.1255 | Iter Mean Loss 0.2844\n",
      "2024-11-14 04:41:10,234 - root - INFO - KG Training: Epoch 0001 Iter 0939 / 3136 | Time 0.1s | Iter Loss 0.1240 | Iter Mean Loss 0.2842\n",
      "2024-11-14 04:41:10,404 - root - INFO - KG Training: Epoch 0001 Iter 0940 / 3136 | Time 0.2s | Iter Loss 0.1308 | Iter Mean Loss 0.2840\n",
      "2024-11-14 04:41:10,643 - root - INFO - KG Training: Epoch 0001 Iter 0941 / 3136 | Time 0.2s | Iter Loss 0.1296 | Iter Mean Loss 0.2839\n",
      "2024-11-14 04:41:10,716 - root - INFO - KG Training: Epoch 0001 Iter 0942 / 3136 | Time 0.1s | Iter Loss 0.1224 | Iter Mean Loss 0.2837\n",
      "2024-11-14 04:41:10,775 - root - INFO - KG Training: Epoch 0001 Iter 0943 / 3136 | Time 0.1s | Iter Loss 0.1333 | Iter Mean Loss 0.2835\n",
      "2024-11-14 04:41:10,834 - root - INFO - KG Training: Epoch 0001 Iter 0944 / 3136 | Time 0.1s | Iter Loss 0.1230 | Iter Mean Loss 0.2834\n",
      "2024-11-14 04:41:10,894 - root - INFO - KG Training: Epoch 0001 Iter 0945 / 3136 | Time 0.1s | Iter Loss 0.1207 | Iter Mean Loss 0.2832\n",
      "2024-11-14 04:41:10,955 - root - INFO - KG Training: Epoch 0001 Iter 0946 / 3136 | Time 0.1s | Iter Loss 0.1304 | Iter Mean Loss 0.2830\n",
      "2024-11-14 04:41:11,014 - root - INFO - KG Training: Epoch 0001 Iter 0947 / 3136 | Time 0.1s | Iter Loss 0.1258 | Iter Mean Loss 0.2829\n",
      "2024-11-14 04:41:11,128 - root - INFO - KG Training: Epoch 0001 Iter 0948 / 3136 | Time 0.1s | Iter Loss 0.1300 | Iter Mean Loss 0.2827\n",
      "2024-11-14 04:41:11,186 - root - INFO - KG Training: Epoch 0001 Iter 0949 / 3136 | Time 0.1s | Iter Loss 0.1259 | Iter Mean Loss 0.2825\n",
      "2024-11-14 04:41:11,246 - root - INFO - KG Training: Epoch 0001 Iter 0950 / 3136 | Time 0.1s | Iter Loss 0.1340 | Iter Mean Loss 0.2824\n",
      "2024-11-14 04:41:11,305 - root - INFO - KG Training: Epoch 0001 Iter 0951 / 3136 | Time 0.1s | Iter Loss 0.1336 | Iter Mean Loss 0.2822\n",
      "2024-11-14 04:41:11,364 - root - INFO - KG Training: Epoch 0001 Iter 0952 / 3136 | Time 0.1s | Iter Loss 0.1309 | Iter Mean Loss 0.2821\n",
      "2024-11-14 04:41:11,424 - root - INFO - KG Training: Epoch 0001 Iter 0953 / 3136 | Time 0.1s | Iter Loss 0.1258 | Iter Mean Loss 0.2819\n",
      "2024-11-14 04:41:11,483 - root - INFO - KG Training: Epoch 0001 Iter 0954 / 3136 | Time 0.1s | Iter Loss 0.1245 | Iter Mean Loss 0.2817\n",
      "2024-11-14 04:41:11,544 - root - INFO - KG Training: Epoch 0001 Iter 0955 / 3136 | Time 0.1s | Iter Loss 0.1230 | Iter Mean Loss 0.2816\n",
      "2024-11-14 04:41:11,604 - root - INFO - KG Training: Epoch 0001 Iter 0956 / 3136 | Time 0.1s | Iter Loss 0.1366 | Iter Mean Loss 0.2814\n",
      "2024-11-14 04:41:11,662 - root - INFO - KG Training: Epoch 0001 Iter 0957 / 3136 | Time 0.1s | Iter Loss 0.1200 | Iter Mean Loss 0.2813\n",
      "2024-11-14 04:41:11,721 - root - INFO - KG Training: Epoch 0001 Iter 0958 / 3136 | Time 0.1s | Iter Loss 0.1333 | Iter Mean Loss 0.2811\n",
      "2024-11-14 04:41:11,780 - root - INFO - KG Training: Epoch 0001 Iter 0959 / 3136 | Time 0.1s | Iter Loss 0.1272 | Iter Mean Loss 0.2809\n",
      "2024-11-14 04:41:11,840 - root - INFO - KG Training: Epoch 0001 Iter 0960 / 3136 | Time 0.1s | Iter Loss 0.1383 | Iter Mean Loss 0.2808\n",
      "2024-11-14 04:41:11,897 - root - INFO - KG Training: Epoch 0001 Iter 0961 / 3136 | Time 0.1s | Iter Loss 0.1300 | Iter Mean Loss 0.2806\n",
      "2024-11-14 04:41:11,955 - root - INFO - KG Training: Epoch 0001 Iter 0962 / 3136 | Time 0.1s | Iter Loss 0.1272 | Iter Mean Loss 0.2805\n",
      "2024-11-14 04:41:12,012 - root - INFO - KG Training: Epoch 0001 Iter 0963 / 3136 | Time 0.1s | Iter Loss 0.1291 | Iter Mean Loss 0.2803\n",
      "2024-11-14 04:41:12,073 - root - INFO - KG Training: Epoch 0001 Iter 0964 / 3136 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.2802\n",
      "2024-11-14 04:41:12,133 - root - INFO - KG Training: Epoch 0001 Iter 0965 / 3136 | Time 0.1s | Iter Loss 0.1201 | Iter Mean Loss 0.2800\n",
      "2024-11-14 04:41:12,193 - root - INFO - KG Training: Epoch 0001 Iter 0966 / 3136 | Time 0.1s | Iter Loss 0.1336 | Iter Mean Loss 0.2799\n",
      "2024-11-14 04:41:12,251 - root - INFO - KG Training: Epoch 0001 Iter 0967 / 3136 | Time 0.1s | Iter Loss 0.1223 | Iter Mean Loss 0.2797\n",
      "2024-11-14 04:41:12,314 - root - INFO - KG Training: Epoch 0001 Iter 0968 / 3136 | Time 0.1s | Iter Loss 0.1289 | Iter Mean Loss 0.2795\n",
      "2024-11-14 04:41:12,372 - root - INFO - KG Training: Epoch 0001 Iter 0969 / 3136 | Time 0.1s | Iter Loss 0.1279 | Iter Mean Loss 0.2794\n",
      "2024-11-14 04:41:12,430 - root - INFO - KG Training: Epoch 0001 Iter 0970 / 3136 | Time 0.1s | Iter Loss 0.1219 | Iter Mean Loss 0.2792\n",
      "2024-11-14 04:41:12,488 - root - INFO - KG Training: Epoch 0001 Iter 0971 / 3136 | Time 0.1s | Iter Loss 0.1252 | Iter Mean Loss 0.2791\n",
      "2024-11-14 04:41:12,548 - root - INFO - KG Training: Epoch 0001 Iter 0972 / 3136 | Time 0.1s | Iter Loss 0.1316 | Iter Mean Loss 0.2789\n",
      "2024-11-14 04:41:12,608 - root - INFO - KG Training: Epoch 0001 Iter 0973 / 3136 | Time 0.1s | Iter Loss 0.1254 | Iter Mean Loss 0.2788\n",
      "2024-11-14 04:41:12,764 - root - INFO - KG Training: Epoch 0001 Iter 0974 / 3136 | Time 0.2s | Iter Loss 0.1305 | Iter Mean Loss 0.2786\n",
      "2024-11-14 04:41:12,823 - root - INFO - KG Training: Epoch 0001 Iter 0975 / 3136 | Time 0.1s | Iter Loss 0.1253 | Iter Mean Loss 0.2784\n",
      "2024-11-14 04:41:13,150 - root - INFO - KG Training: Epoch 0001 Iter 0976 / 3136 | Time 0.3s | Iter Loss 0.1142 | Iter Mean Loss 0.2783\n",
      "2024-11-14 04:41:13,208 - root - INFO - KG Training: Epoch 0001 Iter 0977 / 3136 | Time 0.1s | Iter Loss 0.1218 | Iter Mean Loss 0.2781\n",
      "2024-11-14 04:41:13,268 - root - INFO - KG Training: Epoch 0001 Iter 0978 / 3136 | Time 0.1s | Iter Loss 0.1218 | Iter Mean Loss 0.2780\n",
      "2024-11-14 04:41:13,327 - root - INFO - KG Training: Epoch 0001 Iter 0979 / 3136 | Time 0.1s | Iter Loss 0.1237 | Iter Mean Loss 0.2778\n",
      "2024-11-14 04:41:13,387 - root - INFO - KG Training: Epoch 0001 Iter 0980 / 3136 | Time 0.1s | Iter Loss 0.1285 | Iter Mean Loss 0.2776\n",
      "2024-11-14 04:41:13,447 - root - INFO - KG Training: Epoch 0001 Iter 0981 / 3136 | Time 0.1s | Iter Loss 0.1330 | Iter Mean Loss 0.2775\n",
      "2024-11-14 04:41:13,509 - root - INFO - KG Training: Epoch 0001 Iter 0982 / 3136 | Time 0.1s | Iter Loss 0.1294 | Iter Mean Loss 0.2773\n",
      "2024-11-14 04:41:13,570 - root - INFO - KG Training: Epoch 0001 Iter 0983 / 3136 | Time 0.1s | Iter Loss 0.1206 | Iter Mean Loss 0.2772\n",
      "2024-11-14 04:41:13,631 - root - INFO - KG Training: Epoch 0001 Iter 0984 / 3136 | Time 0.1s | Iter Loss 0.1113 | Iter Mean Loss 0.2770\n",
      "2024-11-14 04:41:13,692 - root - INFO - KG Training: Epoch 0001 Iter 0985 / 3136 | Time 0.1s | Iter Loss 0.1267 | Iter Mean Loss 0.2769\n",
      "2024-11-14 04:41:13,752 - root - INFO - KG Training: Epoch 0001 Iter 0986 / 3136 | Time 0.1s | Iter Loss 0.1132 | Iter Mean Loss 0.2767\n",
      "2024-11-14 04:41:13,809 - root - INFO - KG Training: Epoch 0001 Iter 0987 / 3136 | Time 0.1s | Iter Loss 0.1336 | Iter Mean Loss 0.2766\n",
      "2024-11-14 04:41:13,870 - root - INFO - KG Training: Epoch 0001 Iter 0988 / 3136 | Time 0.1s | Iter Loss 0.1205 | Iter Mean Loss 0.2764\n",
      "2024-11-14 04:41:14,119 - root - INFO - KG Training: Epoch 0001 Iter 0989 / 3136 | Time 0.2s | Iter Loss 0.1058 | Iter Mean Loss 0.2762\n",
      "2024-11-14 04:41:14,177 - root - INFO - KG Training: Epoch 0001 Iter 0990 / 3136 | Time 0.1s | Iter Loss 0.1265 | Iter Mean Loss 0.2761\n",
      "2024-11-14 04:41:14,236 - root - INFO - KG Training: Epoch 0001 Iter 0991 / 3136 | Time 0.1s | Iter Loss 0.1259 | Iter Mean Loss 0.2759\n",
      "2024-11-14 04:41:14,298 - root - INFO - KG Training: Epoch 0001 Iter 0992 / 3136 | Time 0.1s | Iter Loss 0.1239 | Iter Mean Loss 0.2758\n",
      "2024-11-14 04:41:14,357 - root - INFO - KG Training: Epoch 0001 Iter 0993 / 3136 | Time 0.1s | Iter Loss 0.1205 | Iter Mean Loss 0.2756\n",
      "2024-11-14 04:41:14,417 - root - INFO - KG Training: Epoch 0001 Iter 0994 / 3136 | Time 0.1s | Iter Loss 0.1235 | Iter Mean Loss 0.2755\n",
      "2024-11-14 04:41:14,479 - root - INFO - KG Training: Epoch 0001 Iter 0995 / 3136 | Time 0.1s | Iter Loss 0.1350 | Iter Mean Loss 0.2753\n",
      "2024-11-14 04:41:14,665 - root - INFO - KG Training: Epoch 0001 Iter 0996 / 3136 | Time 0.2s | Iter Loss 0.1271 | Iter Mean Loss 0.2752\n",
      "2024-11-14 04:41:14,724 - root - INFO - KG Training: Epoch 0001 Iter 0997 / 3136 | Time 0.1s | Iter Loss 0.1248 | Iter Mean Loss 0.2750\n",
      "2024-11-14 04:41:14,781 - root - INFO - KG Training: Epoch 0001 Iter 0998 / 3136 | Time 0.1s | Iter Loss 0.1283 | Iter Mean Loss 0.2749\n",
      "2024-11-14 04:41:14,842 - root - INFO - KG Training: Epoch 0001 Iter 0999 / 3136 | Time 0.1s | Iter Loss 0.1316 | Iter Mean Loss 0.2747\n",
      "2024-11-14 04:41:14,903 - root - INFO - KG Training: Epoch 0001 Iter 1000 / 3136 | Time 0.1s | Iter Loss 0.1370 | Iter Mean Loss 0.2746\n",
      "2024-11-14 04:41:15,097 - root - INFO - KG Training: Epoch 0001 Iter 1001 / 3136 | Time 0.2s | Iter Loss 0.1281 | Iter Mean Loss 0.2744\n",
      "2024-11-14 04:41:15,156 - root - INFO - KG Training: Epoch 0001 Iter 1002 / 3136 | Time 0.1s | Iter Loss 0.1184 | Iter Mean Loss 0.2743\n",
      "2024-11-14 04:41:15,214 - root - INFO - KG Training: Epoch 0001 Iter 1003 / 3136 | Time 0.1s | Iter Loss 0.1285 | Iter Mean Loss 0.2741\n",
      "2024-11-14 04:41:15,275 - root - INFO - KG Training: Epoch 0001 Iter 1004 / 3136 | Time 0.1s | Iter Loss 0.1138 | Iter Mean Loss 0.2740\n",
      "2024-11-14 04:41:15,335 - root - INFO - KG Training: Epoch 0001 Iter 1005 / 3136 | Time 0.1s | Iter Loss 0.1254 | Iter Mean Loss 0.2738\n",
      "2024-11-14 04:41:15,399 - root - INFO - KG Training: Epoch 0001 Iter 1006 / 3136 | Time 0.1s | Iter Loss 0.1234 | Iter Mean Loss 0.2737\n",
      "2024-11-14 04:41:15,459 - root - INFO - KG Training: Epoch 0001 Iter 1007 / 3136 | Time 0.1s | Iter Loss 0.1184 | Iter Mean Loss 0.2735\n",
      "2024-11-14 04:41:15,516 - root - INFO - KG Training: Epoch 0001 Iter 1008 / 3136 | Time 0.1s | Iter Loss 0.1109 | Iter Mean Loss 0.2734\n",
      "2024-11-14 04:41:15,576 - root - INFO - KG Training: Epoch 0001 Iter 1009 / 3136 | Time 0.1s | Iter Loss 0.1287 | Iter Mean Loss 0.2732\n",
      "2024-11-14 04:41:15,635 - root - INFO - KG Training: Epoch 0001 Iter 1010 / 3136 | Time 0.1s | Iter Loss 0.1167 | Iter Mean Loss 0.2731\n",
      "2024-11-14 04:41:15,695 - root - INFO - KG Training: Epoch 0001 Iter 1011 / 3136 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.2729\n",
      "2024-11-14 04:41:15,758 - root - INFO - KG Training: Epoch 0001 Iter 1012 / 3136 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.2728\n",
      "2024-11-14 04:41:15,817 - root - INFO - KG Training: Epoch 0001 Iter 1013 / 3136 | Time 0.1s | Iter Loss 0.1184 | Iter Mean Loss 0.2726\n",
      "2024-11-14 04:41:15,876 - root - INFO - KG Training: Epoch 0001 Iter 1014 / 3136 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.2725\n",
      "2024-11-14 04:41:15,935 - root - INFO - KG Training: Epoch 0001 Iter 1015 / 3136 | Time 0.1s | Iter Loss 0.1171 | Iter Mean Loss 0.2723\n",
      "2024-11-14 04:41:15,994 - root - INFO - KG Training: Epoch 0001 Iter 1016 / 3136 | Time 0.1s | Iter Loss 0.1162 | Iter Mean Loss 0.2722\n",
      "2024-11-14 04:41:16,053 - root - INFO - KG Training: Epoch 0001 Iter 1017 / 3136 | Time 0.1s | Iter Loss 0.1117 | Iter Mean Loss 0.2720\n",
      "2024-11-14 04:41:16,112 - root - INFO - KG Training: Epoch 0001 Iter 1018 / 3136 | Time 0.1s | Iter Loss 0.1239 | Iter Mean Loss 0.2719\n",
      "2024-11-14 04:41:16,172 - root - INFO - KG Training: Epoch 0001 Iter 1019 / 3136 | Time 0.1s | Iter Loss 0.1188 | Iter Mean Loss 0.2717\n",
      "2024-11-14 04:41:16,231 - root - INFO - KG Training: Epoch 0001 Iter 1020 / 3136 | Time 0.1s | Iter Loss 0.1079 | Iter Mean Loss 0.2715\n",
      "2024-11-14 04:41:16,292 - root - INFO - KG Training: Epoch 0001 Iter 1021 / 3136 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.2714\n",
      "2024-11-14 04:41:16,352 - root - INFO - KG Training: Epoch 0001 Iter 1022 / 3136 | Time 0.1s | Iter Loss 0.1212 | Iter Mean Loss 0.2713\n",
      "2024-11-14 04:41:16,413 - root - INFO - KG Training: Epoch 0001 Iter 1023 / 3136 | Time 0.1s | Iter Loss 0.1346 | Iter Mean Loss 0.2711\n",
      "2024-11-14 04:41:16,472 - root - INFO - KG Training: Epoch 0001 Iter 1024 / 3136 | Time 0.1s | Iter Loss 0.1299 | Iter Mean Loss 0.2710\n",
      "2024-11-14 04:41:16,531 - root - INFO - KG Training: Epoch 0001 Iter 1025 / 3136 | Time 0.1s | Iter Loss 0.1205 | Iter Mean Loss 0.2708\n",
      "2024-11-14 04:41:16,591 - root - INFO - KG Training: Epoch 0001 Iter 1026 / 3136 | Time 0.1s | Iter Loss 0.1178 | Iter Mean Loss 0.2707\n",
      "2024-11-14 04:41:16,649 - root - INFO - KG Training: Epoch 0001 Iter 1027 / 3136 | Time 0.1s | Iter Loss 0.1193 | Iter Mean Loss 0.2705\n",
      "2024-11-14 04:41:16,709 - root - INFO - KG Training: Epoch 0001 Iter 1028 / 3136 | Time 0.1s | Iter Loss 0.1261 | Iter Mean Loss 0.2704\n",
      "2024-11-14 04:41:16,766 - root - INFO - KG Training: Epoch 0001 Iter 1029 / 3136 | Time 0.1s | Iter Loss 0.1278 | Iter Mean Loss 0.2703\n",
      "2024-11-14 04:41:16,823 - root - INFO - KG Training: Epoch 0001 Iter 1030 / 3136 | Time 0.1s | Iter Loss 0.1172 | Iter Mean Loss 0.2701\n",
      "2024-11-14 04:41:16,885 - root - INFO - KG Training: Epoch 0001 Iter 1031 / 3136 | Time 0.1s | Iter Loss 0.1294 | Iter Mean Loss 0.2700\n",
      "2024-11-14 04:41:16,945 - root - INFO - KG Training: Epoch 0001 Iter 1032 / 3136 | Time 0.1s | Iter Loss 0.1093 | Iter Mean Loss 0.2698\n",
      "2024-11-14 04:41:17,006 - root - INFO - KG Training: Epoch 0001 Iter 1033 / 3136 | Time 0.1s | Iter Loss 0.1188 | Iter Mean Loss 0.2697\n",
      "2024-11-14 04:41:17,063 - root - INFO - KG Training: Epoch 0001 Iter 1034 / 3136 | Time 0.1s | Iter Loss 0.1153 | Iter Mean Loss 0.2695\n",
      "2024-11-14 04:41:17,120 - root - INFO - KG Training: Epoch 0001 Iter 1035 / 3136 | Time 0.1s | Iter Loss 0.1125 | Iter Mean Loss 0.2694\n",
      "2024-11-14 04:41:17,180 - root - INFO - KG Training: Epoch 0001 Iter 1036 / 3136 | Time 0.1s | Iter Loss 0.1241 | Iter Mean Loss 0.2692\n",
      "2024-11-14 04:41:17,241 - root - INFO - KG Training: Epoch 0001 Iter 1037 / 3136 | Time 0.1s | Iter Loss 0.1267 | Iter Mean Loss 0.2691\n",
      "2024-11-14 04:41:17,302 - root - INFO - KG Training: Epoch 0001 Iter 1038 / 3136 | Time 0.1s | Iter Loss 0.1237 | Iter Mean Loss 0.2690\n",
      "2024-11-14 04:41:17,425 - root - INFO - KG Training: Epoch 0001 Iter 1039 / 3136 | Time 0.1s | Iter Loss 0.1188 | Iter Mean Loss 0.2688\n",
      "2024-11-14 04:41:17,574 - root - INFO - KG Training: Epoch 0001 Iter 1040 / 3136 | Time 0.1s | Iter Loss 0.1081 | Iter Mean Loss 0.2687\n",
      "2024-11-14 04:41:17,635 - root - INFO - KG Training: Epoch 0001 Iter 1041 / 3136 | Time 0.1s | Iter Loss 0.1219 | Iter Mean Loss 0.2685\n",
      "2024-11-14 04:41:17,698 - root - INFO - KG Training: Epoch 0001 Iter 1042 / 3136 | Time 0.1s | Iter Loss 0.1214 | Iter Mean Loss 0.2684\n",
      "2024-11-14 04:41:17,757 - root - INFO - KG Training: Epoch 0001 Iter 1043 / 3136 | Time 0.1s | Iter Loss 0.1170 | Iter Mean Loss 0.2682\n",
      "2024-11-14 04:41:17,818 - root - INFO - KG Training: Epoch 0001 Iter 1044 / 3136 | Time 0.1s | Iter Loss 0.1175 | Iter Mean Loss 0.2681\n",
      "2024-11-14 04:41:17,947 - root - INFO - KG Training: Epoch 0001 Iter 1045 / 3136 | Time 0.1s | Iter Loss 0.1252 | Iter Mean Loss 0.2679\n",
      "2024-11-14 04:41:18,009 - root - INFO - KG Training: Epoch 0001 Iter 1046 / 3136 | Time 0.1s | Iter Loss 0.1340 | Iter Mean Loss 0.2678\n",
      "2024-11-14 04:41:18,068 - root - INFO - KG Training: Epoch 0001 Iter 1047 / 3136 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.2677\n",
      "2024-11-14 04:41:18,130 - root - INFO - KG Training: Epoch 0001 Iter 1048 / 3136 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.2675\n",
      "2024-11-14 04:41:18,191 - root - INFO - KG Training: Epoch 0001 Iter 1049 / 3136 | Time 0.1s | Iter Loss 0.1145 | Iter Mean Loss 0.2674\n",
      "2024-11-14 04:41:18,326 - root - INFO - KG Training: Epoch 0001 Iter 1050 / 3136 | Time 0.1s | Iter Loss 0.1116 | Iter Mean Loss 0.2672\n",
      "2024-11-14 04:41:18,386 - root - INFO - KG Training: Epoch 0001 Iter 1051 / 3136 | Time 0.1s | Iter Loss 0.1187 | Iter Mean Loss 0.2671\n",
      "2024-11-14 04:41:18,444 - root - INFO - KG Training: Epoch 0001 Iter 1052 / 3136 | Time 0.1s | Iter Loss 0.1139 | Iter Mean Loss 0.2669\n",
      "2024-11-14 04:41:18,503 - root - INFO - KG Training: Epoch 0001 Iter 1053 / 3136 | Time 0.1s | Iter Loss 0.1170 | Iter Mean Loss 0.2668\n",
      "2024-11-14 04:41:18,563 - root - INFO - KG Training: Epoch 0001 Iter 1054 / 3136 | Time 0.1s | Iter Loss 0.1155 | Iter Mean Loss 0.2667\n",
      "2024-11-14 04:41:18,622 - root - INFO - KG Training: Epoch 0001 Iter 1055 / 3136 | Time 0.1s | Iter Loss 0.1162 | Iter Mean Loss 0.2665\n",
      "2024-11-14 04:41:18,687 - root - INFO - KG Training: Epoch 0001 Iter 1056 / 3136 | Time 0.1s | Iter Loss 0.1108 | Iter Mean Loss 0.2664\n",
      "2024-11-14 04:41:18,746 - root - INFO - KG Training: Epoch 0001 Iter 1057 / 3136 | Time 0.1s | Iter Loss 0.1135 | Iter Mean Loss 0.2662\n",
      "2024-11-14 04:41:18,807 - root - INFO - KG Training: Epoch 0001 Iter 1058 / 3136 | Time 0.1s | Iter Loss 0.1233 | Iter Mean Loss 0.2661\n",
      "2024-11-14 04:41:18,874 - root - INFO - KG Training: Epoch 0001 Iter 1059 / 3136 | Time 0.1s | Iter Loss 0.1241 | Iter Mean Loss 0.2660\n",
      "2024-11-14 04:41:18,935 - root - INFO - KG Training: Epoch 0001 Iter 1060 / 3136 | Time 0.1s | Iter Loss 0.1149 | Iter Mean Loss 0.2658\n",
      "2024-11-14 04:41:19,086 - root - INFO - KG Training: Epoch 0001 Iter 1061 / 3136 | Time 0.2s | Iter Loss 0.1214 | Iter Mean Loss 0.2657\n",
      "2024-11-14 04:41:19,146 - root - INFO - KG Training: Epoch 0001 Iter 1062 / 3136 | Time 0.1s | Iter Loss 0.1229 | Iter Mean Loss 0.2655\n",
      "2024-11-14 04:41:19,213 - root - INFO - KG Training: Epoch 0001 Iter 1063 / 3136 | Time 0.1s | Iter Loss 0.1129 | Iter Mean Loss 0.2654\n",
      "2024-11-14 04:41:19,275 - root - INFO - KG Training: Epoch 0001 Iter 1064 / 3136 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.2653\n",
      "2024-11-14 04:41:19,337 - root - INFO - KG Training: Epoch 0001 Iter 1065 / 3136 | Time 0.1s | Iter Loss 0.1177 | Iter Mean Loss 0.2651\n",
      "2024-11-14 04:41:19,401 - root - INFO - KG Training: Epoch 0001 Iter 1066 / 3136 | Time 0.1s | Iter Loss 0.1226 | Iter Mean Loss 0.2650\n",
      "2024-11-14 04:41:19,462 - root - INFO - KG Training: Epoch 0001 Iter 1067 / 3136 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.2649\n",
      "2024-11-14 04:41:19,522 - root - INFO - KG Training: Epoch 0001 Iter 1068 / 3136 | Time 0.1s | Iter Loss 0.1172 | Iter Mean Loss 0.2647\n",
      "2024-11-14 04:41:19,596 - root - INFO - KG Training: Epoch 0001 Iter 1069 / 3136 | Time 0.1s | Iter Loss 0.1152 | Iter Mean Loss 0.2646\n",
      "2024-11-14 04:41:19,735 - root - INFO - KG Training: Epoch 0001 Iter 1070 / 3136 | Time 0.1s | Iter Loss 0.1116 | Iter Mean Loss 0.2644\n",
      "2024-11-14 04:41:19,807 - root - INFO - KG Training: Epoch 0001 Iter 1071 / 3136 | Time 0.1s | Iter Loss 0.1264 | Iter Mean Loss 0.2643\n",
      "2024-11-14 04:41:19,866 - root - INFO - KG Training: Epoch 0001 Iter 1072 / 3136 | Time 0.1s | Iter Loss 0.1173 | Iter Mean Loss 0.2642\n",
      "2024-11-14 04:41:19,927 - root - INFO - KG Training: Epoch 0001 Iter 1073 / 3136 | Time 0.1s | Iter Loss 0.1220 | Iter Mean Loss 0.2640\n",
      "2024-11-14 04:41:19,999 - root - INFO - KG Training: Epoch 0001 Iter 1074 / 3136 | Time 0.1s | Iter Loss 0.1253 | Iter Mean Loss 0.2639\n",
      "2024-11-14 04:41:20,109 - root - INFO - KG Training: Epoch 0001 Iter 1075 / 3136 | Time 0.1s | Iter Loss 0.1174 | Iter Mean Loss 0.2638\n",
      "2024-11-14 04:41:20,188 - root - INFO - KG Training: Epoch 0001 Iter 1076 / 3136 | Time 0.1s | Iter Loss 0.1231 | Iter Mean Loss 0.2636\n",
      "2024-11-14 04:41:20,251 - root - INFO - KG Training: Epoch 0001 Iter 1077 / 3136 | Time 0.1s | Iter Loss 0.1175 | Iter Mean Loss 0.2635\n",
      "2024-11-14 04:41:20,305 - root - INFO - KG Training: Epoch 0001 Iter 1078 / 3136 | Time 0.1s | Iter Loss 0.1181 | Iter Mean Loss 0.2634\n",
      "2024-11-14 04:41:20,364 - root - INFO - KG Training: Epoch 0001 Iter 1079 / 3136 | Time 0.1s | Iter Loss 0.1197 | Iter Mean Loss 0.2632\n",
      "2024-11-14 04:41:20,425 - root - INFO - KG Training: Epoch 0001 Iter 1080 / 3136 | Time 0.1s | Iter Loss 0.1190 | Iter Mean Loss 0.2631\n",
      "2024-11-14 04:41:20,484 - root - INFO - KG Training: Epoch 0001 Iter 1081 / 3136 | Time 0.1s | Iter Loss 0.1110 | Iter Mean Loss 0.2630\n",
      "2024-11-14 04:41:20,818 - root - INFO - KG Training: Epoch 0001 Iter 1082 / 3136 | Time 0.3s | Iter Loss 0.1203 | Iter Mean Loss 0.2628\n",
      "2024-11-14 04:41:20,886 - root - INFO - KG Training: Epoch 0001 Iter 1083 / 3136 | Time 0.1s | Iter Loss 0.1152 | Iter Mean Loss 0.2627\n",
      "2024-11-14 04:41:20,949 - root - INFO - KG Training: Epoch 0001 Iter 1084 / 3136 | Time 0.1s | Iter Loss 0.1104 | Iter Mean Loss 0.2626\n",
      "2024-11-14 04:41:21,009 - root - INFO - KG Training: Epoch 0001 Iter 1085 / 3136 | Time 0.1s | Iter Loss 0.1129 | Iter Mean Loss 0.2624\n",
      "2024-11-14 04:41:21,075 - root - INFO - KG Training: Epoch 0001 Iter 1086 / 3136 | Time 0.1s | Iter Loss 0.1163 | Iter Mean Loss 0.2623\n",
      "2024-11-14 04:41:21,136 - root - INFO - KG Training: Epoch 0001 Iter 1087 / 3136 | Time 0.1s | Iter Loss 0.1146 | Iter Mean Loss 0.2622\n",
      "2024-11-14 04:41:21,198 - root - INFO - KG Training: Epoch 0001 Iter 1088 / 3136 | Time 0.1s | Iter Loss 0.1172 | Iter Mean Loss 0.2620\n",
      "2024-11-14 04:41:21,258 - root - INFO - KG Training: Epoch 0001 Iter 1089 / 3136 | Time 0.1s | Iter Loss 0.1064 | Iter Mean Loss 0.2619\n",
      "2024-11-14 04:41:21,317 - root - INFO - KG Training: Epoch 0001 Iter 1090 / 3136 | Time 0.1s | Iter Loss 0.1077 | Iter Mean Loss 0.2617\n",
      "2024-11-14 04:41:21,379 - root - INFO - KG Training: Epoch 0001 Iter 1091 / 3136 | Time 0.1s | Iter Loss 0.1114 | Iter Mean Loss 0.2616\n",
      "2024-11-14 04:41:21,439 - root - INFO - KG Training: Epoch 0001 Iter 1092 / 3136 | Time 0.1s | Iter Loss 0.1204 | Iter Mean Loss 0.2615\n",
      "2024-11-14 04:41:21,499 - root - INFO - KG Training: Epoch 0001 Iter 1093 / 3136 | Time 0.1s | Iter Loss 0.1058 | Iter Mean Loss 0.2613\n",
      "2024-11-14 04:41:21,559 - root - INFO - KG Training: Epoch 0001 Iter 1094 / 3136 | Time 0.1s | Iter Loss 0.1225 | Iter Mean Loss 0.2612\n",
      "2024-11-14 04:41:21,627 - root - INFO - KG Training: Epoch 0001 Iter 1095 / 3136 | Time 0.1s | Iter Loss 0.1083 | Iter Mean Loss 0.2611\n",
      "2024-11-14 04:41:21,688 - root - INFO - KG Training: Epoch 0001 Iter 1096 / 3136 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.2609\n",
      "2024-11-14 04:41:21,748 - root - INFO - KG Training: Epoch 0001 Iter 1097 / 3136 | Time 0.1s | Iter Loss 0.1146 | Iter Mean Loss 0.2608\n",
      "2024-11-14 04:41:21,805 - root - INFO - KG Training: Epoch 0001 Iter 1098 / 3136 | Time 0.1s | Iter Loss 0.1052 | Iter Mean Loss 0.2607\n",
      "2024-11-14 04:41:21,883 - root - INFO - KG Training: Epoch 0001 Iter 1099 / 3136 | Time 0.1s | Iter Loss 0.1094 | Iter Mean Loss 0.2605\n",
      "2024-11-14 04:41:21,945 - root - INFO - KG Training: Epoch 0001 Iter 1100 / 3136 | Time 0.1s | Iter Loss 0.1196 | Iter Mean Loss 0.2604\n",
      "2024-11-14 04:41:22,075 - root - INFO - KG Training: Epoch 0001 Iter 1101 / 3136 | Time 0.1s | Iter Loss 0.1155 | Iter Mean Loss 0.2603\n",
      "2024-11-14 04:41:22,139 - root - INFO - KG Training: Epoch 0001 Iter 1102 / 3136 | Time 0.1s | Iter Loss 0.1135 | Iter Mean Loss 0.2601\n",
      "2024-11-14 04:41:22,200 - root - INFO - KG Training: Epoch 0001 Iter 1103 / 3136 | Time 0.1s | Iter Loss 0.1147 | Iter Mean Loss 0.2600\n",
      "2024-11-14 04:41:22,259 - root - INFO - KG Training: Epoch 0001 Iter 1104 / 3136 | Time 0.1s | Iter Loss 0.1197 | Iter Mean Loss 0.2599\n",
      "2024-11-14 04:41:22,319 - root - INFO - KG Training: Epoch 0001 Iter 1105 / 3136 | Time 0.1s | Iter Loss 0.1194 | Iter Mean Loss 0.2597\n",
      "2024-11-14 04:41:22,377 - root - INFO - KG Training: Epoch 0001 Iter 1106 / 3136 | Time 0.1s | Iter Loss 0.1062 | Iter Mean Loss 0.2596\n",
      "2024-11-14 04:41:22,437 - root - INFO - KG Training: Epoch 0001 Iter 1107 / 3136 | Time 0.1s | Iter Loss 0.1213 | Iter Mean Loss 0.2595\n",
      "2024-11-14 04:41:22,495 - root - INFO - KG Training: Epoch 0001 Iter 1108 / 3136 | Time 0.1s | Iter Loss 0.1122 | Iter Mean Loss 0.2593\n",
      "2024-11-14 04:41:22,558 - root - INFO - KG Training: Epoch 0001 Iter 1109 / 3136 | Time 0.1s | Iter Loss 0.1207 | Iter Mean Loss 0.2592\n",
      "2024-11-14 04:41:22,618 - root - INFO - KG Training: Epoch 0001 Iter 1110 / 3136 | Time 0.1s | Iter Loss 0.1235 | Iter Mean Loss 0.2591\n",
      "2024-11-14 04:41:22,677 - root - INFO - KG Training: Epoch 0001 Iter 1111 / 3136 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.2590\n",
      "2024-11-14 04:41:22,734 - root - INFO - KG Training: Epoch 0001 Iter 1112 / 3136 | Time 0.1s | Iter Loss 0.1159 | Iter Mean Loss 0.2588\n",
      "2024-11-14 04:41:22,794 - root - INFO - KG Training: Epoch 0001 Iter 1113 / 3136 | Time 0.1s | Iter Loss 0.1066 | Iter Mean Loss 0.2587\n",
      "2024-11-14 04:41:22,861 - root - INFO - KG Training: Epoch 0001 Iter 1114 / 3136 | Time 0.1s | Iter Loss 0.1039 | Iter Mean Loss 0.2586\n",
      "2024-11-14 04:41:22,920 - root - INFO - KG Training: Epoch 0001 Iter 1115 / 3136 | Time 0.1s | Iter Loss 0.1174 | Iter Mean Loss 0.2584\n",
      "2024-11-14 04:41:22,979 - root - INFO - KG Training: Epoch 0001 Iter 1116 / 3136 | Time 0.1s | Iter Loss 0.1132 | Iter Mean Loss 0.2583\n",
      "2024-11-14 04:41:23,039 - root - INFO - KG Training: Epoch 0001 Iter 1117 / 3136 | Time 0.1s | Iter Loss 0.1008 | Iter Mean Loss 0.2582\n",
      "2024-11-14 04:41:23,098 - root - INFO - KG Training: Epoch 0001 Iter 1118 / 3136 | Time 0.1s | Iter Loss 0.1077 | Iter Mean Loss 0.2580\n",
      "2024-11-14 04:41:23,172 - root - INFO - KG Training: Epoch 0001 Iter 1119 / 3136 | Time 0.1s | Iter Loss 0.1120 | Iter Mean Loss 0.2579\n",
      "2024-11-14 04:41:23,231 - root - INFO - KG Training: Epoch 0001 Iter 1120 / 3136 | Time 0.1s | Iter Loss 0.1142 | Iter Mean Loss 0.2578\n",
      "2024-11-14 04:41:23,295 - root - INFO - KG Training: Epoch 0001 Iter 1121 / 3136 | Time 0.1s | Iter Loss 0.1257 | Iter Mean Loss 0.2576\n",
      "2024-11-14 04:41:23,355 - root - INFO - KG Training: Epoch 0001 Iter 1122 / 3136 | Time 0.1s | Iter Loss 0.1217 | Iter Mean Loss 0.2575\n",
      "2024-11-14 04:41:23,463 - root - INFO - KG Training: Epoch 0001 Iter 1123 / 3136 | Time 0.1s | Iter Loss 0.1148 | Iter Mean Loss 0.2574\n",
      "2024-11-14 04:41:23,520 - root - INFO - KG Training: Epoch 0001 Iter 1124 / 3136 | Time 0.1s | Iter Loss 0.1162 | Iter Mean Loss 0.2573\n",
      "2024-11-14 04:41:23,578 - root - INFO - KG Training: Epoch 0001 Iter 1125 / 3136 | Time 0.1s | Iter Loss 0.1104 | Iter Mean Loss 0.2571\n",
      "2024-11-14 04:41:23,636 - root - INFO - KG Training: Epoch 0001 Iter 1126 / 3136 | Time 0.1s | Iter Loss 0.1178 | Iter Mean Loss 0.2570\n",
      "2024-11-14 04:41:23,695 - root - INFO - KG Training: Epoch 0001 Iter 1127 / 3136 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.2569\n",
      "2024-11-14 04:41:23,812 - root - INFO - KG Training: Epoch 0001 Iter 1128 / 3136 | Time 0.1s | Iter Loss 0.1209 | Iter Mean Loss 0.2568\n",
      "2024-11-14 04:41:23,871 - root - INFO - KG Training: Epoch 0001 Iter 1129 / 3136 | Time 0.1s | Iter Loss 0.1064 | Iter Mean Loss 0.2566\n",
      "2024-11-14 04:41:23,930 - root - INFO - KG Training: Epoch 0001 Iter 1130 / 3136 | Time 0.1s | Iter Loss 0.1131 | Iter Mean Loss 0.2565\n",
      "2024-11-14 04:41:23,989 - root - INFO - KG Training: Epoch 0001 Iter 1131 / 3136 | Time 0.1s | Iter Loss 0.1101 | Iter Mean Loss 0.2564\n",
      "2024-11-14 04:41:24,047 - root - INFO - KG Training: Epoch 0001 Iter 1132 / 3136 | Time 0.1s | Iter Loss 0.1130 | Iter Mean Loss 0.2563\n",
      "2024-11-14 04:41:24,106 - root - INFO - KG Training: Epoch 0001 Iter 1133 / 3136 | Time 0.1s | Iter Loss 0.1146 | Iter Mean Loss 0.2561\n",
      "2024-11-14 04:41:24,167 - root - INFO - KG Training: Epoch 0001 Iter 1134 / 3136 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.2560\n",
      "2024-11-14 04:41:24,228 - root - INFO - KG Training: Epoch 0001 Iter 1135 / 3136 | Time 0.1s | Iter Loss 0.1139 | Iter Mean Loss 0.2559\n",
      "2024-11-14 04:41:24,291 - root - INFO - KG Training: Epoch 0001 Iter 1136 / 3136 | Time 0.1s | Iter Loss 0.1163 | Iter Mean Loss 0.2557\n",
      "2024-11-14 04:41:24,351 - root - INFO - KG Training: Epoch 0001 Iter 1137 / 3136 | Time 0.1s | Iter Loss 0.1027 | Iter Mean Loss 0.2556\n",
      "2024-11-14 04:41:24,411 - root - INFO - KG Training: Epoch 0001 Iter 1138 / 3136 | Time 0.1s | Iter Loss 0.1075 | Iter Mean Loss 0.2555\n",
      "2024-11-14 04:41:24,470 - root - INFO - KG Training: Epoch 0001 Iter 1139 / 3136 | Time 0.1s | Iter Loss 0.1051 | Iter Mean Loss 0.2553\n",
      "2024-11-14 04:41:24,528 - root - INFO - KG Training: Epoch 0001 Iter 1140 / 3136 | Time 0.1s | Iter Loss 0.1188 | Iter Mean Loss 0.2552\n",
      "2024-11-14 04:41:24,585 - root - INFO - KG Training: Epoch 0001 Iter 1141 / 3136 | Time 0.1s | Iter Loss 0.1066 | Iter Mean Loss 0.2551\n",
      "2024-11-14 04:41:24,648 - root - INFO - KG Training: Epoch 0001 Iter 1142 / 3136 | Time 0.1s | Iter Loss 0.1052 | Iter Mean Loss 0.2550\n",
      "2024-11-14 04:41:24,706 - root - INFO - KG Training: Epoch 0001 Iter 1143 / 3136 | Time 0.1s | Iter Loss 0.1087 | Iter Mean Loss 0.2548\n",
      "2024-11-14 04:41:24,766 - root - INFO - KG Training: Epoch 0001 Iter 1144 / 3136 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.2547\n",
      "2024-11-14 04:41:24,949 - root - INFO - KG Training: Epoch 0001 Iter 1145 / 3136 | Time 0.2s | Iter Loss 0.1038 | Iter Mean Loss 0.2546\n",
      "2024-11-14 04:41:25,009 - root - INFO - KG Training: Epoch 0001 Iter 1146 / 3136 | Time 0.1s | Iter Loss 0.1101 | Iter Mean Loss 0.2545\n",
      "2024-11-14 04:41:25,071 - root - INFO - KG Training: Epoch 0001 Iter 1147 / 3136 | Time 0.1s | Iter Loss 0.1129 | Iter Mean Loss 0.2543\n",
      "2024-11-14 04:41:25,130 - root - INFO - KG Training: Epoch 0001 Iter 1148 / 3136 | Time 0.1s | Iter Loss 0.0996 | Iter Mean Loss 0.2542\n",
      "2024-11-14 04:41:25,192 - root - INFO - KG Training: Epoch 0001 Iter 1149 / 3136 | Time 0.1s | Iter Loss 0.1095 | Iter Mean Loss 0.2541\n",
      "2024-11-14 04:41:25,260 - root - INFO - KG Training: Epoch 0001 Iter 1150 / 3136 | Time 0.1s | Iter Loss 0.1063 | Iter Mean Loss 0.2540\n",
      "2024-11-14 04:41:25,325 - root - INFO - KG Training: Epoch 0001 Iter 1151 / 3136 | Time 0.1s | Iter Loss 0.1056 | Iter Mean Loss 0.2538\n",
      "2024-11-14 04:41:25,399 - root - INFO - KG Training: Epoch 0001 Iter 1152 / 3136 | Time 0.1s | Iter Loss 0.1248 | Iter Mean Loss 0.2537\n",
      "2024-11-14 04:41:25,464 - root - INFO - KG Training: Epoch 0001 Iter 1153 / 3136 | Time 0.1s | Iter Loss 0.1173 | Iter Mean Loss 0.2536\n",
      "2024-11-14 04:41:25,526 - root - INFO - KG Training: Epoch 0001 Iter 1154 / 3136 | Time 0.1s | Iter Loss 0.1060 | Iter Mean Loss 0.2535\n",
      "2024-11-14 04:41:25,584 - root - INFO - KG Training: Epoch 0001 Iter 1155 / 3136 | Time 0.1s | Iter Loss 0.1202 | Iter Mean Loss 0.2533\n",
      "2024-11-14 04:41:25,649 - root - INFO - KG Training: Epoch 0001 Iter 1156 / 3136 | Time 0.1s | Iter Loss 0.1092 | Iter Mean Loss 0.2532\n",
      "2024-11-14 04:41:25,708 - root - INFO - KG Training: Epoch 0001 Iter 1157 / 3136 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.2531\n",
      "2024-11-14 04:41:25,777 - root - INFO - KG Training: Epoch 0001 Iter 1158 / 3136 | Time 0.1s | Iter Loss 0.1057 | Iter Mean Loss 0.2530\n",
      "2024-11-14 04:41:25,856 - root - INFO - KG Training: Epoch 0001 Iter 1159 / 3136 | Time 0.1s | Iter Loss 0.1088 | Iter Mean Loss 0.2529\n",
      "2024-11-14 04:41:25,913 - root - INFO - KG Training: Epoch 0001 Iter 1160 / 3136 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.2527\n",
      "2024-11-14 04:41:25,987 - root - INFO - KG Training: Epoch 0001 Iter 1161 / 3136 | Time 0.1s | Iter Loss 0.1069 | Iter Mean Loss 0.2526\n",
      "2024-11-14 04:41:26,046 - root - INFO - KG Training: Epoch 0001 Iter 1162 / 3136 | Time 0.1s | Iter Loss 0.1035 | Iter Mean Loss 0.2525\n",
      "2024-11-14 04:41:26,123 - root - INFO - KG Training: Epoch 0001 Iter 1163 / 3136 | Time 0.1s | Iter Loss 0.1108 | Iter Mean Loss 0.2524\n",
      "2024-11-14 04:41:26,194 - root - INFO - KG Training: Epoch 0001 Iter 1164 / 3136 | Time 0.1s | Iter Loss 0.1005 | Iter Mean Loss 0.2522\n",
      "2024-11-14 04:41:26,250 - root - INFO - KG Training: Epoch 0001 Iter 1165 / 3136 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.2521\n",
      "2024-11-14 04:41:26,319 - root - INFO - KG Training: Epoch 0001 Iter 1166 / 3136 | Time 0.1s | Iter Loss 0.1143 | Iter Mean Loss 0.2520\n",
      "2024-11-14 04:41:26,391 - root - INFO - KG Training: Epoch 0001 Iter 1167 / 3136 | Time 0.1s | Iter Loss 0.1077 | Iter Mean Loss 0.2519\n",
      "2024-11-14 04:41:26,454 - root - INFO - KG Training: Epoch 0001 Iter 1168 / 3136 | Time 0.1s | Iter Loss 0.0991 | Iter Mean Loss 0.2517\n",
      "2024-11-14 04:41:26,513 - root - INFO - KG Training: Epoch 0001 Iter 1169 / 3136 | Time 0.1s | Iter Loss 0.1125 | Iter Mean Loss 0.2516\n",
      "2024-11-14 04:41:26,572 - root - INFO - KG Training: Epoch 0001 Iter 1170 / 3136 | Time 0.1s | Iter Loss 0.1084 | Iter Mean Loss 0.2515\n",
      "2024-11-14 04:41:26,632 - root - INFO - KG Training: Epoch 0001 Iter 1171 / 3136 | Time 0.1s | Iter Loss 0.1038 | Iter Mean Loss 0.2514\n",
      "2024-11-14 04:41:26,691 - root - INFO - KG Training: Epoch 0001 Iter 1172 / 3136 | Time 0.1s | Iter Loss 0.1106 | Iter Mean Loss 0.2512\n",
      "2024-11-14 04:41:26,754 - root - INFO - KG Training: Epoch 0001 Iter 1173 / 3136 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.2511\n",
      "2024-11-14 04:41:26,867 - root - INFO - KG Training: Epoch 0001 Iter 1174 / 3136 | Time 0.1s | Iter Loss 0.1191 | Iter Mean Loss 0.2510\n",
      "2024-11-14 04:41:26,925 - root - INFO - KG Training: Epoch 0001 Iter 1175 / 3136 | Time 0.1s | Iter Loss 0.1071 | Iter Mean Loss 0.2509\n",
      "2024-11-14 04:41:26,983 - root - INFO - KG Training: Epoch 0001 Iter 1176 / 3136 | Time 0.1s | Iter Loss 0.0998 | Iter Mean Loss 0.2508\n",
      "2024-11-14 04:41:27,097 - root - INFO - KG Training: Epoch 0001 Iter 1177 / 3136 | Time 0.1s | Iter Loss 0.1064 | Iter Mean Loss 0.2506\n",
      "2024-11-14 04:41:27,156 - root - INFO - KG Training: Epoch 0001 Iter 1178 / 3136 | Time 0.1s | Iter Loss 0.1093 | Iter Mean Loss 0.2505\n",
      "2024-11-14 04:41:27,215 - root - INFO - KG Training: Epoch 0001 Iter 1179 / 3136 | Time 0.1s | Iter Loss 0.1104 | Iter Mean Loss 0.2504\n",
      "2024-11-14 04:41:27,303 - root - INFO - KG Training: Epoch 0001 Iter 1180 / 3136 | Time 0.1s | Iter Loss 0.1032 | Iter Mean Loss 0.2503\n",
      "2024-11-14 04:41:27,365 - root - INFO - KG Training: Epoch 0001 Iter 1181 / 3136 | Time 0.1s | Iter Loss 0.1085 | Iter Mean Loss 0.2502\n",
      "2024-11-14 04:41:27,425 - root - INFO - KG Training: Epoch 0001 Iter 1182 / 3136 | Time 0.1s | Iter Loss 0.1103 | Iter Mean Loss 0.2500\n",
      "2024-11-14 04:41:27,488 - root - INFO - KG Training: Epoch 0001 Iter 1183 / 3136 | Time 0.1s | Iter Loss 0.1005 | Iter Mean Loss 0.2499\n",
      "2024-11-14 04:41:27,549 - root - INFO - KG Training: Epoch 0001 Iter 1184 / 3136 | Time 0.1s | Iter Loss 0.1176 | Iter Mean Loss 0.2498\n",
      "2024-11-14 04:41:27,613 - root - INFO - KG Training: Epoch 0001 Iter 1185 / 3136 | Time 0.1s | Iter Loss 0.1058 | Iter Mean Loss 0.2497\n",
      "2024-11-14 04:41:27,678 - root - INFO - KG Training: Epoch 0001 Iter 1186 / 3136 | Time 0.1s | Iter Loss 0.1183 | Iter Mean Loss 0.2496\n",
      "2024-11-14 04:41:27,736 - root - INFO - KG Training: Epoch 0001 Iter 1187 / 3136 | Time 0.1s | Iter Loss 0.1182 | Iter Mean Loss 0.2495\n",
      "2024-11-14 04:41:27,797 - root - INFO - KG Training: Epoch 0001 Iter 1188 / 3136 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.2493\n",
      "2024-11-14 04:41:27,855 - root - INFO - KG Training: Epoch 0001 Iter 1189 / 3136 | Time 0.1s | Iter Loss 0.0988 | Iter Mean Loss 0.2492\n",
      "2024-11-14 04:41:27,912 - root - INFO - KG Training: Epoch 0001 Iter 1190 / 3136 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.2491\n",
      "2024-11-14 04:41:27,975 - root - INFO - KG Training: Epoch 0001 Iter 1191 / 3136 | Time 0.1s | Iter Loss 0.1171 | Iter Mean Loss 0.2490\n",
      "2024-11-14 04:41:28,033 - root - INFO - KG Training: Epoch 0001 Iter 1192 / 3136 | Time 0.1s | Iter Loss 0.1002 | Iter Mean Loss 0.2489\n",
      "2024-11-14 04:41:28,090 - root - INFO - KG Training: Epoch 0001 Iter 1193 / 3136 | Time 0.1s | Iter Loss 0.0953 | Iter Mean Loss 0.2487\n",
      "2024-11-14 04:41:28,151 - root - INFO - KG Training: Epoch 0001 Iter 1194 / 3136 | Time 0.1s | Iter Loss 0.1076 | Iter Mean Loss 0.2486\n",
      "2024-11-14 04:41:28,214 - root - INFO - KG Training: Epoch 0001 Iter 1195 / 3136 | Time 0.1s | Iter Loss 0.1132 | Iter Mean Loss 0.2485\n",
      "2024-11-14 04:41:28,297 - root - INFO - KG Training: Epoch 0001 Iter 1196 / 3136 | Time 0.1s | Iter Loss 0.1067 | Iter Mean Loss 0.2484\n",
      "2024-11-14 04:41:28,361 - root - INFO - KG Training: Epoch 0001 Iter 1197 / 3136 | Time 0.1s | Iter Loss 0.1014 | Iter Mean Loss 0.2483\n",
      "2024-11-14 04:41:28,423 - root - INFO - KG Training: Epoch 0001 Iter 1198 / 3136 | Time 0.1s | Iter Loss 0.1037 | Iter Mean Loss 0.2481\n",
      "2024-11-14 04:41:28,486 - root - INFO - KG Training: Epoch 0001 Iter 1199 / 3136 | Time 0.1s | Iter Loss 0.1133 | Iter Mean Loss 0.2480\n",
      "2024-11-14 04:41:28,555 - root - INFO - KG Training: Epoch 0001 Iter 1200 / 3136 | Time 0.1s | Iter Loss 0.0964 | Iter Mean Loss 0.2479\n",
      "2024-11-14 04:41:28,616 - root - INFO - KG Training: Epoch 0001 Iter 1201 / 3136 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.2478\n",
      "2024-11-14 04:41:28,679 - root - INFO - KG Training: Epoch 0001 Iter 1202 / 3136 | Time 0.1s | Iter Loss 0.1113 | Iter Mean Loss 0.2477\n",
      "2024-11-14 04:41:28,742 - root - INFO - KG Training: Epoch 0001 Iter 1203 / 3136 | Time 0.1s | Iter Loss 0.1124 | Iter Mean Loss 0.2476\n",
      "2024-11-14 04:41:28,816 - root - INFO - KG Training: Epoch 0001 Iter 1204 / 3136 | Time 0.1s | Iter Loss 0.1146 | Iter Mean Loss 0.2475\n",
      "2024-11-14 04:41:28,880 - root - INFO - KG Training: Epoch 0001 Iter 1205 / 3136 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.2473\n",
      "2024-11-14 04:41:28,940 - root - INFO - KG Training: Epoch 0001 Iter 1206 / 3136 | Time 0.1s | Iter Loss 0.1088 | Iter Mean Loss 0.2472\n",
      "2024-11-14 04:41:29,003 - root - INFO - KG Training: Epoch 0001 Iter 1207 / 3136 | Time 0.1s | Iter Loss 0.1155 | Iter Mean Loss 0.2471\n",
      "2024-11-14 04:41:29,066 - root - INFO - KG Training: Epoch 0001 Iter 1208 / 3136 | Time 0.1s | Iter Loss 0.1005 | Iter Mean Loss 0.2470\n",
      "2024-11-14 04:41:29,127 - root - INFO - KG Training: Epoch 0001 Iter 1209 / 3136 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.2469\n",
      "2024-11-14 04:41:29,188 - root - INFO - KG Training: Epoch 0001 Iter 1210 / 3136 | Time 0.1s | Iter Loss 0.1124 | Iter Mean Loss 0.2468\n",
      "2024-11-14 04:41:29,249 - root - INFO - KG Training: Epoch 0001 Iter 1211 / 3136 | Time 0.1s | Iter Loss 0.1034 | Iter Mean Loss 0.2466\n",
      "2024-11-14 04:41:29,310 - root - INFO - KG Training: Epoch 0001 Iter 1212 / 3136 | Time 0.1s | Iter Loss 0.0938 | Iter Mean Loss 0.2465\n",
      "2024-11-14 04:41:29,370 - root - INFO - KG Training: Epoch 0001 Iter 1213 / 3136 | Time 0.1s | Iter Loss 0.1043 | Iter Mean Loss 0.2464\n",
      "2024-11-14 04:41:29,435 - root - INFO - KG Training: Epoch 0001 Iter 1214 / 3136 | Time 0.1s | Iter Loss 0.1144 | Iter Mean Loss 0.2463\n",
      "2024-11-14 04:41:29,494 - root - INFO - KG Training: Epoch 0001 Iter 1215 / 3136 | Time 0.1s | Iter Loss 0.1093 | Iter Mean Loss 0.2462\n",
      "2024-11-14 04:41:29,555 - root - INFO - KG Training: Epoch 0001 Iter 1216 / 3136 | Time 0.1s | Iter Loss 0.1115 | Iter Mean Loss 0.2461\n",
      "2024-11-14 04:41:29,613 - root - INFO - KG Training: Epoch 0001 Iter 1217 / 3136 | Time 0.1s | Iter Loss 0.1009 | Iter Mean Loss 0.2460\n",
      "2024-11-14 04:41:29,671 - root - INFO - KG Training: Epoch 0001 Iter 1218 / 3136 | Time 0.1s | Iter Loss 0.1139 | Iter Mean Loss 0.2458\n",
      "2024-11-14 04:41:29,781 - root - INFO - KG Training: Epoch 0001 Iter 1219 / 3136 | Time 0.1s | Iter Loss 0.1100 | Iter Mean Loss 0.2457\n",
      "2024-11-14 04:41:29,844 - root - INFO - KG Training: Epoch 0001 Iter 1220 / 3136 | Time 0.1s | Iter Loss 0.1144 | Iter Mean Loss 0.2456\n",
      "2024-11-14 04:41:29,985 - root - INFO - KG Training: Epoch 0001 Iter 1221 / 3136 | Time 0.1s | Iter Loss 0.1115 | Iter Mean Loss 0.2455\n",
      "2024-11-14 04:41:30,043 - root - INFO - KG Training: Epoch 0001 Iter 1222 / 3136 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.2454\n",
      "2024-11-14 04:41:30,104 - root - INFO - KG Training: Epoch 0001 Iter 1223 / 3136 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.2453\n",
      "2024-11-14 04:41:30,163 - root - INFO - KG Training: Epoch 0001 Iter 1224 / 3136 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.2452\n",
      "2024-11-14 04:41:30,221 - root - INFO - KG Training: Epoch 0001 Iter 1225 / 3136 | Time 0.1s | Iter Loss 0.1153 | Iter Mean Loss 0.2451\n",
      "2024-11-14 04:41:30,280 - root - INFO - KG Training: Epoch 0001 Iter 1226 / 3136 | Time 0.1s | Iter Loss 0.0985 | Iter Mean Loss 0.2449\n",
      "2024-11-14 04:41:30,340 - root - INFO - KG Training: Epoch 0001 Iter 1227 / 3136 | Time 0.1s | Iter Loss 0.1064 | Iter Mean Loss 0.2448\n",
      "2024-11-14 04:41:30,398 - root - INFO - KG Training: Epoch 0001 Iter 1228 / 3136 | Time 0.1s | Iter Loss 0.1176 | Iter Mean Loss 0.2447\n",
      "2024-11-14 04:41:30,458 - root - INFO - KG Training: Epoch 0001 Iter 1229 / 3136 | Time 0.1s | Iter Loss 0.1096 | Iter Mean Loss 0.2446\n",
      "2024-11-14 04:41:30,517 - root - INFO - KG Training: Epoch 0001 Iter 1230 / 3136 | Time 0.1s | Iter Loss 0.1023 | Iter Mean Loss 0.2445\n",
      "2024-11-14 04:41:30,578 - root - INFO - KG Training: Epoch 0001 Iter 1231 / 3136 | Time 0.1s | Iter Loss 0.1080 | Iter Mean Loss 0.2444\n",
      "2024-11-14 04:41:30,640 - root - INFO - KG Training: Epoch 0001 Iter 1232 / 3136 | Time 0.1s | Iter Loss 0.1109 | Iter Mean Loss 0.2443\n",
      "2024-11-14 04:41:30,700 - root - INFO - KG Training: Epoch 0001 Iter 1233 / 3136 | Time 0.1s | Iter Loss 0.1062 | Iter Mean Loss 0.2442\n",
      "2024-11-14 04:41:30,760 - root - INFO - KG Training: Epoch 0001 Iter 1234 / 3136 | Time 0.1s | Iter Loss 0.1149 | Iter Mean Loss 0.2441\n",
      "2024-11-14 04:41:30,820 - root - INFO - KG Training: Epoch 0001 Iter 1235 / 3136 | Time 0.1s | Iter Loss 0.1079 | Iter Mean Loss 0.2440\n",
      "2024-11-14 04:41:30,882 - root - INFO - KG Training: Epoch 0001 Iter 1236 / 3136 | Time 0.1s | Iter Loss 0.1171 | Iter Mean Loss 0.2438\n",
      "2024-11-14 04:41:30,937 - root - INFO - KG Training: Epoch 0001 Iter 1237 / 3136 | Time 0.1s | Iter Loss 0.1136 | Iter Mean Loss 0.2437\n",
      "2024-11-14 04:41:30,996 - root - INFO - KG Training: Epoch 0001 Iter 1238 / 3136 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.2436\n",
      "2024-11-14 04:41:31,051 - root - INFO - KG Training: Epoch 0001 Iter 1239 / 3136 | Time 0.1s | Iter Loss 0.1054 | Iter Mean Loss 0.2435\n",
      "2024-11-14 04:41:31,111 - root - INFO - KG Training: Epoch 0001 Iter 1240 / 3136 | Time 0.1s | Iter Loss 0.1022 | Iter Mean Loss 0.2434\n",
      "2024-11-14 04:41:31,171 - root - INFO - KG Training: Epoch 0001 Iter 1241 / 3136 | Time 0.1s | Iter Loss 0.1101 | Iter Mean Loss 0.2433\n",
      "2024-11-14 04:41:31,231 - root - INFO - KG Training: Epoch 0001 Iter 1242 / 3136 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.2432\n",
      "2024-11-14 04:41:31,290 - root - INFO - KG Training: Epoch 0001 Iter 1243 / 3136 | Time 0.1s | Iter Loss 0.1133 | Iter Mean Loss 0.2431\n",
      "2024-11-14 04:41:31,407 - root - INFO - KG Training: Epoch 0001 Iter 1244 / 3136 | Time 0.1s | Iter Loss 0.1044 | Iter Mean Loss 0.2430\n",
      "2024-11-14 04:41:31,467 - root - INFO - KG Training: Epoch 0001 Iter 1245 / 3136 | Time 0.1s | Iter Loss 0.1112 | Iter Mean Loss 0.2429\n",
      "2024-11-14 04:41:31,542 - root - INFO - KG Training: Epoch 0001 Iter 1246 / 3136 | Time 0.1s | Iter Loss 0.1020 | Iter Mean Loss 0.2428\n",
      "2024-11-14 04:41:31,602 - root - INFO - KG Training: Epoch 0001 Iter 1247 / 3136 | Time 0.1s | Iter Loss 0.1107 | Iter Mean Loss 0.2426\n",
      "2024-11-14 04:41:31,662 - root - INFO - KG Training: Epoch 0001 Iter 1248 / 3136 | Time 0.1s | Iter Loss 0.1153 | Iter Mean Loss 0.2425\n",
      "2024-11-14 04:41:31,723 - root - INFO - KG Training: Epoch 0001 Iter 1249 / 3136 | Time 0.1s | Iter Loss 0.1088 | Iter Mean Loss 0.2424\n",
      "2024-11-14 04:41:31,783 - root - INFO - KG Training: Epoch 0001 Iter 1250 / 3136 | Time 0.1s | Iter Loss 0.0977 | Iter Mean Loss 0.2423\n",
      "2024-11-14 04:41:31,842 - root - INFO - KG Training: Epoch 0001 Iter 1251 / 3136 | Time 0.1s | Iter Loss 0.1045 | Iter Mean Loss 0.2422\n",
      "2024-11-14 04:41:31,901 - root - INFO - KG Training: Epoch 0001 Iter 1252 / 3136 | Time 0.1s | Iter Loss 0.1033 | Iter Mean Loss 0.2421\n",
      "2024-11-14 04:41:31,961 - root - INFO - KG Training: Epoch 0001 Iter 1253 / 3136 | Time 0.1s | Iter Loss 0.1073 | Iter Mean Loss 0.2420\n",
      "2024-11-14 04:41:32,020 - root - INFO - KG Training: Epoch 0001 Iter 1254 / 3136 | Time 0.1s | Iter Loss 0.1001 | Iter Mean Loss 0.2419\n",
      "2024-11-14 04:41:32,079 - root - INFO - KG Training: Epoch 0001 Iter 1255 / 3136 | Time 0.1s | Iter Loss 0.1028 | Iter Mean Loss 0.2418\n",
      "2024-11-14 04:41:32,137 - root - INFO - KG Training: Epoch 0001 Iter 1256 / 3136 | Time 0.1s | Iter Loss 0.1015 | Iter Mean Loss 0.2417\n",
      "2024-11-14 04:41:32,203 - root - INFO - KG Training: Epoch 0001 Iter 1257 / 3136 | Time 0.1s | Iter Loss 0.1079 | Iter Mean Loss 0.2415\n",
      "2024-11-14 04:41:32,262 - root - INFO - KG Training: Epoch 0001 Iter 1258 / 3136 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.2414\n",
      "2024-11-14 04:41:32,323 - root - INFO - KG Training: Epoch 0001 Iter 1259 / 3136 | Time 0.1s | Iter Loss 0.1041 | Iter Mean Loss 0.2413\n",
      "2024-11-14 04:41:32,387 - root - INFO - KG Training: Epoch 0001 Iter 1260 / 3136 | Time 0.1s | Iter Loss 0.1091 | Iter Mean Loss 0.2412\n",
      "2024-11-14 04:41:32,448 - root - INFO - KG Training: Epoch 0001 Iter 1261 / 3136 | Time 0.1s | Iter Loss 0.1028 | Iter Mean Loss 0.2411\n",
      "2024-11-14 04:41:32,508 - root - INFO - KG Training: Epoch 0001 Iter 1262 / 3136 | Time 0.1s | Iter Loss 0.1174 | Iter Mean Loss 0.2410\n",
      "2024-11-14 04:41:32,620 - root - INFO - KG Training: Epoch 0001 Iter 1263 / 3136 | Time 0.1s | Iter Loss 0.1092 | Iter Mean Loss 0.2409\n",
      "2024-11-14 04:41:32,741 - root - INFO - KG Training: Epoch 0001 Iter 1264 / 3136 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.2408\n",
      "2024-11-14 04:41:32,800 - root - INFO - KG Training: Epoch 0001 Iter 1265 / 3136 | Time 0.1s | Iter Loss 0.1053 | Iter Mean Loss 0.2407\n",
      "2024-11-14 04:41:32,862 - root - INFO - KG Training: Epoch 0001 Iter 1266 / 3136 | Time 0.1s | Iter Loss 0.1085 | Iter Mean Loss 0.2406\n",
      "2024-11-14 04:41:32,923 - root - INFO - KG Training: Epoch 0001 Iter 1267 / 3136 | Time 0.1s | Iter Loss 0.1129 | Iter Mean Loss 0.2405\n",
      "2024-11-14 04:41:32,983 - root - INFO - KG Training: Epoch 0001 Iter 1268 / 3136 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.2404\n",
      "2024-11-14 04:41:33,044 - root - INFO - KG Training: Epoch 0001 Iter 1269 / 3136 | Time 0.1s | Iter Loss 0.0965 | Iter Mean Loss 0.2403\n",
      "2024-11-14 04:41:33,106 - root - INFO - KG Training: Epoch 0001 Iter 1270 / 3136 | Time 0.1s | Iter Loss 0.0964 | Iter Mean Loss 0.2401\n",
      "2024-11-14 04:41:33,166 - root - INFO - KG Training: Epoch 0001 Iter 1271 / 3136 | Time 0.1s | Iter Loss 0.1097 | Iter Mean Loss 0.2400\n",
      "2024-11-14 04:41:33,225 - root - INFO - KG Training: Epoch 0001 Iter 1272 / 3136 | Time 0.1s | Iter Loss 0.1125 | Iter Mean Loss 0.2399\n",
      "2024-11-14 04:41:33,284 - root - INFO - KG Training: Epoch 0001 Iter 1273 / 3136 | Time 0.1s | Iter Loss 0.0990 | Iter Mean Loss 0.2398\n",
      "2024-11-14 04:41:33,344 - root - INFO - KG Training: Epoch 0001 Iter 1274 / 3136 | Time 0.1s | Iter Loss 0.0990 | Iter Mean Loss 0.2397\n",
      "2024-11-14 04:41:33,405 - root - INFO - KG Training: Epoch 0001 Iter 1275 / 3136 | Time 0.1s | Iter Loss 0.1043 | Iter Mean Loss 0.2396\n",
      "2024-11-14 04:41:33,466 - root - INFO - KG Training: Epoch 0001 Iter 1276 / 3136 | Time 0.1s | Iter Loss 0.0996 | Iter Mean Loss 0.2395\n",
      "2024-11-14 04:41:33,526 - root - INFO - KG Training: Epoch 0001 Iter 1277 / 3136 | Time 0.1s | Iter Loss 0.1069 | Iter Mean Loss 0.2394\n",
      "2024-11-14 04:41:33,588 - root - INFO - KG Training: Epoch 0001 Iter 1278 / 3136 | Time 0.1s | Iter Loss 0.0958 | Iter Mean Loss 0.2393\n",
      "2024-11-14 04:41:33,649 - root - INFO - KG Training: Epoch 0001 Iter 1279 / 3136 | Time 0.1s | Iter Loss 0.1111 | Iter Mean Loss 0.2392\n",
      "2024-11-14 04:41:33,709 - root - INFO - KG Training: Epoch 0001 Iter 1280 / 3136 | Time 0.1s | Iter Loss 0.0928 | Iter Mean Loss 0.2391\n",
      "2024-11-14 04:41:33,775 - root - INFO - KG Training: Epoch 0001 Iter 1281 / 3136 | Time 0.1s | Iter Loss 0.1116 | Iter Mean Loss 0.2390\n",
      "2024-11-14 04:41:33,835 - root - INFO - KG Training: Epoch 0001 Iter 1282 / 3136 | Time 0.1s | Iter Loss 0.1043 | Iter Mean Loss 0.2389\n",
      "2024-11-14 04:41:33,897 - root - INFO - KG Training: Epoch 0001 Iter 1283 / 3136 | Time 0.1s | Iter Loss 0.1105 | Iter Mean Loss 0.2388\n",
      "2024-11-14 04:41:33,958 - root - INFO - KG Training: Epoch 0001 Iter 1284 / 3136 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.2387\n",
      "2024-11-14 04:41:34,018 - root - INFO - KG Training: Epoch 0001 Iter 1285 / 3136 | Time 0.1s | Iter Loss 0.1004 | Iter Mean Loss 0.2386\n",
      "2024-11-14 04:41:34,079 - root - INFO - KG Training: Epoch 0001 Iter 1286 / 3136 | Time 0.1s | Iter Loss 0.0987 | Iter Mean Loss 0.2384\n",
      "2024-11-14 04:41:34,159 - root - INFO - KG Training: Epoch 0001 Iter 1287 / 3136 | Time 0.1s | Iter Loss 0.1040 | Iter Mean Loss 0.2383\n",
      "2024-11-14 04:41:34,217 - root - INFO - KG Training: Epoch 0001 Iter 1288 / 3136 | Time 0.1s | Iter Loss 0.0998 | Iter Mean Loss 0.2382\n",
      "2024-11-14 04:41:34,277 - root - INFO - KG Training: Epoch 0001 Iter 1289 / 3136 | Time 0.1s | Iter Loss 0.1048 | Iter Mean Loss 0.2381\n",
      "2024-11-14 04:41:34,349 - root - INFO - KG Training: Epoch 0001 Iter 1290 / 3136 | Time 0.1s | Iter Loss 0.0993 | Iter Mean Loss 0.2380\n",
      "2024-11-14 04:41:34,415 - root - INFO - KG Training: Epoch 0001 Iter 1291 / 3136 | Time 0.1s | Iter Loss 0.0940 | Iter Mean Loss 0.2379\n",
      "2024-11-14 04:41:34,474 - root - INFO - KG Training: Epoch 0001 Iter 1292 / 3136 | Time 0.1s | Iter Loss 0.1081 | Iter Mean Loss 0.2378\n",
      "2024-11-14 04:41:34,534 - root - INFO - KG Training: Epoch 0001 Iter 1293 / 3136 | Time 0.1s | Iter Loss 0.1032 | Iter Mean Loss 0.2377\n",
      "2024-11-14 04:41:34,598 - root - INFO - KG Training: Epoch 0001 Iter 1294 / 3136 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.2376\n",
      "2024-11-14 04:41:34,929 - root - INFO - KG Training: Epoch 0001 Iter 1295 / 3136 | Time 0.3s | Iter Loss 0.0948 | Iter Mean Loss 0.2375\n",
      "2024-11-14 04:41:34,993 - root - INFO - KG Training: Epoch 0001 Iter 1296 / 3136 | Time 0.1s | Iter Loss 0.0951 | Iter Mean Loss 0.2374\n",
      "2024-11-14 04:41:35,053 - root - INFO - KG Training: Epoch 0001 Iter 1297 / 3136 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.2373\n",
      "2024-11-14 04:41:35,114 - root - INFO - KG Training: Epoch 0001 Iter 1298 / 3136 | Time 0.1s | Iter Loss 0.1067 | Iter Mean Loss 0.2372\n",
      "2024-11-14 04:41:35,175 - root - INFO - KG Training: Epoch 0001 Iter 1299 / 3136 | Time 0.1s | Iter Loss 0.0941 | Iter Mean Loss 0.2371\n",
      "2024-11-14 04:41:35,300 - root - INFO - KG Training: Epoch 0001 Iter 1300 / 3136 | Time 0.1s | Iter Loss 0.1119 | Iter Mean Loss 0.2370\n",
      "2024-11-14 04:41:35,488 - root - INFO - KG Training: Epoch 0001 Iter 1301 / 3136 | Time 0.2s | Iter Loss 0.1013 | Iter Mean Loss 0.2369\n",
      "2024-11-14 04:41:35,550 - root - INFO - KG Training: Epoch 0001 Iter 1302 / 3136 | Time 0.1s | Iter Loss 0.1004 | Iter Mean Loss 0.2368\n",
      "2024-11-14 04:41:35,613 - root - INFO - KG Training: Epoch 0001 Iter 1303 / 3136 | Time 0.1s | Iter Loss 0.1138 | Iter Mean Loss 0.2367\n",
      "2024-11-14 04:41:35,678 - root - INFO - KG Training: Epoch 0001 Iter 1304 / 3136 | Time 0.1s | Iter Loss 0.0945 | Iter Mean Loss 0.2366\n",
      "2024-11-14 04:41:35,743 - root - INFO - KG Training: Epoch 0001 Iter 1305 / 3136 | Time 0.1s | Iter Loss 0.0990 | Iter Mean Loss 0.2365\n",
      "2024-11-14 04:41:35,804 - root - INFO - KG Training: Epoch 0001 Iter 1306 / 3136 | Time 0.1s | Iter Loss 0.1044 | Iter Mean Loss 0.2364\n",
      "2024-11-14 04:41:35,865 - root - INFO - KG Training: Epoch 0001 Iter 1307 / 3136 | Time 0.1s | Iter Loss 0.1120 | Iter Mean Loss 0.2363\n",
      "2024-11-14 04:41:35,927 - root - INFO - KG Training: Epoch 0001 Iter 1308 / 3136 | Time 0.1s | Iter Loss 0.0982 | Iter Mean Loss 0.2362\n",
      "2024-11-14 04:41:35,995 - root - INFO - KG Training: Epoch 0001 Iter 1309 / 3136 | Time 0.1s | Iter Loss 0.1005 | Iter Mean Loss 0.2361\n",
      "2024-11-14 04:41:36,057 - root - INFO - KG Training: Epoch 0001 Iter 1310 / 3136 | Time 0.1s | Iter Loss 0.1054 | Iter Mean Loss 0.2360\n",
      "2024-11-14 04:41:36,124 - root - INFO - KG Training: Epoch 0001 Iter 1311 / 3136 | Time 0.1s | Iter Loss 0.1076 | Iter Mean Loss 0.2359\n",
      "2024-11-14 04:41:36,185 - root - INFO - KG Training: Epoch 0001 Iter 1312 / 3136 | Time 0.1s | Iter Loss 0.1071 | Iter Mean Loss 0.2358\n",
      "2024-11-14 04:41:36,247 - root - INFO - KG Training: Epoch 0001 Iter 1313 / 3136 | Time 0.1s | Iter Loss 0.0960 | Iter Mean Loss 0.2357\n",
      "2024-11-14 04:41:36,311 - root - INFO - KG Training: Epoch 0001 Iter 1314 / 3136 | Time 0.1s | Iter Loss 0.0965 | Iter Mean Loss 0.2355\n",
      "2024-11-14 04:41:36,375 - root - INFO - KG Training: Epoch 0001 Iter 1315 / 3136 | Time 0.1s | Iter Loss 0.1116 | Iter Mean Loss 0.2355\n",
      "2024-11-14 04:41:36,437 - root - INFO - KG Training: Epoch 0001 Iter 1316 / 3136 | Time 0.1s | Iter Loss 0.0945 | Iter Mean Loss 0.2353\n",
      "2024-11-14 04:41:36,499 - root - INFO - KG Training: Epoch 0001 Iter 1317 / 3136 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.2352\n",
      "2024-11-14 04:41:36,558 - root - INFO - KG Training: Epoch 0001 Iter 1318 / 3136 | Time 0.1s | Iter Loss 0.1038 | Iter Mean Loss 0.2351\n",
      "2024-11-14 04:41:36,617 - root - INFO - KG Training: Epoch 0001 Iter 1319 / 3136 | Time 0.1s | Iter Loss 0.1135 | Iter Mean Loss 0.2351\n",
      "2024-11-14 04:41:36,736 - root - INFO - KG Training: Epoch 0001 Iter 1320 / 3136 | Time 0.1s | Iter Loss 0.1108 | Iter Mean Loss 0.2350\n",
      "2024-11-14 04:41:36,795 - root - INFO - KG Training: Epoch 0001 Iter 1321 / 3136 | Time 0.1s | Iter Loss 0.0905 | Iter Mean Loss 0.2348\n",
      "2024-11-14 04:41:36,857 - root - INFO - KG Training: Epoch 0001 Iter 1322 / 3136 | Time 0.1s | Iter Loss 0.1133 | Iter Mean Loss 0.2348\n",
      "2024-11-14 04:41:36,921 - root - INFO - KG Training: Epoch 0001 Iter 1323 / 3136 | Time 0.1s | Iter Loss 0.0958 | Iter Mean Loss 0.2347\n",
      "2024-11-14 04:41:36,982 - root - INFO - KG Training: Epoch 0001 Iter 1324 / 3136 | Time 0.1s | Iter Loss 0.1047 | Iter Mean Loss 0.2346\n",
      "2024-11-14 04:41:37,046 - root - INFO - KG Training: Epoch 0001 Iter 1325 / 3136 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.2345\n",
      "2024-11-14 04:41:37,108 - root - INFO - KG Training: Epoch 0001 Iter 1326 / 3136 | Time 0.1s | Iter Loss 0.1000 | Iter Mean Loss 0.2344\n",
      "2024-11-14 04:41:37,168 - root - INFO - KG Training: Epoch 0001 Iter 1327 / 3136 | Time 0.1s | Iter Loss 0.1059 | Iter Mean Loss 0.2343\n",
      "2024-11-14 04:41:37,231 - root - INFO - KG Training: Epoch 0001 Iter 1328 / 3136 | Time 0.1s | Iter Loss 0.0988 | Iter Mean Loss 0.2342\n",
      "2024-11-14 04:41:37,396 - root - INFO - KG Training: Epoch 0001 Iter 1329 / 3136 | Time 0.2s | Iter Loss 0.1020 | Iter Mean Loss 0.2341\n",
      "2024-11-14 04:41:37,458 - root - INFO - KG Training: Epoch 0001 Iter 1330 / 3136 | Time 0.1s | Iter Loss 0.1029 | Iter Mean Loss 0.2340\n",
      "2024-11-14 04:41:37,519 - root - INFO - KG Training: Epoch 0001 Iter 1331 / 3136 | Time 0.1s | Iter Loss 0.1102 | Iter Mean Loss 0.2339\n",
      "2024-11-14 04:41:37,634 - root - INFO - KG Training: Epoch 0001 Iter 1332 / 3136 | Time 0.1s | Iter Loss 0.0940 | Iter Mean Loss 0.2338\n",
      "2024-11-14 04:41:37,696 - root - INFO - KG Training: Epoch 0001 Iter 1333 / 3136 | Time 0.1s | Iter Loss 0.1031 | Iter Mean Loss 0.2337\n",
      "2024-11-14 04:41:37,758 - root - INFO - KG Training: Epoch 0001 Iter 1334 / 3136 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.2336\n",
      "2024-11-14 04:41:37,819 - root - INFO - KG Training: Epoch 0001 Iter 1335 / 3136 | Time 0.1s | Iter Loss 0.0959 | Iter Mean Loss 0.2335\n",
      "2024-11-14 04:41:37,880 - root - INFO - KG Training: Epoch 0001 Iter 1336 / 3136 | Time 0.1s | Iter Loss 0.0986 | Iter Mean Loss 0.2334\n",
      "2024-11-14 04:41:37,942 - root - INFO - KG Training: Epoch 0001 Iter 1337 / 3136 | Time 0.1s | Iter Loss 0.1017 | Iter Mean Loss 0.2333\n",
      "2024-11-14 04:41:38,002 - root - INFO - KG Training: Epoch 0001 Iter 1338 / 3136 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.2332\n",
      "2024-11-14 04:41:38,064 - root - INFO - KG Training: Epoch 0001 Iter 1339 / 3136 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.2331\n",
      "2024-11-14 04:41:38,126 - root - INFO - KG Training: Epoch 0001 Iter 1340 / 3136 | Time 0.1s | Iter Loss 0.0883 | Iter Mean Loss 0.2330\n",
      "2024-11-14 04:41:38,191 - root - INFO - KG Training: Epoch 0001 Iter 1341 / 3136 | Time 0.1s | Iter Loss 0.1012 | Iter Mean Loss 0.2329\n",
      "2024-11-14 04:41:38,252 - root - INFO - KG Training: Epoch 0001 Iter 1342 / 3136 | Time 0.1s | Iter Loss 0.0929 | Iter Mean Loss 0.2327\n",
      "2024-11-14 04:41:38,313 - root - INFO - KG Training: Epoch 0001 Iter 1343 / 3136 | Time 0.1s | Iter Loss 0.1040 | Iter Mean Loss 0.2327\n",
      "2024-11-14 04:41:38,375 - root - INFO - KG Training: Epoch 0001 Iter 1344 / 3136 | Time 0.1s | Iter Loss 0.1026 | Iter Mean Loss 0.2326\n",
      "2024-11-14 04:41:38,443 - root - INFO - KG Training: Epoch 0001 Iter 1345 / 3136 | Time 0.1s | Iter Loss 0.1066 | Iter Mean Loss 0.2325\n",
      "2024-11-14 04:41:38,510 - root - INFO - KG Training: Epoch 0001 Iter 1346 / 3136 | Time 0.1s | Iter Loss 0.1067 | Iter Mean Loss 0.2324\n",
      "2024-11-14 04:41:38,574 - root - INFO - KG Training: Epoch 0001 Iter 1347 / 3136 | Time 0.1s | Iter Loss 0.0968 | Iter Mean Loss 0.2323\n",
      "2024-11-14 04:41:38,633 - root - INFO - KG Training: Epoch 0001 Iter 1348 / 3136 | Time 0.1s | Iter Loss 0.0932 | Iter Mean Loss 0.2322\n",
      "2024-11-14 04:41:38,694 - root - INFO - KG Training: Epoch 0001 Iter 1349 / 3136 | Time 0.1s | Iter Loss 0.0953 | Iter Mean Loss 0.2321\n",
      "2024-11-14 04:41:38,755 - root - INFO - KG Training: Epoch 0001 Iter 1350 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.2320\n",
      "2024-11-14 04:41:38,816 - root - INFO - KG Training: Epoch 0001 Iter 1351 / 3136 | Time 0.1s | Iter Loss 0.1064 | Iter Mean Loss 0.2319\n",
      "2024-11-14 04:41:38,876 - root - INFO - KG Training: Epoch 0001 Iter 1352 / 3136 | Time 0.1s | Iter Loss 0.0914 | Iter Mean Loss 0.2318\n",
      "2024-11-14 04:41:38,940 - root - INFO - KG Training: Epoch 0001 Iter 1353 / 3136 | Time 0.1s | Iter Loss 0.0936 | Iter Mean Loss 0.2317\n",
      "2024-11-14 04:41:39,003 - root - INFO - KG Training: Epoch 0001 Iter 1354 / 3136 | Time 0.1s | Iter Loss 0.0961 | Iter Mean Loss 0.2316\n",
      "2024-11-14 04:41:39,069 - root - INFO - KG Training: Epoch 0001 Iter 1355 / 3136 | Time 0.1s | Iter Loss 0.0983 | Iter Mean Loss 0.2315\n",
      "2024-11-14 04:41:39,136 - root - INFO - KG Training: Epoch 0001 Iter 1356 / 3136 | Time 0.1s | Iter Loss 0.1080 | Iter Mean Loss 0.2314\n",
      "2024-11-14 04:41:39,253 - root - INFO - KG Training: Epoch 0001 Iter 1357 / 3136 | Time 0.1s | Iter Loss 0.1019 | Iter Mean Loss 0.2313\n",
      "2024-11-14 04:41:39,312 - root - INFO - KG Training: Epoch 0001 Iter 1358 / 3136 | Time 0.1s | Iter Loss 0.0936 | Iter Mean Loss 0.2312\n",
      "2024-11-14 04:41:39,372 - root - INFO - KG Training: Epoch 0001 Iter 1359 / 3136 | Time 0.1s | Iter Loss 0.1003 | Iter Mean Loss 0.2311\n",
      "2024-11-14 04:41:39,437 - root - INFO - KG Training: Epoch 0001 Iter 1360 / 3136 | Time 0.1s | Iter Loss 0.1031 | Iter Mean Loss 0.2310\n",
      "2024-11-14 04:41:39,499 - root - INFO - KG Training: Epoch 0001 Iter 1361 / 3136 | Time 0.1s | Iter Loss 0.1060 | Iter Mean Loss 0.2309\n",
      "2024-11-14 04:41:39,564 - root - INFO - KG Training: Epoch 0001 Iter 1362 / 3136 | Time 0.1s | Iter Loss 0.1075 | Iter Mean Loss 0.2308\n",
      "2024-11-14 04:41:39,628 - root - INFO - KG Training: Epoch 0001 Iter 1363 / 3136 | Time 0.1s | Iter Loss 0.0905 | Iter Mean Loss 0.2307\n",
      "2024-11-14 04:41:39,690 - root - INFO - KG Training: Epoch 0001 Iter 1364 / 3136 | Time 0.1s | Iter Loss 0.1106 | Iter Mean Loss 0.2306\n",
      "2024-11-14 04:41:39,753 - root - INFO - KG Training: Epoch 0001 Iter 1365 / 3136 | Time 0.1s | Iter Loss 0.0998 | Iter Mean Loss 0.2305\n",
      "2024-11-14 04:41:39,816 - root - INFO - KG Training: Epoch 0001 Iter 1366 / 3136 | Time 0.1s | Iter Loss 0.0970 | Iter Mean Loss 0.2304\n",
      "2024-11-14 04:41:39,876 - root - INFO - KG Training: Epoch 0001 Iter 1367 / 3136 | Time 0.1s | Iter Loss 0.0979 | Iter Mean Loss 0.2303\n",
      "2024-11-14 04:41:39,943 - root - INFO - KG Training: Epoch 0001 Iter 1368 / 3136 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.2302\n",
      "2024-11-14 04:41:40,010 - root - INFO - KG Training: Epoch 0001 Iter 1369 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2301\n",
      "2024-11-14 04:41:40,075 - root - INFO - KG Training: Epoch 0001 Iter 1370 / 3136 | Time 0.1s | Iter Loss 0.1084 | Iter Mean Loss 0.2300\n",
      "2024-11-14 04:41:40,137 - root - INFO - KG Training: Epoch 0001 Iter 1371 / 3136 | Time 0.1s | Iter Loss 0.0902 | Iter Mean Loss 0.2299\n",
      "2024-11-14 04:41:40,196 - root - INFO - KG Training: Epoch 0001 Iter 1372 / 3136 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.2298\n",
      "2024-11-14 04:41:40,258 - root - INFO - KG Training: Epoch 0001 Iter 1373 / 3136 | Time 0.1s | Iter Loss 0.0985 | Iter Mean Loss 0.2297\n",
      "2024-11-14 04:41:40,319 - root - INFO - KG Training: Epoch 0001 Iter 1374 / 3136 | Time 0.1s | Iter Loss 0.1003 | Iter Mean Loss 0.2297\n",
      "2024-11-14 04:41:40,377 - root - INFO - KG Training: Epoch 0001 Iter 1375 / 3136 | Time 0.1s | Iter Loss 0.0946 | Iter Mean Loss 0.2296\n",
      "2024-11-14 04:41:40,438 - root - INFO - KG Training: Epoch 0001 Iter 1376 / 3136 | Time 0.1s | Iter Loss 0.1051 | Iter Mean Loss 0.2295\n",
      "2024-11-14 04:41:40,496 - root - INFO - KG Training: Epoch 0001 Iter 1377 / 3136 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.2294\n",
      "2024-11-14 04:41:40,557 - root - INFO - KG Training: Epoch 0001 Iter 1378 / 3136 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.2293\n",
      "2024-11-14 04:41:40,622 - root - INFO - KG Training: Epoch 0001 Iter 1379 / 3136 | Time 0.1s | Iter Loss 0.0994 | Iter Mean Loss 0.2292\n",
      "2024-11-14 04:41:40,682 - root - INFO - KG Training: Epoch 0001 Iter 1380 / 3136 | Time 0.1s | Iter Loss 0.1056 | Iter Mean Loss 0.2291\n",
      "2024-11-14 04:41:40,851 - root - INFO - KG Training: Epoch 0001 Iter 1381 / 3136 | Time 0.2s | Iter Loss 0.1043 | Iter Mean Loss 0.2290\n",
      "2024-11-14 04:41:40,910 - root - INFO - KG Training: Epoch 0001 Iter 1382 / 3136 | Time 0.1s | Iter Loss 0.0967 | Iter Mean Loss 0.2289\n",
      "2024-11-14 04:41:40,972 - root - INFO - KG Training: Epoch 0001 Iter 1383 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2288\n",
      "2024-11-14 04:41:41,032 - root - INFO - KG Training: Epoch 0001 Iter 1384 / 3136 | Time 0.1s | Iter Loss 0.1045 | Iter Mean Loss 0.2287\n",
      "2024-11-14 04:41:41,093 - root - INFO - KG Training: Epoch 0001 Iter 1385 / 3136 | Time 0.1s | Iter Loss 0.0869 | Iter Mean Loss 0.2286\n",
      "2024-11-14 04:41:41,155 - root - INFO - KG Training: Epoch 0001 Iter 1386 / 3136 | Time 0.1s | Iter Loss 0.0974 | Iter Mean Loss 0.2285\n",
      "2024-11-14 04:41:41,215 - root - INFO - KG Training: Epoch 0001 Iter 1387 / 3136 | Time 0.1s | Iter Loss 0.1122 | Iter Mean Loss 0.2284\n",
      "2024-11-14 04:41:41,278 - root - INFO - KG Training: Epoch 0001 Iter 1388 / 3136 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.2283\n",
      "2024-11-14 04:41:41,398 - root - INFO - KG Training: Epoch 0001 Iter 1389 / 3136 | Time 0.1s | Iter Loss 0.0967 | Iter Mean Loss 0.2283\n",
      "2024-11-14 04:41:41,466 - root - INFO - KG Training: Epoch 0001 Iter 1390 / 3136 | Time 0.1s | Iter Loss 0.0974 | Iter Mean Loss 0.2282\n",
      "2024-11-14 04:41:41,528 - root - INFO - KG Training: Epoch 0001 Iter 1391 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2281\n",
      "2024-11-14 04:41:41,588 - root - INFO - KG Training: Epoch 0001 Iter 1392 / 3136 | Time 0.1s | Iter Loss 0.0887 | Iter Mean Loss 0.2280\n",
      "2024-11-14 04:41:41,651 - root - INFO - KG Training: Epoch 0001 Iter 1393 / 3136 | Time 0.1s | Iter Loss 0.0876 | Iter Mean Loss 0.2279\n",
      "2024-11-14 04:41:41,713 - root - INFO - KG Training: Epoch 0001 Iter 1394 / 3136 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.2278\n",
      "2024-11-14 04:41:41,775 - root - INFO - KG Training: Epoch 0001 Iter 1395 / 3136 | Time 0.1s | Iter Loss 0.1023 | Iter Mean Loss 0.2277\n",
      "2024-11-14 04:41:41,839 - root - INFO - KG Training: Epoch 0001 Iter 1396 / 3136 | Time 0.1s | Iter Loss 0.0953 | Iter Mean Loss 0.2276\n",
      "2024-11-14 04:41:41,903 - root - INFO - KG Training: Epoch 0001 Iter 1397 / 3136 | Time 0.1s | Iter Loss 0.1041 | Iter Mean Loss 0.2275\n",
      "2024-11-14 04:41:41,963 - root - INFO - KG Training: Epoch 0001 Iter 1398 / 3136 | Time 0.1s | Iter Loss 0.1070 | Iter Mean Loss 0.2274\n",
      "2024-11-14 04:41:42,025 - root - INFO - KG Training: Epoch 0001 Iter 1399 / 3136 | Time 0.1s | Iter Loss 0.0968 | Iter Mean Loss 0.2273\n",
      "2024-11-14 04:41:42,088 - root - INFO - KG Training: Epoch 0001 Iter 1400 / 3136 | Time 0.1s | Iter Loss 0.0924 | Iter Mean Loss 0.2272\n",
      "2024-11-14 04:41:42,151 - root - INFO - KG Training: Epoch 0001 Iter 1401 / 3136 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.2271\n",
      "2024-11-14 04:41:42,214 - root - INFO - KG Training: Epoch 0001 Iter 1402 / 3136 | Time 0.1s | Iter Loss 0.0908 | Iter Mean Loss 0.2270\n",
      "2024-11-14 04:41:42,274 - root - INFO - KG Training: Epoch 0001 Iter 1403 / 3136 | Time 0.1s | Iter Loss 0.0969 | Iter Mean Loss 0.2269\n",
      "2024-11-14 04:41:42,335 - root - INFO - KG Training: Epoch 0001 Iter 1404 / 3136 | Time 0.1s | Iter Loss 0.0859 | Iter Mean Loss 0.2268\n",
      "2024-11-14 04:41:42,513 - root - INFO - KG Training: Epoch 0001 Iter 1405 / 3136 | Time 0.2s | Iter Loss 0.1110 | Iter Mean Loss 0.2268\n",
      "2024-11-14 04:41:42,574 - root - INFO - KG Training: Epoch 0001 Iter 1406 / 3136 | Time 0.1s | Iter Loss 0.1040 | Iter Mean Loss 0.2267\n",
      "2024-11-14 04:41:42,636 - root - INFO - KG Training: Epoch 0001 Iter 1407 / 3136 | Time 0.1s | Iter Loss 0.0908 | Iter Mean Loss 0.2266\n",
      "2024-11-14 04:41:42,698 - root - INFO - KG Training: Epoch 0001 Iter 1408 / 3136 | Time 0.1s | Iter Loss 0.0998 | Iter Mean Loss 0.2265\n",
      "2024-11-14 04:41:42,759 - root - INFO - KG Training: Epoch 0001 Iter 1409 / 3136 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.2264\n",
      "2024-11-14 04:41:42,821 - root - INFO - KG Training: Epoch 0001 Iter 1410 / 3136 | Time 0.1s | Iter Loss 0.1014 | Iter Mean Loss 0.2263\n",
      "2024-11-14 04:41:42,882 - root - INFO - KG Training: Epoch 0001 Iter 1411 / 3136 | Time 0.1s | Iter Loss 0.1027 | Iter Mean Loss 0.2262\n",
      "2024-11-14 04:41:42,942 - root - INFO - KG Training: Epoch 0001 Iter 1412 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.2261\n",
      "2024-11-14 04:41:43,002 - root - INFO - KG Training: Epoch 0001 Iter 1413 / 3136 | Time 0.1s | Iter Loss 0.0945 | Iter Mean Loss 0.2260\n",
      "2024-11-14 04:41:43,065 - root - INFO - KG Training: Epoch 0001 Iter 1414 / 3136 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.2259\n",
      "2024-11-14 04:41:43,127 - root - INFO - KG Training: Epoch 0001 Iter 1415 / 3136 | Time 0.1s | Iter Loss 0.0970 | Iter Mean Loss 0.2258\n",
      "2024-11-14 04:41:43,284 - root - INFO - KG Training: Epoch 0001 Iter 1416 / 3136 | Time 0.2s | Iter Loss 0.0904 | Iter Mean Loss 0.2258\n",
      "2024-11-14 04:41:43,350 - root - INFO - KG Training: Epoch 0001 Iter 1417 / 3136 | Time 0.1s | Iter Loss 0.1001 | Iter Mean Loss 0.2257\n",
      "2024-11-14 04:41:43,479 - root - INFO - KG Training: Epoch 0001 Iter 1418 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2256\n",
      "2024-11-14 04:41:43,548 - root - INFO - KG Training: Epoch 0001 Iter 1419 / 3136 | Time 0.1s | Iter Loss 0.0925 | Iter Mean Loss 0.2255\n",
      "2024-11-14 04:41:43,610 - root - INFO - KG Training: Epoch 0001 Iter 1420 / 3136 | Time 0.1s | Iter Loss 0.0945 | Iter Mean Loss 0.2254\n",
      "2024-11-14 04:41:43,670 - root - INFO - KG Training: Epoch 0001 Iter 1421 / 3136 | Time 0.1s | Iter Loss 0.0986 | Iter Mean Loss 0.2253\n",
      "2024-11-14 04:41:43,728 - root - INFO - KG Training: Epoch 0001 Iter 1422 / 3136 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.2252\n",
      "2024-11-14 04:41:43,789 - root - INFO - KG Training: Epoch 0001 Iter 1423 / 3136 | Time 0.1s | Iter Loss 0.0909 | Iter Mean Loss 0.2251\n",
      "2024-11-14 04:41:43,849 - root - INFO - KG Training: Epoch 0001 Iter 1424 / 3136 | Time 0.1s | Iter Loss 0.0870 | Iter Mean Loss 0.2250\n",
      "2024-11-14 04:41:43,925 - root - INFO - KG Training: Epoch 0001 Iter 1425 / 3136 | Time 0.1s | Iter Loss 0.0865 | Iter Mean Loss 0.2249\n",
      "2024-11-14 04:41:44,002 - root - INFO - KG Training: Epoch 0001 Iter 1426 / 3136 | Time 0.1s | Iter Loss 0.1020 | Iter Mean Loss 0.2248\n",
      "2024-11-14 04:41:44,060 - root - INFO - KG Training: Epoch 0001 Iter 1427 / 3136 | Time 0.1s | Iter Loss 0.0933 | Iter Mean Loss 0.2247\n",
      "2024-11-14 04:41:44,139 - root - INFO - KG Training: Epoch 0001 Iter 1428 / 3136 | Time 0.1s | Iter Loss 0.0963 | Iter Mean Loss 0.2247\n",
      "2024-11-14 04:41:44,199 - root - INFO - KG Training: Epoch 0001 Iter 1429 / 3136 | Time 0.1s | Iter Loss 0.0988 | Iter Mean Loss 0.2246\n",
      "2024-11-14 04:41:44,272 - root - INFO - KG Training: Epoch 0001 Iter 1430 / 3136 | Time 0.1s | Iter Loss 0.1014 | Iter Mean Loss 0.2245\n",
      "2024-11-14 04:41:44,330 - root - INFO - KG Training: Epoch 0001 Iter 1431 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.2244\n",
      "2024-11-14 04:41:44,392 - root - INFO - KG Training: Epoch 0001 Iter 1432 / 3136 | Time 0.1s | Iter Loss 0.0998 | Iter Mean Loss 0.2243\n",
      "2024-11-14 04:41:44,457 - root - INFO - KG Training: Epoch 0001 Iter 1433 / 3136 | Time 0.1s | Iter Loss 0.0989 | Iter Mean Loss 0.2242\n",
      "2024-11-14 04:41:44,523 - root - INFO - KG Training: Epoch 0001 Iter 1434 / 3136 | Time 0.1s | Iter Loss 0.0928 | Iter Mean Loss 0.2241\n",
      "2024-11-14 04:41:44,597 - root - INFO - KG Training: Epoch 0001 Iter 1435 / 3136 | Time 0.1s | Iter Loss 0.0855 | Iter Mean Loss 0.2240\n",
      "2024-11-14 04:41:44,673 - root - INFO - KG Training: Epoch 0001 Iter 1436 / 3136 | Time 0.1s | Iter Loss 0.0990 | Iter Mean Loss 0.2239\n",
      "2024-11-14 04:41:44,735 - root - INFO - KG Training: Epoch 0001 Iter 1437 / 3136 | Time 0.1s | Iter Loss 0.0881 | Iter Mean Loss 0.2238\n",
      "2024-11-14 04:41:44,794 - root - INFO - KG Training: Epoch 0001 Iter 1438 / 3136 | Time 0.1s | Iter Loss 0.1034 | Iter Mean Loss 0.2238\n",
      "2024-11-14 04:41:44,856 - root - INFO - KG Training: Epoch 0001 Iter 1439 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.2237\n",
      "2024-11-14 04:41:44,916 - root - INFO - KG Training: Epoch 0001 Iter 1440 / 3136 | Time 0.1s | Iter Loss 0.0897 | Iter Mean Loss 0.2236\n",
      "2024-11-14 04:41:44,976 - root - INFO - KG Training: Epoch 0001 Iter 1441 / 3136 | Time 0.1s | Iter Loss 0.1011 | Iter Mean Loss 0.2235\n",
      "2024-11-14 04:41:45,039 - root - INFO - KG Training: Epoch 0001 Iter 1442 / 3136 | Time 0.1s | Iter Loss 0.0951 | Iter Mean Loss 0.2234\n",
      "2024-11-14 04:41:45,099 - root - INFO - KG Training: Epoch 0001 Iter 1443 / 3136 | Time 0.1s | Iter Loss 0.0945 | Iter Mean Loss 0.2233\n",
      "2024-11-14 04:41:45,159 - root - INFO - KG Training: Epoch 0001 Iter 1444 / 3136 | Time 0.1s | Iter Loss 0.0892 | Iter Mean Loss 0.2232\n",
      "2024-11-14 04:41:45,270 - root - INFO - KG Training: Epoch 0001 Iter 1445 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.2231\n",
      "2024-11-14 04:41:45,331 - root - INFO - KG Training: Epoch 0001 Iter 1446 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2230\n",
      "2024-11-14 04:41:45,392 - root - INFO - KG Training: Epoch 0001 Iter 1447 / 3136 | Time 0.1s | Iter Loss 0.0944 | Iter Mean Loss 0.2229\n",
      "2024-11-14 04:41:45,459 - root - INFO - KG Training: Epoch 0001 Iter 1448 / 3136 | Time 0.1s | Iter Loss 0.0909 | Iter Mean Loss 0.2229\n",
      "2024-11-14 04:41:45,583 - root - INFO - KG Training: Epoch 0001 Iter 1449 / 3136 | Time 0.1s | Iter Loss 0.0964 | Iter Mean Loss 0.2228\n",
      "2024-11-14 04:41:45,644 - root - INFO - KG Training: Epoch 0001 Iter 1450 / 3136 | Time 0.1s | Iter Loss 0.0894 | Iter Mean Loss 0.2227\n",
      "2024-11-14 04:41:45,711 - root - INFO - KG Training: Epoch 0001 Iter 1451 / 3136 | Time 0.1s | Iter Loss 0.1021 | Iter Mean Loss 0.2226\n",
      "2024-11-14 04:41:45,768 - root - INFO - KG Training: Epoch 0001 Iter 1452 / 3136 | Time 0.1s | Iter Loss 0.0889 | Iter Mean Loss 0.2225\n",
      "2024-11-14 04:41:45,829 - root - INFO - KG Training: Epoch 0001 Iter 1453 / 3136 | Time 0.1s | Iter Loss 0.0953 | Iter Mean Loss 0.2224\n",
      "2024-11-14 04:41:45,889 - root - INFO - KG Training: Epoch 0001 Iter 1454 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2223\n",
      "2024-11-14 04:41:45,955 - root - INFO - KG Training: Epoch 0001 Iter 1455 / 3136 | Time 0.1s | Iter Loss 0.0969 | Iter Mean Loss 0.2222\n",
      "2024-11-14 04:41:46,015 - root - INFO - KG Training: Epoch 0001 Iter 1456 / 3136 | Time 0.1s | Iter Loss 0.0989 | Iter Mean Loss 0.2222\n",
      "2024-11-14 04:41:46,077 - root - INFO - KG Training: Epoch 0001 Iter 1457 / 3136 | Time 0.1s | Iter Loss 0.1054 | Iter Mean Loss 0.2221\n",
      "2024-11-14 04:41:46,140 - root - INFO - KG Training: Epoch 0001 Iter 1458 / 3136 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.2220\n",
      "2024-11-14 04:41:46,202 - root - INFO - KG Training: Epoch 0001 Iter 1459 / 3136 | Time 0.1s | Iter Loss 0.0891 | Iter Mean Loss 0.2219\n",
      "2024-11-14 04:41:46,263 - root - INFO - KG Training: Epoch 0001 Iter 1460 / 3136 | Time 0.1s | Iter Loss 0.0958 | Iter Mean Loss 0.2218\n",
      "2024-11-14 04:41:46,324 - root - INFO - KG Training: Epoch 0001 Iter 1461 / 3136 | Time 0.1s | Iter Loss 0.0974 | Iter Mean Loss 0.2217\n",
      "2024-11-14 04:41:46,387 - root - INFO - KG Training: Epoch 0001 Iter 1462 / 3136 | Time 0.1s | Iter Loss 0.0960 | Iter Mean Loss 0.2216\n",
      "2024-11-14 04:41:46,446 - root - INFO - KG Training: Epoch 0001 Iter 1463 / 3136 | Time 0.1s | Iter Loss 0.0970 | Iter Mean Loss 0.2216\n",
      "2024-11-14 04:41:46,505 - root - INFO - KG Training: Epoch 0001 Iter 1464 / 3136 | Time 0.1s | Iter Loss 0.0904 | Iter Mean Loss 0.2215\n",
      "2024-11-14 04:41:46,572 - root - INFO - KG Training: Epoch 0001 Iter 1465 / 3136 | Time 0.1s | Iter Loss 0.0892 | Iter Mean Loss 0.2214\n",
      "2024-11-14 04:41:46,631 - root - INFO - KG Training: Epoch 0001 Iter 1466 / 3136 | Time 0.1s | Iter Loss 0.0938 | Iter Mean Loss 0.2213\n",
      "2024-11-14 04:41:46,689 - root - INFO - KG Training: Epoch 0001 Iter 1467 / 3136 | Time 0.1s | Iter Loss 0.0872 | Iter Mean Loss 0.2212\n",
      "2024-11-14 04:41:46,752 - root - INFO - KG Training: Epoch 0001 Iter 1468 / 3136 | Time 0.1s | Iter Loss 0.0979 | Iter Mean Loss 0.2211\n",
      "2024-11-14 04:41:46,811 - root - INFO - KG Training: Epoch 0001 Iter 1469 / 3136 | Time 0.1s | Iter Loss 0.0900 | Iter Mean Loss 0.2210\n",
      "2024-11-14 04:41:46,871 - root - INFO - KG Training: Epoch 0001 Iter 1470 / 3136 | Time 0.1s | Iter Loss 0.0847 | Iter Mean Loss 0.2209\n",
      "2024-11-14 04:41:46,931 - root - INFO - KG Training: Epoch 0001 Iter 1471 / 3136 | Time 0.1s | Iter Loss 0.0985 | Iter Mean Loss 0.2208\n",
      "2024-11-14 04:41:46,991 - root - INFO - KG Training: Epoch 0001 Iter 1472 / 3136 | Time 0.1s | Iter Loss 0.0862 | Iter Mean Loss 0.2208\n",
      "2024-11-14 04:41:47,051 - root - INFO - KG Training: Epoch 0001 Iter 1473 / 3136 | Time 0.1s | Iter Loss 0.0927 | Iter Mean Loss 0.2207\n",
      "2024-11-14 04:41:47,116 - root - INFO - KG Training: Epoch 0001 Iter 1474 / 3136 | Time 0.1s | Iter Loss 0.1008 | Iter Mean Loss 0.2206\n",
      "2024-11-14 04:41:47,176 - root - INFO - KG Training: Epoch 0001 Iter 1475 / 3136 | Time 0.1s | Iter Loss 0.0947 | Iter Mean Loss 0.2205\n",
      "2024-11-14 04:41:47,237 - root - INFO - KG Training: Epoch 0001 Iter 1476 / 3136 | Time 0.1s | Iter Loss 0.0918 | Iter Mean Loss 0.2204\n",
      "2024-11-14 04:41:47,297 - root - INFO - KG Training: Epoch 0001 Iter 1477 / 3136 | Time 0.1s | Iter Loss 0.0921 | Iter Mean Loss 0.2203\n",
      "2024-11-14 04:41:47,407 - root - INFO - KG Training: Epoch 0001 Iter 1478 / 3136 | Time 0.1s | Iter Loss 0.0903 | Iter Mean Loss 0.2202\n",
      "2024-11-14 04:41:47,515 - root - INFO - KG Training: Epoch 0001 Iter 1479 / 3136 | Time 0.1s | Iter Loss 0.0869 | Iter Mean Loss 0.2202\n",
      "2024-11-14 04:41:47,573 - root - INFO - KG Training: Epoch 0001 Iter 1480 / 3136 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.2201\n",
      "2024-11-14 04:41:47,633 - root - INFO - KG Training: Epoch 0001 Iter 1481 / 3136 | Time 0.1s | Iter Loss 0.0891 | Iter Mean Loss 0.2200\n",
      "2024-11-14 04:41:47,693 - root - INFO - KG Training: Epoch 0001 Iter 1482 / 3136 | Time 0.1s | Iter Loss 0.0921 | Iter Mean Loss 0.2199\n",
      "2024-11-14 04:41:47,757 - root - INFO - KG Training: Epoch 0001 Iter 1483 / 3136 | Time 0.1s | Iter Loss 0.0917 | Iter Mean Loss 0.2198\n",
      "2024-11-14 04:41:47,817 - root - INFO - KG Training: Epoch 0001 Iter 1484 / 3136 | Time 0.1s | Iter Loss 0.0910 | Iter Mean Loss 0.2197\n",
      "2024-11-14 04:41:47,878 - root - INFO - KG Training: Epoch 0001 Iter 1485 / 3136 | Time 0.1s | Iter Loss 0.0905 | Iter Mean Loss 0.2196\n",
      "2024-11-14 04:41:47,938 - root - INFO - KG Training: Epoch 0001 Iter 1486 / 3136 | Time 0.1s | Iter Loss 0.0976 | Iter Mean Loss 0.2196\n",
      "2024-11-14 04:41:47,999 - root - INFO - KG Training: Epoch 0001 Iter 1487 / 3136 | Time 0.1s | Iter Loss 0.0927 | Iter Mean Loss 0.2195\n",
      "2024-11-14 04:41:48,059 - root - INFO - KG Training: Epoch 0001 Iter 1488 / 3136 | Time 0.1s | Iter Loss 0.0837 | Iter Mean Loss 0.2194\n",
      "2024-11-14 04:41:48,122 - root - INFO - KG Training: Epoch 0001 Iter 1489 / 3136 | Time 0.1s | Iter Loss 0.0920 | Iter Mean Loss 0.2193\n",
      "2024-11-14 04:41:48,182 - root - INFO - KG Training: Epoch 0001 Iter 1490 / 3136 | Time 0.1s | Iter Loss 0.0911 | Iter Mean Loss 0.2192\n",
      "2024-11-14 04:41:48,242 - root - INFO - KG Training: Epoch 0001 Iter 1491 / 3136 | Time 0.1s | Iter Loss 0.0940 | Iter Mean Loss 0.2191\n",
      "2024-11-14 04:41:48,397 - root - INFO - KG Training: Epoch 0001 Iter 1492 / 3136 | Time 0.2s | Iter Loss 0.1011 | Iter Mean Loss 0.2190\n",
      "2024-11-14 04:41:48,457 - root - INFO - KG Training: Epoch 0001 Iter 1493 / 3136 | Time 0.1s | Iter Loss 0.0907 | Iter Mean Loss 0.2190\n",
      "2024-11-14 04:41:48,518 - root - INFO - KG Training: Epoch 0001 Iter 1494 / 3136 | Time 0.1s | Iter Loss 0.0957 | Iter Mean Loss 0.2189\n",
      "2024-11-14 04:41:48,589 - root - INFO - KG Training: Epoch 0001 Iter 1495 / 3136 | Time 0.1s | Iter Loss 0.0897 | Iter Mean Loss 0.2188\n",
      "2024-11-14 04:41:48,651 - root - INFO - KG Training: Epoch 0001 Iter 1496 / 3136 | Time 0.1s | Iter Loss 0.0868 | Iter Mean Loss 0.2187\n",
      "2024-11-14 04:41:48,712 - root - INFO - KG Training: Epoch 0001 Iter 1497 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.2186\n",
      "2024-11-14 04:41:48,771 - root - INFO - KG Training: Epoch 0001 Iter 1498 / 3136 | Time 0.1s | Iter Loss 0.0943 | Iter Mean Loss 0.2185\n",
      "2024-11-14 04:41:48,832 - root - INFO - KG Training: Epoch 0001 Iter 1499 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.2184\n",
      "2024-11-14 04:41:48,899 - root - INFO - KG Training: Epoch 0001 Iter 1500 / 3136 | Time 0.1s | Iter Loss 0.0924 | Iter Mean Loss 0.2183\n",
      "2024-11-14 04:41:48,960 - root - INFO - KG Training: Epoch 0001 Iter 1501 / 3136 | Time 0.1s | Iter Loss 0.0863 | Iter Mean Loss 0.2183\n",
      "2024-11-14 04:41:49,027 - root - INFO - KG Training: Epoch 0001 Iter 1502 / 3136 | Time 0.1s | Iter Loss 0.0890 | Iter Mean Loss 0.2182\n",
      "2024-11-14 04:41:49,093 - root - INFO - KG Training: Epoch 0001 Iter 1503 / 3136 | Time 0.1s | Iter Loss 0.0943 | Iter Mean Loss 0.2181\n",
      "2024-11-14 04:41:49,154 - root - INFO - KG Training: Epoch 0001 Iter 1504 / 3136 | Time 0.1s | Iter Loss 0.0854 | Iter Mean Loss 0.2180\n",
      "2024-11-14 04:41:49,217 - root - INFO - KG Training: Epoch 0001 Iter 1505 / 3136 | Time 0.1s | Iter Loss 0.0882 | Iter Mean Loss 0.2179\n",
      "2024-11-14 04:41:49,283 - root - INFO - KG Training: Epoch 0001 Iter 1506 / 3136 | Time 0.1s | Iter Loss 0.0941 | Iter Mean Loss 0.2178\n",
      "2024-11-14 04:41:49,343 - root - INFO - KG Training: Epoch 0001 Iter 1507 / 3136 | Time 0.1s | Iter Loss 0.0879 | Iter Mean Loss 0.2178\n",
      "2024-11-14 04:41:49,404 - root - INFO - KG Training: Epoch 0001 Iter 1508 / 3136 | Time 0.1s | Iter Loss 0.0946 | Iter Mean Loss 0.2177\n",
      "2024-11-14 04:41:49,518 - root - INFO - KG Training: Epoch 0001 Iter 1509 / 3136 | Time 0.1s | Iter Loss 0.1019 | Iter Mean Loss 0.2176\n",
      "2024-11-14 04:41:49,583 - root - INFO - KG Training: Epoch 0001 Iter 1510 / 3136 | Time 0.1s | Iter Loss 0.0891 | Iter Mean Loss 0.2175\n",
      "2024-11-14 04:41:49,644 - root - INFO - KG Training: Epoch 0001 Iter 1511 / 3136 | Time 0.1s | Iter Loss 0.0924 | Iter Mean Loss 0.2174\n",
      "2024-11-14 04:41:49,701 - root - INFO - KG Training: Epoch 0001 Iter 1512 / 3136 | Time 0.1s | Iter Loss 0.0900 | Iter Mean Loss 0.2173\n",
      "2024-11-14 04:41:49,767 - root - INFO - KG Training: Epoch 0001 Iter 1513 / 3136 | Time 0.1s | Iter Loss 0.0956 | Iter Mean Loss 0.2173\n",
      "2024-11-14 04:41:49,828 - root - INFO - KG Training: Epoch 0001 Iter 1514 / 3136 | Time 0.1s | Iter Loss 0.0900 | Iter Mean Loss 0.2172\n",
      "2024-11-14 04:41:49,891 - root - INFO - KG Training: Epoch 0001 Iter 1515 / 3136 | Time 0.1s | Iter Loss 0.0870 | Iter Mean Loss 0.2171\n",
      "2024-11-14 04:41:49,953 - root - INFO - KG Training: Epoch 0001 Iter 1516 / 3136 | Time 0.1s | Iter Loss 0.0871 | Iter Mean Loss 0.2170\n",
      "2024-11-14 04:41:50,013 - root - INFO - KG Training: Epoch 0001 Iter 1517 / 3136 | Time 0.1s | Iter Loss 0.0939 | Iter Mean Loss 0.2169\n",
      "2024-11-14 04:41:50,073 - root - INFO - KG Training: Epoch 0001 Iter 1518 / 3136 | Time 0.1s | Iter Loss 0.0903 | Iter Mean Loss 0.2168\n",
      "2024-11-14 04:41:50,134 - root - INFO - KG Training: Epoch 0001 Iter 1519 / 3136 | Time 0.1s | Iter Loss 0.0936 | Iter Mean Loss 0.2168\n",
      "2024-11-14 04:41:50,195 - root - INFO - KG Training: Epoch 0001 Iter 1520 / 3136 | Time 0.1s | Iter Loss 0.0850 | Iter Mean Loss 0.2167\n",
      "2024-11-14 04:41:50,262 - root - INFO - KG Training: Epoch 0001 Iter 1521 / 3136 | Time 0.1s | Iter Loss 0.0864 | Iter Mean Loss 0.2166\n",
      "2024-11-14 04:41:50,331 - root - INFO - KG Training: Epoch 0001 Iter 1522 / 3136 | Time 0.1s | Iter Loss 0.0905 | Iter Mean Loss 0.2165\n",
      "2024-11-14 04:41:50,396 - root - INFO - KG Training: Epoch 0001 Iter 1523 / 3136 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.2164\n",
      "2024-11-14 04:41:50,457 - root - INFO - KG Training: Epoch 0001 Iter 1524 / 3136 | Time 0.1s | Iter Loss 0.0890 | Iter Mean Loss 0.2163\n",
      "2024-11-14 04:41:50,517 - root - INFO - KG Training: Epoch 0001 Iter 1525 / 3136 | Time 0.1s | Iter Loss 0.0923 | Iter Mean Loss 0.2163\n",
      "2024-11-14 04:41:50,579 - root - INFO - KG Training: Epoch 0001 Iter 1526 / 3136 | Time 0.1s | Iter Loss 0.0824 | Iter Mean Loss 0.2162\n",
      "2024-11-14 04:41:50,641 - root - INFO - KG Training: Epoch 0001 Iter 1527 / 3136 | Time 0.1s | Iter Loss 0.1055 | Iter Mean Loss 0.2161\n",
      "2024-11-14 04:41:50,701 - root - INFO - KG Training: Epoch 0001 Iter 1528 / 3136 | Time 0.1s | Iter Loss 0.0896 | Iter Mean Loss 0.2160\n",
      "2024-11-14 04:41:50,763 - root - INFO - KG Training: Epoch 0001 Iter 1529 / 3136 | Time 0.1s | Iter Loss 0.0878 | Iter Mean Loss 0.2159\n",
      "2024-11-14 04:41:50,823 - root - INFO - KG Training: Epoch 0001 Iter 1530 / 3136 | Time 0.1s | Iter Loss 0.0894 | Iter Mean Loss 0.2159\n",
      "2024-11-14 04:41:50,886 - root - INFO - KG Training: Epoch 0001 Iter 1531 / 3136 | Time 0.1s | Iter Loss 0.0858 | Iter Mean Loss 0.2158\n",
      "2024-11-14 04:41:50,947 - root - INFO - KG Training: Epoch 0001 Iter 1532 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.2157\n",
      "2024-11-14 04:41:51,008 - root - INFO - KG Training: Epoch 0001 Iter 1533 / 3136 | Time 0.1s | Iter Loss 0.1020 | Iter Mean Loss 0.2156\n",
      "2024-11-14 04:41:51,189 - root - INFO - KG Training: Epoch 0001 Iter 1534 / 3136 | Time 0.2s | Iter Loss 0.0847 | Iter Mean Loss 0.2155\n",
      "2024-11-14 04:41:51,250 - root - INFO - KG Training: Epoch 0001 Iter 1535 / 3136 | Time 0.1s | Iter Loss 0.0883 | Iter Mean Loss 0.2154\n",
      "2024-11-14 04:41:51,429 - root - INFO - KG Training: Epoch 0001 Iter 1536 / 3136 | Time 0.2s | Iter Loss 0.0900 | Iter Mean Loss 0.2154\n",
      "2024-11-14 04:41:51,490 - root - INFO - KG Training: Epoch 0001 Iter 1537 / 3136 | Time 0.1s | Iter Loss 0.0958 | Iter Mean Loss 0.2153\n",
      "2024-11-14 04:41:51,553 - root - INFO - KG Training: Epoch 0001 Iter 1538 / 3136 | Time 0.1s | Iter Loss 0.0926 | Iter Mean Loss 0.2152\n",
      "2024-11-14 04:41:51,614 - root - INFO - KG Training: Epoch 0001 Iter 1539 / 3136 | Time 0.1s | Iter Loss 0.1006 | Iter Mean Loss 0.2151\n",
      "2024-11-14 04:41:51,677 - root - INFO - KG Training: Epoch 0001 Iter 1540 / 3136 | Time 0.1s | Iter Loss 0.0850 | Iter Mean Loss 0.2150\n",
      "2024-11-14 04:41:51,757 - root - INFO - KG Training: Epoch 0001 Iter 1541 / 3136 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.2150\n",
      "2024-11-14 04:41:51,815 - root - INFO - KG Training: Epoch 0001 Iter 1542 / 3136 | Time 0.1s | Iter Loss 0.0997 | Iter Mean Loss 0.2149\n",
      "2024-11-14 04:41:51,945 - root - INFO - KG Training: Epoch 0001 Iter 1543 / 3136 | Time 0.1s | Iter Loss 0.0878 | Iter Mean Loss 0.2148\n",
      "2024-11-14 04:41:52,018 - root - INFO - KG Training: Epoch 0001 Iter 1544 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.2147\n",
      "2024-11-14 04:41:52,079 - root - INFO - KG Training: Epoch 0001 Iter 1545 / 3136 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.2146\n",
      "2024-11-14 04:41:52,137 - root - INFO - KG Training: Epoch 0001 Iter 1546 / 3136 | Time 0.1s | Iter Loss 0.0937 | Iter Mean Loss 0.2146\n",
      "2024-11-14 04:41:52,214 - root - INFO - KG Training: Epoch 0001 Iter 1547 / 3136 | Time 0.1s | Iter Loss 0.0909 | Iter Mean Loss 0.2145\n",
      "2024-11-14 04:41:52,277 - root - INFO - KG Training: Epoch 0001 Iter 1548 / 3136 | Time 0.1s | Iter Loss 0.0859 | Iter Mean Loss 0.2144\n",
      "2024-11-14 04:41:52,338 - root - INFO - KG Training: Epoch 0001 Iter 1549 / 3136 | Time 0.1s | Iter Loss 0.0878 | Iter Mean Loss 0.2143\n",
      "2024-11-14 04:41:52,401 - root - INFO - KG Training: Epoch 0001 Iter 1550 / 3136 | Time 0.1s | Iter Loss 0.0822 | Iter Mean Loss 0.2142\n",
      "2024-11-14 04:41:52,458 - root - INFO - KG Training: Epoch 0001 Iter 1551 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.2141\n",
      "2024-11-14 04:41:52,518 - root - INFO - KG Training: Epoch 0001 Iter 1552 / 3136 | Time 0.1s | Iter Loss 0.0893 | Iter Mean Loss 0.2141\n",
      "2024-11-14 04:41:52,582 - root - INFO - KG Training: Epoch 0001 Iter 1553 / 3136 | Time 0.1s | Iter Loss 0.0781 | Iter Mean Loss 0.2140\n",
      "2024-11-14 04:41:52,644 - root - INFO - KG Training: Epoch 0001 Iter 1554 / 3136 | Time 0.1s | Iter Loss 0.1105 | Iter Mean Loss 0.2139\n",
      "2024-11-14 04:41:52,707 - root - INFO - KG Training: Epoch 0001 Iter 1555 / 3136 | Time 0.1s | Iter Loss 0.0946 | Iter Mean Loss 0.2138\n",
      "2024-11-14 04:41:52,776 - root - INFO - KG Training: Epoch 0001 Iter 1556 / 3136 | Time 0.1s | Iter Loss 0.0960 | Iter Mean Loss 0.2138\n",
      "2024-11-14 04:41:52,897 - root - INFO - KG Training: Epoch 0001 Iter 1557 / 3136 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.2137\n",
      "2024-11-14 04:41:52,964 - root - INFO - KG Training: Epoch 0001 Iter 1558 / 3136 | Time 0.1s | Iter Loss 0.0915 | Iter Mean Loss 0.2136\n",
      "2024-11-14 04:41:53,024 - root - INFO - KG Training: Epoch 0001 Iter 1559 / 3136 | Time 0.1s | Iter Loss 0.0932 | Iter Mean Loss 0.2135\n",
      "2024-11-14 04:41:53,088 - root - INFO - KG Training: Epoch 0001 Iter 1560 / 3136 | Time 0.1s | Iter Loss 0.0868 | Iter Mean Loss 0.2134\n",
      "2024-11-14 04:41:53,154 - root - INFO - KG Training: Epoch 0001 Iter 1561 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.2134\n",
      "2024-11-14 04:41:53,214 - root - INFO - KG Training: Epoch 0001 Iter 1562 / 3136 | Time 0.1s | Iter Loss 0.0936 | Iter Mean Loss 0.2133\n",
      "2024-11-14 04:41:53,279 - root - INFO - KG Training: Epoch 0001 Iter 1563 / 3136 | Time 0.1s | Iter Loss 0.1000 | Iter Mean Loss 0.2132\n",
      "2024-11-14 04:41:53,343 - root - INFO - KG Training: Epoch 0001 Iter 1564 / 3136 | Time 0.1s | Iter Loss 0.1130 | Iter Mean Loss 0.2132\n",
      "2024-11-14 04:41:53,406 - root - INFO - KG Training: Epoch 0001 Iter 1565 / 3136 | Time 0.1s | Iter Loss 0.0843 | Iter Mean Loss 0.2131\n",
      "2024-11-14 04:41:53,524 - root - INFO - KG Training: Epoch 0001 Iter 1566 / 3136 | Time 0.1s | Iter Loss 0.0943 | Iter Mean Loss 0.2130\n",
      "2024-11-14 04:41:53,587 - root - INFO - KG Training: Epoch 0001 Iter 1567 / 3136 | Time 0.1s | Iter Loss 0.0916 | Iter Mean Loss 0.2129\n",
      "2024-11-14 04:41:53,648 - root - INFO - KG Training: Epoch 0001 Iter 1568 / 3136 | Time 0.1s | Iter Loss 0.0890 | Iter Mean Loss 0.2128\n",
      "2024-11-14 04:41:53,707 - root - INFO - KG Training: Epoch 0001 Iter 1569 / 3136 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.2128\n",
      "2024-11-14 04:41:53,769 - root - INFO - KG Training: Epoch 0001 Iter 1570 / 3136 | Time 0.1s | Iter Loss 0.0848 | Iter Mean Loss 0.2127\n",
      "2024-11-14 04:41:53,830 - root - INFO - KG Training: Epoch 0001 Iter 1571 / 3136 | Time 0.1s | Iter Loss 0.0828 | Iter Mean Loss 0.2126\n",
      "2024-11-14 04:41:53,892 - root - INFO - KG Training: Epoch 0001 Iter 1572 / 3136 | Time 0.1s | Iter Loss 0.0973 | Iter Mean Loss 0.2125\n",
      "2024-11-14 04:41:53,954 - root - INFO - KG Training: Epoch 0001 Iter 1573 / 3136 | Time 0.1s | Iter Loss 0.0808 | Iter Mean Loss 0.2124\n",
      "2024-11-14 04:41:54,016 - root - INFO - KG Training: Epoch 0001 Iter 1574 / 3136 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.2124\n",
      "2024-11-14 04:41:54,081 - root - INFO - KG Training: Epoch 0001 Iter 1575 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.2123\n",
      "2024-11-14 04:41:54,144 - root - INFO - KG Training: Epoch 0001 Iter 1576 / 3136 | Time 0.1s | Iter Loss 0.0975 | Iter Mean Loss 0.2122\n",
      "2024-11-14 04:41:54,206 - root - INFO - KG Training: Epoch 0001 Iter 1577 / 3136 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.2121\n",
      "2024-11-14 04:41:54,266 - root - INFO - KG Training: Epoch 0001 Iter 1578 / 3136 | Time 0.1s | Iter Loss 0.0789 | Iter Mean Loss 0.2120\n",
      "2024-11-14 04:41:54,332 - root - INFO - KG Training: Epoch 0001 Iter 1579 / 3136 | Time 0.1s | Iter Loss 0.0974 | Iter Mean Loss 0.2120\n",
      "2024-11-14 04:41:54,393 - root - INFO - KG Training: Epoch 0001 Iter 1580 / 3136 | Time 0.1s | Iter Loss 0.0912 | Iter Mean Loss 0.2119\n",
      "2024-11-14 04:41:54,455 - root - INFO - KG Training: Epoch 0001 Iter 1581 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.2118\n",
      "2024-11-14 04:41:54,577 - root - INFO - KG Training: Epoch 0001 Iter 1582 / 3136 | Time 0.1s | Iter Loss 0.0986 | Iter Mean Loss 0.2117\n",
      "2024-11-14 04:41:54,638 - root - INFO - KG Training: Epoch 0001 Iter 1583 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.2116\n",
      "2024-11-14 04:41:54,703 - root - INFO - KG Training: Epoch 0001 Iter 1584 / 3136 | Time 0.1s | Iter Loss 0.0850 | Iter Mean Loss 0.2116\n",
      "2024-11-14 04:41:54,769 - root - INFO - KG Training: Epoch 0001 Iter 1585 / 3136 | Time 0.1s | Iter Loss 0.0857 | Iter Mean Loss 0.2115\n",
      "2024-11-14 04:41:54,831 - root - INFO - KG Training: Epoch 0001 Iter 1586 / 3136 | Time 0.1s | Iter Loss 0.0933 | Iter Mean Loss 0.2114\n",
      "2024-11-14 04:41:54,896 - root - INFO - KG Training: Epoch 0001 Iter 1587 / 3136 | Time 0.1s | Iter Loss 0.0825 | Iter Mean Loss 0.2113\n",
      "2024-11-14 04:41:54,956 - root - INFO - KG Training: Epoch 0001 Iter 1588 / 3136 | Time 0.1s | Iter Loss 0.0904 | Iter Mean Loss 0.2113\n",
      "2024-11-14 04:41:55,020 - root - INFO - KG Training: Epoch 0001 Iter 1589 / 3136 | Time 0.1s | Iter Loss 0.0864 | Iter Mean Loss 0.2112\n",
      "2024-11-14 04:41:55,081 - root - INFO - KG Training: Epoch 0001 Iter 1590 / 3136 | Time 0.1s | Iter Loss 0.0860 | Iter Mean Loss 0.2111\n",
      "2024-11-14 04:41:55,143 - root - INFO - KG Training: Epoch 0001 Iter 1591 / 3136 | Time 0.1s | Iter Loss 0.0906 | Iter Mean Loss 0.2110\n",
      "2024-11-14 04:41:55,210 - root - INFO - KG Training: Epoch 0001 Iter 1592 / 3136 | Time 0.1s | Iter Loss 0.0866 | Iter Mean Loss 0.2109\n",
      "2024-11-14 04:41:55,273 - root - INFO - KG Training: Epoch 0001 Iter 1593 / 3136 | Time 0.1s | Iter Loss 0.0928 | Iter Mean Loss 0.2109\n",
      "2024-11-14 04:41:55,336 - root - INFO - KG Training: Epoch 0001 Iter 1594 / 3136 | Time 0.1s | Iter Loss 0.0868 | Iter Mean Loss 0.2108\n",
      "2024-11-14 04:41:55,397 - root - INFO - KG Training: Epoch 0001 Iter 1595 / 3136 | Time 0.1s | Iter Loss 0.0849 | Iter Mean Loss 0.2107\n",
      "2024-11-14 04:41:55,459 - root - INFO - KG Training: Epoch 0001 Iter 1596 / 3136 | Time 0.1s | Iter Loss 0.0891 | Iter Mean Loss 0.2106\n",
      "2024-11-14 04:41:55,524 - root - INFO - KG Training: Epoch 0001 Iter 1597 / 3136 | Time 0.1s | Iter Loss 0.0913 | Iter Mean Loss 0.2106\n",
      "2024-11-14 04:41:55,585 - root - INFO - KG Training: Epoch 0001 Iter 1598 / 3136 | Time 0.1s | Iter Loss 0.0977 | Iter Mean Loss 0.2105\n",
      "2024-11-14 04:41:55,646 - root - INFO - KG Training: Epoch 0001 Iter 1599 / 3136 | Time 0.1s | Iter Loss 0.0861 | Iter Mean Loss 0.2104\n",
      "2024-11-14 04:41:55,708 - root - INFO - KG Training: Epoch 0001 Iter 1600 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.2103\n",
      "2024-11-14 04:41:55,772 - root - INFO - KG Training: Epoch 0001 Iter 1601 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.2103\n",
      "2024-11-14 04:41:55,833 - root - INFO - KG Training: Epoch 0001 Iter 1602 / 3136 | Time 0.1s | Iter Loss 0.0942 | Iter Mean Loss 0.2102\n",
      "2024-11-14 04:41:55,988 - root - INFO - KG Training: Epoch 0001 Iter 1603 / 3136 | Time 0.2s | Iter Loss 0.0872 | Iter Mean Loss 0.2101\n",
      "2024-11-14 04:41:56,049 - root - INFO - KG Training: Epoch 0001 Iter 1604 / 3136 | Time 0.1s | Iter Loss 0.0846 | Iter Mean Loss 0.2100\n",
      "2024-11-14 04:41:56,112 - root - INFO - KG Training: Epoch 0001 Iter 1605 / 3136 | Time 0.1s | Iter Loss 0.0849 | Iter Mean Loss 0.2100\n",
      "2024-11-14 04:41:56,175 - root - INFO - KG Training: Epoch 0001 Iter 1606 / 3136 | Time 0.1s | Iter Loss 0.0889 | Iter Mean Loss 0.2099\n",
      "2024-11-14 04:41:56,240 - root - INFO - KG Training: Epoch 0001 Iter 1607 / 3136 | Time 0.1s | Iter Loss 0.0860 | Iter Mean Loss 0.2098\n",
      "2024-11-14 04:41:56,494 - root - INFO - KG Training: Epoch 0001 Iter 1608 / 3136 | Time 0.3s | Iter Loss 0.0849 | Iter Mean Loss 0.2097\n",
      "2024-11-14 04:41:56,627 - root - INFO - KG Training: Epoch 0001 Iter 1609 / 3136 | Time 0.1s | Iter Loss 0.0867 | Iter Mean Loss 0.2096\n",
      "2024-11-14 04:41:56,689 - root - INFO - KG Training: Epoch 0001 Iter 1610 / 3136 | Time 0.1s | Iter Loss 0.0907 | Iter Mean Loss 0.2096\n",
      "2024-11-14 04:41:56,752 - root - INFO - KG Training: Epoch 0001 Iter 1611 / 3136 | Time 0.1s | Iter Loss 0.0799 | Iter Mean Loss 0.2095\n",
      "2024-11-14 04:41:56,813 - root - INFO - KG Training: Epoch 0001 Iter 1612 / 3136 | Time 0.1s | Iter Loss 0.0914 | Iter Mean Loss 0.2094\n",
      "2024-11-14 04:41:56,877 - root - INFO - KG Training: Epoch 0001 Iter 1613 / 3136 | Time 0.1s | Iter Loss 0.0975 | Iter Mean Loss 0.2094\n",
      "2024-11-14 04:41:56,939 - root - INFO - KG Training: Epoch 0001 Iter 1614 / 3136 | Time 0.1s | Iter Loss 0.0804 | Iter Mean Loss 0.2093\n",
      "2024-11-14 04:41:57,001 - root - INFO - KG Training: Epoch 0001 Iter 1615 / 3136 | Time 0.1s | Iter Loss 0.0857 | Iter Mean Loss 0.2092\n",
      "2024-11-14 04:41:57,063 - root - INFO - KG Training: Epoch 0001 Iter 1616 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.2091\n",
      "2024-11-14 04:41:57,127 - root - INFO - KG Training: Epoch 0001 Iter 1617 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.2090\n",
      "2024-11-14 04:41:57,186 - root - INFO - KG Training: Epoch 0001 Iter 1618 / 3136 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.2090\n",
      "2024-11-14 04:41:57,246 - root - INFO - KG Training: Epoch 0001 Iter 1619 / 3136 | Time 0.1s | Iter Loss 0.0942 | Iter Mean Loss 0.2089\n",
      "2024-11-14 04:41:57,309 - root - INFO - KG Training: Epoch 0001 Iter 1620 / 3136 | Time 0.1s | Iter Loss 0.0892 | Iter Mean Loss 0.2088\n",
      "2024-11-14 04:41:57,368 - root - INFO - KG Training: Epoch 0001 Iter 1621 / 3136 | Time 0.1s | Iter Loss 0.0757 | Iter Mean Loss 0.2087\n",
      "2024-11-14 04:41:57,569 - root - INFO - KG Training: Epoch 0001 Iter 1622 / 3136 | Time 0.2s | Iter Loss 0.0876 | Iter Mean Loss 0.2087\n",
      "2024-11-14 04:41:57,629 - root - INFO - KG Training: Epoch 0001 Iter 1623 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.2086\n",
      "2024-11-14 04:41:57,690 - root - INFO - KG Training: Epoch 0001 Iter 1624 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.2085\n",
      "2024-11-14 04:41:57,803 - root - INFO - KG Training: Epoch 0001 Iter 1625 / 3136 | Time 0.1s | Iter Loss 0.0923 | Iter Mean Loss 0.2084\n",
      "2024-11-14 04:41:57,867 - root - INFO - KG Training: Epoch 0001 Iter 1626 / 3136 | Time 0.1s | Iter Loss 0.0820 | Iter Mean Loss 0.2084\n",
      "2024-11-14 04:41:57,929 - root - INFO - KG Training: Epoch 0001 Iter 1627 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.2083\n",
      "2024-11-14 04:41:57,991 - root - INFO - KG Training: Epoch 0001 Iter 1628 / 3136 | Time 0.1s | Iter Loss 0.0894 | Iter Mean Loss 0.2082\n",
      "2024-11-14 04:41:58,056 - root - INFO - KG Training: Epoch 0001 Iter 1629 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.2081\n",
      "2024-11-14 04:41:58,257 - root - INFO - KG Training: Epoch 0001 Iter 1630 / 3136 | Time 0.2s | Iter Loss 0.0859 | Iter Mean Loss 0.2081\n",
      "2024-11-14 04:41:58,316 - root - INFO - KG Training: Epoch 0001 Iter 1631 / 3136 | Time 0.1s | Iter Loss 0.0863 | Iter Mean Loss 0.2080\n",
      "2024-11-14 04:41:58,432 - root - INFO - KG Training: Epoch 0001 Iter 1632 / 3136 | Time 0.1s | Iter Loss 0.0904 | Iter Mean Loss 0.2079\n",
      "2024-11-14 04:41:58,494 - root - INFO - KG Training: Epoch 0001 Iter 1633 / 3136 | Time 0.1s | Iter Loss 0.0930 | Iter Mean Loss 0.2078\n",
      "2024-11-14 04:41:58,555 - root - INFO - KG Training: Epoch 0001 Iter 1634 / 3136 | Time 0.1s | Iter Loss 0.0918 | Iter Mean Loss 0.2078\n",
      "2024-11-14 04:41:58,612 - root - INFO - KG Training: Epoch 0001 Iter 1635 / 3136 | Time 0.1s | Iter Loss 0.0878 | Iter Mean Loss 0.2077\n",
      "2024-11-14 04:41:58,675 - root - INFO - KG Training: Epoch 0001 Iter 1636 / 3136 | Time 0.1s | Iter Loss 0.0991 | Iter Mean Loss 0.2076\n",
      "2024-11-14 04:41:58,739 - root - INFO - KG Training: Epoch 0001 Iter 1637 / 3136 | Time 0.1s | Iter Loss 0.0764 | Iter Mean Loss 0.2075\n",
      "2024-11-14 04:41:58,802 - root - INFO - KG Training: Epoch 0001 Iter 1638 / 3136 | Time 0.1s | Iter Loss 0.0861 | Iter Mean Loss 0.2075\n",
      "2024-11-14 04:41:58,863 - root - INFO - KG Training: Epoch 0001 Iter 1639 / 3136 | Time 0.1s | Iter Loss 0.0935 | Iter Mean Loss 0.2074\n",
      "2024-11-14 04:41:58,925 - root - INFO - KG Training: Epoch 0001 Iter 1640 / 3136 | Time 0.1s | Iter Loss 0.0904 | Iter Mean Loss 0.2073\n",
      "2024-11-14 04:41:58,990 - root - INFO - KG Training: Epoch 0001 Iter 1641 / 3136 | Time 0.1s | Iter Loss 0.0948 | Iter Mean Loss 0.2073\n",
      "2024-11-14 04:41:59,103 - root - INFO - KG Training: Epoch 0001 Iter 1642 / 3136 | Time 0.1s | Iter Loss 0.0863 | Iter Mean Loss 0.2072\n",
      "2024-11-14 04:41:59,167 - root - INFO - KG Training: Epoch 0001 Iter 1643 / 3136 | Time 0.1s | Iter Loss 0.0877 | Iter Mean Loss 0.2071\n",
      "2024-11-14 04:41:59,452 - root - INFO - KG Training: Epoch 0001 Iter 1644 / 3136 | Time 0.3s | Iter Loss 0.0888 | Iter Mean Loss 0.2070\n",
      "2024-11-14 04:41:59,529 - root - INFO - KG Training: Epoch 0001 Iter 1645 / 3136 | Time 0.1s | Iter Loss 0.0898 | Iter Mean Loss 0.2070\n",
      "2024-11-14 04:41:59,588 - root - INFO - KG Training: Epoch 0001 Iter 1646 / 3136 | Time 0.1s | Iter Loss 0.0777 | Iter Mean Loss 0.2069\n",
      "2024-11-14 04:41:59,669 - root - INFO - KG Training: Epoch 0001 Iter 1647 / 3136 | Time 0.1s | Iter Loss 0.0954 | Iter Mean Loss 0.2068\n",
      "2024-11-14 04:41:59,747 - root - INFO - KG Training: Epoch 0001 Iter 1648 / 3136 | Time 0.1s | Iter Loss 0.0929 | Iter Mean Loss 0.2068\n",
      "2024-11-14 04:41:59,805 - root - INFO - KG Training: Epoch 0001 Iter 1649 / 3136 | Time 0.1s | Iter Loss 0.0957 | Iter Mean Loss 0.2067\n",
      "2024-11-14 04:41:59,863 - root - INFO - KG Training: Epoch 0001 Iter 1650 / 3136 | Time 0.1s | Iter Loss 0.0912 | Iter Mean Loss 0.2066\n",
      "2024-11-14 04:41:59,919 - root - INFO - KG Training: Epoch 0001 Iter 1651 / 3136 | Time 0.1s | Iter Loss 0.0765 | Iter Mean Loss 0.2065\n",
      "2024-11-14 04:41:59,980 - root - INFO - KG Training: Epoch 0001 Iter 1652 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.2065\n",
      "2024-11-14 04:42:00,046 - root - INFO - KG Training: Epoch 0001 Iter 1653 / 3136 | Time 0.1s | Iter Loss 0.0746 | Iter Mean Loss 0.2064\n",
      "2024-11-14 04:42:00,104 - root - INFO - KG Training: Epoch 0001 Iter 1654 / 3136 | Time 0.1s | Iter Loss 0.0821 | Iter Mean Loss 0.2063\n",
      "2024-11-14 04:42:00,169 - root - INFO - KG Training: Epoch 0001 Iter 1655 / 3136 | Time 0.1s | Iter Loss 0.0995 | Iter Mean Loss 0.2062\n",
      "2024-11-14 04:42:00,231 - root - INFO - KG Training: Epoch 0001 Iter 1656 / 3136 | Time 0.1s | Iter Loss 0.0908 | Iter Mean Loss 0.2062\n",
      "2024-11-14 04:42:00,291 - root - INFO - KG Training: Epoch 0001 Iter 1657 / 3136 | Time 0.1s | Iter Loss 0.0927 | Iter Mean Loss 0.2061\n",
      "2024-11-14 04:42:00,352 - root - INFO - KG Training: Epoch 0001 Iter 1658 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.2060\n",
      "2024-11-14 04:42:00,410 - root - INFO - KG Training: Epoch 0001 Iter 1659 / 3136 | Time 0.1s | Iter Loss 0.0919 | Iter Mean Loss 0.2060\n",
      "2024-11-14 04:42:00,469 - root - INFO - KG Training: Epoch 0001 Iter 1660 / 3136 | Time 0.1s | Iter Loss 0.0848 | Iter Mean Loss 0.2059\n",
      "2024-11-14 04:42:00,527 - root - INFO - KG Training: Epoch 0001 Iter 1661 / 3136 | Time 0.1s | Iter Loss 0.0846 | Iter Mean Loss 0.2058\n",
      "2024-11-14 04:42:00,585 - root - INFO - KG Training: Epoch 0001 Iter 1662 / 3136 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.2058\n",
      "2024-11-14 04:42:00,642 - root - INFO - KG Training: Epoch 0001 Iter 1663 / 3136 | Time 0.1s | Iter Loss 0.0814 | Iter Mean Loss 0.2057\n",
      "2024-11-14 04:42:00,699 - root - INFO - KG Training: Epoch 0001 Iter 1664 / 3136 | Time 0.1s | Iter Loss 0.0861 | Iter Mean Loss 0.2056\n",
      "2024-11-14 04:42:00,757 - root - INFO - KG Training: Epoch 0001 Iter 1665 / 3136 | Time 0.1s | Iter Loss 0.1018 | Iter Mean Loss 0.2055\n",
      "2024-11-14 04:42:00,817 - root - INFO - KG Training: Epoch 0001 Iter 1666 / 3136 | Time 0.1s | Iter Loss 0.0752 | Iter Mean Loss 0.2055\n",
      "2024-11-14 04:42:00,879 - root - INFO - KG Training: Epoch 0001 Iter 1667 / 3136 | Time 0.1s | Iter Loss 0.0888 | Iter Mean Loss 0.2054\n",
      "2024-11-14 04:42:00,940 - root - INFO - KG Training: Epoch 0001 Iter 1668 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.2053\n",
      "2024-11-14 04:42:01,001 - root - INFO - KG Training: Epoch 0001 Iter 1669 / 3136 | Time 0.1s | Iter Loss 0.0840 | Iter Mean Loss 0.2052\n",
      "2024-11-14 04:42:01,062 - root - INFO - KG Training: Epoch 0001 Iter 1670 / 3136 | Time 0.1s | Iter Loss 0.0856 | Iter Mean Loss 0.2052\n",
      "2024-11-14 04:42:01,242 - root - INFO - KG Training: Epoch 0001 Iter 1671 / 3136 | Time 0.2s | Iter Loss 0.0830 | Iter Mean Loss 0.2051\n",
      "2024-11-14 04:42:01,302 - root - INFO - KG Training: Epoch 0001 Iter 1672 / 3136 | Time 0.1s | Iter Loss 0.0832 | Iter Mean Loss 0.2050\n",
      "2024-11-14 04:42:01,361 - root - INFO - KG Training: Epoch 0001 Iter 1673 / 3136 | Time 0.1s | Iter Loss 0.0902 | Iter Mean Loss 0.2050\n",
      "2024-11-14 04:42:01,423 - root - INFO - KG Training: Epoch 0001 Iter 1674 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.2049\n",
      "2024-11-14 04:42:01,483 - root - INFO - KG Training: Epoch 0001 Iter 1675 / 3136 | Time 0.1s | Iter Loss 0.0783 | Iter Mean Loss 0.2048\n",
      "2024-11-14 04:42:01,541 - root - INFO - KG Training: Epoch 0001 Iter 1676 / 3136 | Time 0.1s | Iter Loss 0.0889 | Iter Mean Loss 0.2047\n",
      "2024-11-14 04:42:01,681 - root - INFO - KG Training: Epoch 0001 Iter 1677 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.2047\n",
      "2024-11-14 04:42:01,741 - root - INFO - KG Training: Epoch 0001 Iter 1678 / 3136 | Time 0.1s | Iter Loss 0.0791 | Iter Mean Loss 0.2046\n",
      "2024-11-14 04:42:01,801 - root - INFO - KG Training: Epoch 0001 Iter 1679 / 3136 | Time 0.1s | Iter Loss 0.0815 | Iter Mean Loss 0.2045\n",
      "2024-11-14 04:42:01,861 - root - INFO - KG Training: Epoch 0001 Iter 1680 / 3136 | Time 0.1s | Iter Loss 0.0928 | Iter Mean Loss 0.2045\n",
      "2024-11-14 04:42:01,924 - root - INFO - KG Training: Epoch 0001 Iter 1681 / 3136 | Time 0.1s | Iter Loss 0.0888 | Iter Mean Loss 0.2044\n",
      "2024-11-14 04:42:02,085 - root - INFO - KG Training: Epoch 0001 Iter 1682 / 3136 | Time 0.2s | Iter Loss 0.0952 | Iter Mean Loss 0.2043\n",
      "2024-11-14 04:42:02,145 - root - INFO - KG Training: Epoch 0001 Iter 1683 / 3136 | Time 0.1s | Iter Loss 0.0850 | Iter Mean Loss 0.2042\n",
      "2024-11-14 04:42:02,205 - root - INFO - KG Training: Epoch 0001 Iter 1684 / 3136 | Time 0.1s | Iter Loss 0.0818 | Iter Mean Loss 0.2042\n",
      "2024-11-14 04:42:02,266 - root - INFO - KG Training: Epoch 0001 Iter 1685 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.2041\n",
      "2024-11-14 04:42:02,325 - root - INFO - KG Training: Epoch 0001 Iter 1686 / 3136 | Time 0.1s | Iter Loss 0.0830 | Iter Mean Loss 0.2040\n",
      "2024-11-14 04:42:02,389 - root - INFO - KG Training: Epoch 0001 Iter 1687 / 3136 | Time 0.1s | Iter Loss 0.0869 | Iter Mean Loss 0.2040\n",
      "2024-11-14 04:42:02,448 - root - INFO - KG Training: Epoch 0001 Iter 1688 / 3136 | Time 0.1s | Iter Loss 0.0886 | Iter Mean Loss 0.2039\n",
      "2024-11-14 04:42:02,508 - root - INFO - KG Training: Epoch 0001 Iter 1689 / 3136 | Time 0.1s | Iter Loss 0.0861 | Iter Mean Loss 0.2038\n",
      "2024-11-14 04:42:02,574 - root - INFO - KG Training: Epoch 0001 Iter 1690 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.2038\n",
      "2024-11-14 04:42:02,635 - root - INFO - KG Training: Epoch 0001 Iter 1691 / 3136 | Time 0.1s | Iter Loss 0.0920 | Iter Mean Loss 0.2037\n",
      "2024-11-14 04:42:02,695 - root - INFO - KG Training: Epoch 0001 Iter 1692 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.2036\n",
      "2024-11-14 04:42:02,758 - root - INFO - KG Training: Epoch 0001 Iter 1693 / 3136 | Time 0.1s | Iter Loss 0.0778 | Iter Mean Loss 0.2035\n",
      "2024-11-14 04:42:02,825 - root - INFO - KG Training: Epoch 0001 Iter 1694 / 3136 | Time 0.1s | Iter Loss 0.0892 | Iter Mean Loss 0.2035\n",
      "2024-11-14 04:42:02,886 - root - INFO - KG Training: Epoch 0001 Iter 1695 / 3136 | Time 0.1s | Iter Loss 0.0957 | Iter Mean Loss 0.2034\n",
      "2024-11-14 04:42:02,952 - root - INFO - KG Training: Epoch 0001 Iter 1696 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.2033\n",
      "2024-11-14 04:42:03,014 - root - INFO - KG Training: Epoch 0001 Iter 1697 / 3136 | Time 0.1s | Iter Loss 0.0898 | Iter Mean Loss 0.2033\n",
      "2024-11-14 04:42:03,074 - root - INFO - KG Training: Epoch 0001 Iter 1698 / 3136 | Time 0.1s | Iter Loss 0.0875 | Iter Mean Loss 0.2032\n",
      "2024-11-14 04:42:03,137 - root - INFO - KG Training: Epoch 0001 Iter 1699 / 3136 | Time 0.1s | Iter Loss 0.0938 | Iter Mean Loss 0.2031\n",
      "2024-11-14 04:42:03,200 - root - INFO - KG Training: Epoch 0001 Iter 1700 / 3136 | Time 0.1s | Iter Loss 0.0866 | Iter Mean Loss 0.2031\n",
      "2024-11-14 04:42:03,267 - root - INFO - KG Training: Epoch 0001 Iter 1701 / 3136 | Time 0.1s | Iter Loss 0.0866 | Iter Mean Loss 0.2030\n",
      "2024-11-14 04:42:03,502 - root - INFO - KG Training: Epoch 0001 Iter 1702 / 3136 | Time 0.2s | Iter Loss 0.0803 | Iter Mean Loss 0.2029\n",
      "2024-11-14 04:42:03,560 - root - INFO - KG Training: Epoch 0001 Iter 1703 / 3136 | Time 0.1s | Iter Loss 0.0794 | Iter Mean Loss 0.2029\n",
      "2024-11-14 04:42:03,623 - root - INFO - KG Training: Epoch 0001 Iter 1704 / 3136 | Time 0.1s | Iter Loss 0.0826 | Iter Mean Loss 0.2028\n",
      "2024-11-14 04:42:03,683 - root - INFO - KG Training: Epoch 0001 Iter 1705 / 3136 | Time 0.1s | Iter Loss 0.0873 | Iter Mean Loss 0.2027\n",
      "2024-11-14 04:42:03,745 - root - INFO - KG Training: Epoch 0001 Iter 1706 / 3136 | Time 0.1s | Iter Loss 0.0804 | Iter Mean Loss 0.2026\n",
      "2024-11-14 04:42:03,805 - root - INFO - KG Training: Epoch 0001 Iter 1707 / 3136 | Time 0.1s | Iter Loss 0.0914 | Iter Mean Loss 0.2026\n",
      "2024-11-14 04:42:03,877 - root - INFO - KG Training: Epoch 0001 Iter 1708 / 3136 | Time 0.1s | Iter Loss 0.0820 | Iter Mean Loss 0.2025\n",
      "2024-11-14 04:42:03,932 - root - INFO - KG Training: Epoch 0001 Iter 1709 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.2024\n",
      "2024-11-14 04:42:04,010 - root - INFO - KG Training: Epoch 0001 Iter 1710 / 3136 | Time 0.1s | Iter Loss 0.0819 | Iter Mean Loss 0.2024\n",
      "2024-11-14 04:42:04,071 - root - INFO - KG Training: Epoch 0001 Iter 1711 / 3136 | Time 0.1s | Iter Loss 0.0882 | Iter Mean Loss 0.2023\n",
      "2024-11-14 04:42:04,131 - root - INFO - KG Training: Epoch 0001 Iter 1712 / 3136 | Time 0.1s | Iter Loss 0.0818 | Iter Mean Loss 0.2022\n",
      "2024-11-14 04:42:04,192 - root - INFO - KG Training: Epoch 0001 Iter 1713 / 3136 | Time 0.1s | Iter Loss 0.0879 | Iter Mean Loss 0.2022\n",
      "2024-11-14 04:42:04,251 - root - INFO - KG Training: Epoch 0001 Iter 1714 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.2021\n",
      "2024-11-14 04:42:04,314 - root - INFO - KG Training: Epoch 0001 Iter 1715 / 3136 | Time 0.1s | Iter Loss 0.1012 | Iter Mean Loss 0.2020\n",
      "2024-11-14 04:42:04,374 - root - INFO - KG Training: Epoch 0001 Iter 1716 / 3136 | Time 0.1s | Iter Loss 0.0796 | Iter Mean Loss 0.2020\n",
      "2024-11-14 04:42:04,434 - root - INFO - KG Training: Epoch 0001 Iter 1717 / 3136 | Time 0.1s | Iter Loss 0.0803 | Iter Mean Loss 0.2019\n",
      "2024-11-14 04:42:04,494 - root - INFO - KG Training: Epoch 0001 Iter 1718 / 3136 | Time 0.1s | Iter Loss 0.0828 | Iter Mean Loss 0.2018\n",
      "2024-11-14 04:42:04,554 - root - INFO - KG Training: Epoch 0001 Iter 1719 / 3136 | Time 0.1s | Iter Loss 0.0844 | Iter Mean Loss 0.2018\n",
      "2024-11-14 04:42:04,614 - root - INFO - KG Training: Epoch 0001 Iter 1720 / 3136 | Time 0.1s | Iter Loss 0.0938 | Iter Mean Loss 0.2017\n",
      "2024-11-14 04:42:04,672 - root - INFO - KG Training: Epoch 0001 Iter 1721 / 3136 | Time 0.1s | Iter Loss 0.0804 | Iter Mean Loss 0.2016\n",
      "2024-11-14 04:42:04,729 - root - INFO - KG Training: Epoch 0001 Iter 1722 / 3136 | Time 0.1s | Iter Loss 0.0740 | Iter Mean Loss 0.2015\n",
      "2024-11-14 04:42:04,790 - root - INFO - KG Training: Epoch 0001 Iter 1723 / 3136 | Time 0.1s | Iter Loss 0.0822 | Iter Mean Loss 0.2015\n",
      "2024-11-14 04:42:04,848 - root - INFO - KG Training: Epoch 0001 Iter 1724 / 3136 | Time 0.1s | Iter Loss 0.0792 | Iter Mean Loss 0.2014\n",
      "2024-11-14 04:42:04,958 - root - INFO - KG Training: Epoch 0001 Iter 1725 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.2013\n",
      "2024-11-14 04:42:05,014 - root - INFO - KG Training: Epoch 0001 Iter 1726 / 3136 | Time 0.1s | Iter Loss 0.0862 | Iter Mean Loss 0.2013\n",
      "2024-11-14 04:42:05,073 - root - INFO - KG Training: Epoch 0001 Iter 1727 / 3136 | Time 0.1s | Iter Loss 0.0759 | Iter Mean Loss 0.2012\n",
      "2024-11-14 04:42:05,132 - root - INFO - KG Training: Epoch 0001 Iter 1728 / 3136 | Time 0.1s | Iter Loss 0.0835 | Iter Mean Loss 0.2011\n",
      "2024-11-14 04:42:05,192 - root - INFO - KG Training: Epoch 0001 Iter 1729 / 3136 | Time 0.1s | Iter Loss 0.0814 | Iter Mean Loss 0.2011\n",
      "2024-11-14 04:42:05,254 - root - INFO - KG Training: Epoch 0001 Iter 1730 / 3136 | Time 0.1s | Iter Loss 0.0809 | Iter Mean Loss 0.2010\n",
      "2024-11-14 04:42:05,317 - root - INFO - KG Training: Epoch 0001 Iter 1731 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.2009\n",
      "2024-11-14 04:42:05,379 - root - INFO - KG Training: Epoch 0001 Iter 1732 / 3136 | Time 0.1s | Iter Loss 0.0918 | Iter Mean Loss 0.2009\n",
      "2024-11-14 04:42:05,446 - root - INFO - KG Training: Epoch 0001 Iter 1733 / 3136 | Time 0.1s | Iter Loss 0.0816 | Iter Mean Loss 0.2008\n",
      "2024-11-14 04:42:05,505 - root - INFO - KG Training: Epoch 0001 Iter 1734 / 3136 | Time 0.1s | Iter Loss 0.0824 | Iter Mean Loss 0.2007\n",
      "2024-11-14 04:42:05,564 - root - INFO - KG Training: Epoch 0001 Iter 1735 / 3136 | Time 0.1s | Iter Loss 0.0803 | Iter Mean Loss 0.2007\n",
      "2024-11-14 04:42:05,626 - root - INFO - KG Training: Epoch 0001 Iter 1736 / 3136 | Time 0.1s | Iter Loss 0.0816 | Iter Mean Loss 0.2006\n",
      "2024-11-14 04:42:05,683 - root - INFO - KG Training: Epoch 0001 Iter 1737 / 3136 | Time 0.1s | Iter Loss 0.0893 | Iter Mean Loss 0.2005\n",
      "2024-11-14 04:42:05,820 - root - INFO - KG Training: Epoch 0001 Iter 1738 / 3136 | Time 0.1s | Iter Loss 0.0750 | Iter Mean Loss 0.2005\n",
      "2024-11-14 04:42:05,882 - root - INFO - KG Training: Epoch 0001 Iter 1739 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.2004\n",
      "2024-11-14 04:42:05,944 - root - INFO - KG Training: Epoch 0001 Iter 1740 / 3136 | Time 0.1s | Iter Loss 0.0898 | Iter Mean Loss 0.2003\n",
      "2024-11-14 04:42:06,006 - root - INFO - KG Training: Epoch 0001 Iter 1741 / 3136 | Time 0.1s | Iter Loss 0.0885 | Iter Mean Loss 0.2003\n",
      "2024-11-14 04:42:06,069 - root - INFO - KG Training: Epoch 0001 Iter 1742 / 3136 | Time 0.1s | Iter Loss 0.0892 | Iter Mean Loss 0.2002\n",
      "2024-11-14 04:42:06,127 - root - INFO - KG Training: Epoch 0001 Iter 1743 / 3136 | Time 0.1s | Iter Loss 0.0930 | Iter Mean Loss 0.2001\n",
      "2024-11-14 04:42:06,284 - root - INFO - KG Training: Epoch 0001 Iter 1744 / 3136 | Time 0.2s | Iter Loss 0.0773 | Iter Mean Loss 0.2001\n",
      "2024-11-14 04:42:06,347 - root - INFO - KG Training: Epoch 0001 Iter 1745 / 3136 | Time 0.1s | Iter Loss 0.0769 | Iter Mean Loss 0.2000\n",
      "2024-11-14 04:42:06,410 - root - INFO - KG Training: Epoch 0001 Iter 1746 / 3136 | Time 0.1s | Iter Loss 0.0820 | Iter Mean Loss 0.1999\n",
      "2024-11-14 04:42:06,470 - root - INFO - KG Training: Epoch 0001 Iter 1747 / 3136 | Time 0.1s | Iter Loss 0.0809 | Iter Mean Loss 0.1999\n",
      "2024-11-14 04:42:06,530 - root - INFO - KG Training: Epoch 0001 Iter 1748 / 3136 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.1998\n",
      "2024-11-14 04:42:06,591 - root - INFO - KG Training: Epoch 0001 Iter 1749 / 3136 | Time 0.1s | Iter Loss 0.0875 | Iter Mean Loss 0.1997\n",
      "2024-11-14 04:42:06,654 - root - INFO - KG Training: Epoch 0001 Iter 1750 / 3136 | Time 0.1s | Iter Loss 0.0834 | Iter Mean Loss 0.1997\n",
      "2024-11-14 04:42:06,715 - root - INFO - KG Training: Epoch 0001 Iter 1751 / 3136 | Time 0.1s | Iter Loss 0.0757 | Iter Mean Loss 0.1996\n",
      "2024-11-14 04:42:06,775 - root - INFO - KG Training: Epoch 0001 Iter 1752 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1995\n",
      "2024-11-14 04:42:06,836 - root - INFO - KG Training: Epoch 0001 Iter 1753 / 3136 | Time 0.1s | Iter Loss 0.0922 | Iter Mean Loss 0.1994\n",
      "2024-11-14 04:42:06,898 - root - INFO - KG Training: Epoch 0001 Iter 1754 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1994\n",
      "2024-11-14 04:42:06,958 - root - INFO - KG Training: Epoch 0001 Iter 1755 / 3136 | Time 0.1s | Iter Loss 0.0837 | Iter Mean Loss 0.1993\n",
      "2024-11-14 04:42:07,019 - root - INFO - KG Training: Epoch 0001 Iter 1756 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.1992\n",
      "2024-11-14 04:42:07,080 - root - INFO - KG Training: Epoch 0001 Iter 1757 / 3136 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.1992\n",
      "2024-11-14 04:42:07,142 - root - INFO - KG Training: Epoch 0001 Iter 1758 / 3136 | Time 0.1s | Iter Loss 0.0808 | Iter Mean Loss 0.1991\n",
      "2024-11-14 04:42:07,204 - root - INFO - KG Training: Epoch 0001 Iter 1759 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.1990\n",
      "2024-11-14 04:42:07,265 - root - INFO - KG Training: Epoch 0001 Iter 1760 / 3136 | Time 0.1s | Iter Loss 0.0793 | Iter Mean Loss 0.1990\n",
      "2024-11-14 04:42:07,327 - root - INFO - KG Training: Epoch 0001 Iter 1761 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1989\n",
      "2024-11-14 04:42:07,672 - root - INFO - KG Training: Epoch 0001 Iter 1762 / 3136 | Time 0.3s | Iter Loss 0.0727 | Iter Mean Loss 0.1988\n",
      "2024-11-14 04:42:07,733 - root - INFO - KG Training: Epoch 0001 Iter 1763 / 3136 | Time 0.1s | Iter Loss 0.0838 | Iter Mean Loss 0.1988\n",
      "2024-11-14 04:42:07,794 - root - INFO - KG Training: Epoch 0001 Iter 1764 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1987\n",
      "2024-11-14 04:42:07,945 - root - INFO - KG Training: Epoch 0001 Iter 1765 / 3136 | Time 0.2s | Iter Loss 0.1032 | Iter Mean Loss 0.1987\n",
      "2024-11-14 04:42:08,003 - root - INFO - KG Training: Epoch 0001 Iter 1766 / 3136 | Time 0.1s | Iter Loss 0.0898 | Iter Mean Loss 0.1986\n",
      "2024-11-14 04:42:08,063 - root - INFO - KG Training: Epoch 0001 Iter 1767 / 3136 | Time 0.1s | Iter Loss 0.0854 | Iter Mean Loss 0.1985\n",
      "2024-11-14 04:42:08,123 - root - INFO - KG Training: Epoch 0001 Iter 1768 / 3136 | Time 0.1s | Iter Loss 0.1011 | Iter Mean Loss 0.1985\n",
      "2024-11-14 04:42:08,184 - root - INFO - KG Training: Epoch 0001 Iter 1769 / 3136 | Time 0.1s | Iter Loss 0.0829 | Iter Mean Loss 0.1984\n",
      "2024-11-14 04:42:08,244 - root - INFO - KG Training: Epoch 0001 Iter 1770 / 3136 | Time 0.1s | Iter Loss 0.0853 | Iter Mean Loss 0.1983\n",
      "2024-11-14 04:42:08,306 - root - INFO - KG Training: Epoch 0001 Iter 1771 / 3136 | Time 0.1s | Iter Loss 0.0778 | Iter Mean Loss 0.1983\n",
      "2024-11-14 04:42:08,366 - root - INFO - KG Training: Epoch 0001 Iter 1772 / 3136 | Time 0.1s | Iter Loss 0.0795 | Iter Mean Loss 0.1982\n",
      "2024-11-14 04:42:08,426 - root - INFO - KG Training: Epoch 0001 Iter 1773 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.1981\n",
      "2024-11-14 04:42:08,485 - root - INFO - KG Training: Epoch 0001 Iter 1774 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1981\n",
      "2024-11-14 04:42:08,545 - root - INFO - KG Training: Epoch 0001 Iter 1775 / 3136 | Time 0.1s | Iter Loss 0.0882 | Iter Mean Loss 0.1980\n",
      "2024-11-14 04:42:08,605 - root - INFO - KG Training: Epoch 0001 Iter 1776 / 3136 | Time 0.1s | Iter Loss 0.0809 | Iter Mean Loss 0.1980\n",
      "2024-11-14 04:42:08,667 - root - INFO - KG Training: Epoch 0001 Iter 1777 / 3136 | Time 0.1s | Iter Loss 0.0847 | Iter Mean Loss 0.1979\n",
      "2024-11-14 04:42:08,725 - root - INFO - KG Training: Epoch 0001 Iter 1778 / 3136 | Time 0.1s | Iter Loss 0.0828 | Iter Mean Loss 0.1978\n",
      "2024-11-14 04:42:08,784 - root - INFO - KG Training: Epoch 0001 Iter 1779 / 3136 | Time 0.1s | Iter Loss 0.0854 | Iter Mean Loss 0.1978\n",
      "2024-11-14 04:42:08,844 - root - INFO - KG Training: Epoch 0001 Iter 1780 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1977\n",
      "2024-11-14 04:42:08,910 - root - INFO - KG Training: Epoch 0001 Iter 1781 / 3136 | Time 0.1s | Iter Loss 0.0775 | Iter Mean Loss 0.1976\n",
      "2024-11-14 04:42:08,975 - root - INFO - KG Training: Epoch 0001 Iter 1782 / 3136 | Time 0.1s | Iter Loss 0.0814 | Iter Mean Loss 0.1976\n",
      "2024-11-14 04:42:09,037 - root - INFO - KG Training: Epoch 0001 Iter 1783 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1975\n",
      "2024-11-14 04:42:09,099 - root - INFO - KG Training: Epoch 0001 Iter 1784 / 3136 | Time 0.1s | Iter Loss 0.0854 | Iter Mean Loss 0.1974\n",
      "2024-11-14 04:42:09,248 - root - INFO - KG Training: Epoch 0001 Iter 1785 / 3136 | Time 0.1s | Iter Loss 0.0838 | Iter Mean Loss 0.1974\n",
      "2024-11-14 04:42:09,314 - root - INFO - KG Training: Epoch 0001 Iter 1786 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1973\n",
      "2024-11-14 04:42:09,378 - root - INFO - KG Training: Epoch 0001 Iter 1787 / 3136 | Time 0.1s | Iter Loss 0.0813 | Iter Mean Loss 0.1972\n",
      "2024-11-14 04:42:09,442 - root - INFO - KG Training: Epoch 0001 Iter 1788 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.1972\n",
      "2024-11-14 04:42:09,503 - root - INFO - KG Training: Epoch 0001 Iter 1789 / 3136 | Time 0.1s | Iter Loss 0.0746 | Iter Mean Loss 0.1971\n",
      "2024-11-14 04:42:09,567 - root - INFO - KG Training: Epoch 0001 Iter 1790 / 3136 | Time 0.1s | Iter Loss 0.0837 | Iter Mean Loss 0.1970\n",
      "2024-11-14 04:42:09,630 - root - INFO - KG Training: Epoch 0001 Iter 1791 / 3136 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.1970\n",
      "2024-11-14 04:42:09,690 - root - INFO - KG Training: Epoch 0001 Iter 1792 / 3136 | Time 0.1s | Iter Loss 0.0927 | Iter Mean Loss 0.1969\n",
      "2024-11-14 04:42:09,756 - root - INFO - KG Training: Epoch 0001 Iter 1793 / 3136 | Time 0.1s | Iter Loss 0.0772 | Iter Mean Loss 0.1969\n",
      "2024-11-14 04:42:09,818 - root - INFO - KG Training: Epoch 0001 Iter 1794 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1968\n",
      "2024-11-14 04:42:09,878 - root - INFO - KG Training: Epoch 0001 Iter 1795 / 3136 | Time 0.1s | Iter Loss 0.0829 | Iter Mean Loss 0.1967\n",
      "2024-11-14 04:42:10,023 - root - INFO - KG Training: Epoch 0001 Iter 1796 / 3136 | Time 0.1s | Iter Loss 0.0829 | Iter Mean Loss 0.1967\n",
      "2024-11-14 04:42:10,086 - root - INFO - KG Training: Epoch 0001 Iter 1797 / 3136 | Time 0.1s | Iter Loss 0.0859 | Iter Mean Loss 0.1966\n",
      "2024-11-14 04:42:10,149 - root - INFO - KG Training: Epoch 0001 Iter 1798 / 3136 | Time 0.1s | Iter Loss 0.0857 | Iter Mean Loss 0.1965\n",
      "2024-11-14 04:42:10,207 - root - INFO - KG Training: Epoch 0001 Iter 1799 / 3136 | Time 0.1s | Iter Loss 0.0776 | Iter Mean Loss 0.1965\n",
      "2024-11-14 04:42:10,269 - root - INFO - KG Training: Epoch 0001 Iter 1800 / 3136 | Time 0.1s | Iter Loss 0.0808 | Iter Mean Loss 0.1964\n",
      "2024-11-14 04:42:10,330 - root - INFO - KG Training: Epoch 0001 Iter 1801 / 3136 | Time 0.1s | Iter Loss 0.0741 | Iter Mean Loss 0.1963\n",
      "2024-11-14 04:42:10,393 - root - INFO - KG Training: Epoch 0001 Iter 1802 / 3136 | Time 0.1s | Iter Loss 0.0869 | Iter Mean Loss 0.1963\n",
      "2024-11-14 04:42:10,456 - root - INFO - KG Training: Epoch 0001 Iter 1803 / 3136 | Time 0.1s | Iter Loss 0.0688 | Iter Mean Loss 0.1962\n",
      "2024-11-14 04:42:10,516 - root - INFO - KG Training: Epoch 0001 Iter 1804 / 3136 | Time 0.1s | Iter Loss 0.0848 | Iter Mean Loss 0.1961\n",
      "2024-11-14 04:42:10,574 - root - INFO - KG Training: Epoch 0001 Iter 1805 / 3136 | Time 0.1s | Iter Loss 0.0791 | Iter Mean Loss 0.1961\n",
      "2024-11-14 04:42:10,634 - root - INFO - KG Training: Epoch 0001 Iter 1806 / 3136 | Time 0.1s | Iter Loss 0.0810 | Iter Mean Loss 0.1960\n",
      "2024-11-14 04:42:10,692 - root - INFO - KG Training: Epoch 0001 Iter 1807 / 3136 | Time 0.1s | Iter Loss 0.0774 | Iter Mean Loss 0.1960\n",
      "2024-11-14 04:42:10,749 - root - INFO - KG Training: Epoch 0001 Iter 1808 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1959\n",
      "2024-11-14 04:42:10,807 - root - INFO - KG Training: Epoch 0001 Iter 1809 / 3136 | Time 0.1s | Iter Loss 0.0704 | Iter Mean Loss 0.1958\n",
      "2024-11-14 04:42:10,866 - root - INFO - KG Training: Epoch 0001 Iter 1810 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.1958\n",
      "2024-11-14 04:42:10,927 - root - INFO - KG Training: Epoch 0001 Iter 1811 / 3136 | Time 0.1s | Iter Loss 0.0799 | Iter Mean Loss 0.1957\n",
      "2024-11-14 04:42:10,991 - root - INFO - KG Training: Epoch 0001 Iter 1812 / 3136 | Time 0.1s | Iter Loss 0.0866 | Iter Mean Loss 0.1956\n",
      "2024-11-14 04:42:11,057 - root - INFO - KG Training: Epoch 0001 Iter 1813 / 3136 | Time 0.1s | Iter Loss 0.0847 | Iter Mean Loss 0.1956\n",
      "2024-11-14 04:42:11,117 - root - INFO - KG Training: Epoch 0001 Iter 1814 / 3136 | Time 0.1s | Iter Loss 0.0830 | Iter Mean Loss 0.1955\n",
      "2024-11-14 04:42:11,176 - root - INFO - KG Training: Epoch 0001 Iter 1815 / 3136 | Time 0.1s | Iter Loss 0.0758 | Iter Mean Loss 0.1954\n",
      "2024-11-14 04:42:11,237 - root - INFO - KG Training: Epoch 0001 Iter 1816 / 3136 | Time 0.1s | Iter Loss 0.0944 | Iter Mean Loss 0.1954\n",
      "2024-11-14 04:42:11,297 - root - INFO - KG Training: Epoch 0001 Iter 1817 / 3136 | Time 0.1s | Iter Loss 0.0792 | Iter Mean Loss 0.1953\n",
      "2024-11-14 04:42:11,358 - root - INFO - KG Training: Epoch 0001 Iter 1818 / 3136 | Time 0.1s | Iter Loss 0.0797 | Iter Mean Loss 0.1953\n",
      "2024-11-14 04:42:11,416 - root - INFO - KG Training: Epoch 0001 Iter 1819 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.1952\n",
      "2024-11-14 04:42:11,476 - root - INFO - KG Training: Epoch 0001 Iter 1820 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.1951\n",
      "2024-11-14 04:42:11,538 - root - INFO - KG Training: Epoch 0001 Iter 1821 / 3136 | Time 0.1s | Iter Loss 0.0860 | Iter Mean Loss 0.1951\n",
      "2024-11-14 04:42:11,658 - root - INFO - KG Training: Epoch 0001 Iter 1822 / 3136 | Time 0.1s | Iter Loss 0.0819 | Iter Mean Loss 0.1950\n",
      "2024-11-14 04:42:11,716 - root - INFO - KG Training: Epoch 0001 Iter 1823 / 3136 | Time 0.1s | Iter Loss 0.0771 | Iter Mean Loss 0.1950\n",
      "2024-11-14 04:42:11,775 - root - INFO - KG Training: Epoch 0001 Iter 1824 / 3136 | Time 0.1s | Iter Loss 0.0789 | Iter Mean Loss 0.1949\n",
      "2024-11-14 04:42:11,834 - root - INFO - KG Training: Epoch 0001 Iter 1825 / 3136 | Time 0.1s | Iter Loss 0.0803 | Iter Mean Loss 0.1948\n",
      "2024-11-14 04:42:11,893 - root - INFO - KG Training: Epoch 0001 Iter 1826 / 3136 | Time 0.1s | Iter Loss 0.0941 | Iter Mean Loss 0.1948\n",
      "2024-11-14 04:42:11,953 - root - INFO - KG Training: Epoch 0001 Iter 1827 / 3136 | Time 0.1s | Iter Loss 0.0847 | Iter Mean Loss 0.1947\n",
      "2024-11-14 04:42:12,025 - root - INFO - KG Training: Epoch 0001 Iter 1828 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.1947\n",
      "2024-11-14 04:42:12,084 - root - INFO - KG Training: Epoch 0001 Iter 1829 / 3136 | Time 0.1s | Iter Loss 0.0872 | Iter Mean Loss 0.1946\n",
      "2024-11-14 04:42:12,142 - root - INFO - KG Training: Epoch 0001 Iter 1830 / 3136 | Time 0.1s | Iter Loss 0.0821 | Iter Mean Loss 0.1945\n",
      "2024-11-14 04:42:12,206 - root - INFO - KG Training: Epoch 0001 Iter 1831 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1945\n",
      "2024-11-14 04:42:12,263 - root - INFO - KG Training: Epoch 0001 Iter 1832 / 3136 | Time 0.1s | Iter Loss 0.0825 | Iter Mean Loss 0.1944\n",
      "2024-11-14 04:42:12,323 - root - INFO - KG Training: Epoch 0001 Iter 1833 / 3136 | Time 0.1s | Iter Loss 0.0875 | Iter Mean Loss 0.1943\n",
      "2024-11-14 04:42:12,382 - root - INFO - KG Training: Epoch 0001 Iter 1834 / 3136 | Time 0.1s | Iter Loss 0.0712 | Iter Mean Loss 0.1943\n",
      "2024-11-14 04:42:12,491 - root - INFO - KG Training: Epoch 0001 Iter 1835 / 3136 | Time 0.1s | Iter Loss 0.0811 | Iter Mean Loss 0.1942\n",
      "2024-11-14 04:42:12,549 - root - INFO - KG Training: Epoch 0001 Iter 1836 / 3136 | Time 0.1s | Iter Loss 0.0908 | Iter Mean Loss 0.1942\n",
      "2024-11-14 04:42:12,608 - root - INFO - KG Training: Epoch 0001 Iter 1837 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1941\n",
      "2024-11-14 04:42:12,666 - root - INFO - KG Training: Epoch 0001 Iter 1838 / 3136 | Time 0.1s | Iter Loss 0.0793 | Iter Mean Loss 0.1940\n",
      "2024-11-14 04:42:12,776 - root - INFO - KG Training: Epoch 0001 Iter 1839 / 3136 | Time 0.1s | Iter Loss 0.0731 | Iter Mean Loss 0.1940\n",
      "2024-11-14 04:42:12,837 - root - INFO - KG Training: Epoch 0001 Iter 1840 / 3136 | Time 0.1s | Iter Loss 0.0748 | Iter Mean Loss 0.1939\n",
      "2024-11-14 04:42:12,915 - root - INFO - KG Training: Epoch 0001 Iter 1841 / 3136 | Time 0.1s | Iter Loss 0.0828 | Iter Mean Loss 0.1938\n",
      "2024-11-14 04:42:12,976 - root - INFO - KG Training: Epoch 0001 Iter 1842 / 3136 | Time 0.1s | Iter Loss 0.0829 | Iter Mean Loss 0.1938\n",
      "2024-11-14 04:42:13,036 - root - INFO - KG Training: Epoch 0001 Iter 1843 / 3136 | Time 0.1s | Iter Loss 0.0835 | Iter Mean Loss 0.1937\n",
      "2024-11-14 04:42:13,097 - root - INFO - KG Training: Epoch 0001 Iter 1844 / 3136 | Time 0.1s | Iter Loss 0.0853 | Iter Mean Loss 0.1937\n",
      "2024-11-14 04:42:13,156 - root - INFO - KG Training: Epoch 0001 Iter 1845 / 3136 | Time 0.1s | Iter Loss 0.0806 | Iter Mean Loss 0.1936\n",
      "2024-11-14 04:42:13,215 - root - INFO - KG Training: Epoch 0001 Iter 1846 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1935\n",
      "2024-11-14 04:42:13,275 - root - INFO - KG Training: Epoch 0001 Iter 1847 / 3136 | Time 0.1s | Iter Loss 0.0801 | Iter Mean Loss 0.1935\n",
      "2024-11-14 04:42:13,452 - root - INFO - KG Training: Epoch 0001 Iter 1848 / 3136 | Time 0.2s | Iter Loss 0.0750 | Iter Mean Loss 0.1934\n",
      "2024-11-14 04:42:13,510 - root - INFO - KG Training: Epoch 0001 Iter 1849 / 3136 | Time 0.1s | Iter Loss 0.0811 | Iter Mean Loss 0.1934\n",
      "2024-11-14 04:42:13,568 - root - INFO - KG Training: Epoch 0001 Iter 1850 / 3136 | Time 0.1s | Iter Loss 0.0784 | Iter Mean Loss 0.1933\n",
      "2024-11-14 04:42:13,631 - root - INFO - KG Training: Epoch 0001 Iter 1851 / 3136 | Time 0.1s | Iter Loss 0.0765 | Iter Mean Loss 0.1932\n",
      "2024-11-14 04:42:13,687 - root - INFO - KG Training: Epoch 0001 Iter 1852 / 3136 | Time 0.1s | Iter Loss 0.0870 | Iter Mean Loss 0.1932\n",
      "2024-11-14 04:42:13,753 - root - INFO - KG Training: Epoch 0001 Iter 1853 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1931\n",
      "2024-11-14 04:42:13,934 - root - INFO - KG Training: Epoch 0001 Iter 1854 / 3136 | Time 0.2s | Iter Loss 0.0780 | Iter Mean Loss 0.1931\n",
      "2024-11-14 04:42:13,994 - root - INFO - KG Training: Epoch 0001 Iter 1855 / 3136 | Time 0.1s | Iter Loss 0.0697 | Iter Mean Loss 0.1930\n",
      "2024-11-14 04:42:14,051 - root - INFO - KG Training: Epoch 0001 Iter 1856 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1929\n",
      "2024-11-14 04:42:14,177 - root - INFO - KG Training: Epoch 0001 Iter 1857 / 3136 | Time 0.1s | Iter Loss 0.0878 | Iter Mean Loss 0.1929\n",
      "2024-11-14 04:42:14,238 - root - INFO - KG Training: Epoch 0001 Iter 1858 / 3136 | Time 0.1s | Iter Loss 0.0754 | Iter Mean Loss 0.1928\n",
      "2024-11-14 04:42:14,363 - root - INFO - KG Training: Epoch 0001 Iter 1859 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1927\n",
      "2024-11-14 04:42:14,425 - root - INFO - KG Training: Epoch 0001 Iter 1860 / 3136 | Time 0.1s | Iter Loss 0.0789 | Iter Mean Loss 0.1927\n",
      "2024-11-14 04:42:14,486 - root - INFO - KG Training: Epoch 0001 Iter 1861 / 3136 | Time 0.1s | Iter Loss 0.0869 | Iter Mean Loss 0.1926\n",
      "2024-11-14 04:42:14,544 - root - INFO - KG Training: Epoch 0001 Iter 1862 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1926\n",
      "2024-11-14 04:42:14,604 - root - INFO - KG Training: Epoch 0001 Iter 1863 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.1925\n",
      "2024-11-14 04:42:14,663 - root - INFO - KG Training: Epoch 0001 Iter 1864 / 3136 | Time 0.1s | Iter Loss 0.0848 | Iter Mean Loss 0.1924\n",
      "2024-11-14 04:42:14,730 - root - INFO - KG Training: Epoch 0001 Iter 1865 / 3136 | Time 0.1s | Iter Loss 0.0774 | Iter Mean Loss 0.1924\n",
      "2024-11-14 04:42:14,791 - root - INFO - KG Training: Epoch 0001 Iter 1866 / 3136 | Time 0.1s | Iter Loss 0.0872 | Iter Mean Loss 0.1923\n",
      "2024-11-14 04:42:14,851 - root - INFO - KG Training: Epoch 0001 Iter 1867 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1923\n",
      "2024-11-14 04:42:14,911 - root - INFO - KG Training: Epoch 0001 Iter 1868 / 3136 | Time 0.1s | Iter Loss 0.0769 | Iter Mean Loss 0.1922\n",
      "2024-11-14 04:42:14,974 - root - INFO - KG Training: Epoch 0001 Iter 1869 / 3136 | Time 0.1s | Iter Loss 0.0730 | Iter Mean Loss 0.1921\n",
      "2024-11-14 04:42:15,037 - root - INFO - KG Training: Epoch 0001 Iter 1870 / 3136 | Time 0.1s | Iter Loss 0.0804 | Iter Mean Loss 0.1921\n",
      "2024-11-14 04:42:15,098 - root - INFO - KG Training: Epoch 0001 Iter 1871 / 3136 | Time 0.1s | Iter Loss 0.0830 | Iter Mean Loss 0.1920\n",
      "2024-11-14 04:42:15,157 - root - INFO - KG Training: Epoch 0001 Iter 1872 / 3136 | Time 0.1s | Iter Loss 0.0693 | Iter Mean Loss 0.1920\n",
      "2024-11-14 04:42:15,224 - root - INFO - KG Training: Epoch 0001 Iter 1873 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1919\n",
      "2024-11-14 04:42:15,288 - root - INFO - KG Training: Epoch 0001 Iter 1874 / 3136 | Time 0.1s | Iter Loss 0.0835 | Iter Mean Loss 0.1918\n",
      "2024-11-14 04:42:15,347 - root - INFO - KG Training: Epoch 0001 Iter 1875 / 3136 | Time 0.1s | Iter Loss 0.0753 | Iter Mean Loss 0.1918\n",
      "2024-11-14 04:42:15,411 - root - INFO - KG Training: Epoch 0001 Iter 1876 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.1917\n",
      "2024-11-14 04:42:15,472 - root - INFO - KG Training: Epoch 0001 Iter 1877 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1917\n",
      "2024-11-14 04:42:15,533 - root - INFO - KG Training: Epoch 0001 Iter 1878 / 3136 | Time 0.1s | Iter Loss 0.0840 | Iter Mean Loss 0.1916\n",
      "2024-11-14 04:42:15,593 - root - INFO - KG Training: Epoch 0001 Iter 1879 / 3136 | Time 0.1s | Iter Loss 0.0759 | Iter Mean Loss 0.1915\n",
      "2024-11-14 04:42:15,653 - root - INFO - KG Training: Epoch 0001 Iter 1880 / 3136 | Time 0.1s | Iter Loss 0.0890 | Iter Mean Loss 0.1915\n",
      "2024-11-14 04:42:15,713 - root - INFO - KG Training: Epoch 0001 Iter 1881 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.1914\n",
      "2024-11-14 04:42:15,772 - root - INFO - KG Training: Epoch 0001 Iter 1882 / 3136 | Time 0.1s | Iter Loss 0.0909 | Iter Mean Loss 0.1914\n",
      "2024-11-14 04:42:15,827 - root - INFO - KG Training: Epoch 0001 Iter 1883 / 3136 | Time 0.1s | Iter Loss 0.0803 | Iter Mean Loss 0.1913\n",
      "2024-11-14 04:42:15,886 - root - INFO - KG Training: Epoch 0001 Iter 1884 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1912\n",
      "2024-11-14 04:42:15,945 - root - INFO - KG Training: Epoch 0001 Iter 1885 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1912\n",
      "2024-11-14 04:42:16,003 - root - INFO - KG Training: Epoch 0001 Iter 1886 / 3136 | Time 0.1s | Iter Loss 0.0805 | Iter Mean Loss 0.1911\n",
      "2024-11-14 04:42:16,061 - root - INFO - KG Training: Epoch 0001 Iter 1887 / 3136 | Time 0.1s | Iter Loss 0.0899 | Iter Mean Loss 0.1911\n",
      "2024-11-14 04:42:16,123 - root - INFO - KG Training: Epoch 0001 Iter 1888 / 3136 | Time 0.1s | Iter Loss 0.0772 | Iter Mean Loss 0.1910\n",
      "2024-11-14 04:42:16,193 - root - INFO - KG Training: Epoch 0001 Iter 1889 / 3136 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.1910\n",
      "2024-11-14 04:42:16,266 - root - INFO - KG Training: Epoch 0001 Iter 1890 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1909\n",
      "2024-11-14 04:42:16,373 - root - INFO - KG Training: Epoch 0001 Iter 1891 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1908\n",
      "2024-11-14 04:42:16,446 - root - INFO - KG Training: Epoch 0001 Iter 1892 / 3136 | Time 0.1s | Iter Loss 0.0900 | Iter Mean Loss 0.1908\n",
      "2024-11-14 04:42:16,521 - root - INFO - KG Training: Epoch 0001 Iter 1893 / 3136 | Time 0.1s | Iter Loss 0.0791 | Iter Mean Loss 0.1907\n",
      "2024-11-14 04:42:16,582 - root - INFO - KG Training: Epoch 0001 Iter 1894 / 3136 | Time 0.1s | Iter Loss 0.0893 | Iter Mean Loss 0.1907\n",
      "2024-11-14 04:42:16,692 - root - INFO - KG Training: Epoch 0001 Iter 1895 / 3136 | Time 0.1s | Iter Loss 0.0926 | Iter Mean Loss 0.1906\n",
      "2024-11-14 04:42:16,755 - root - INFO - KG Training: Epoch 0001 Iter 1896 / 3136 | Time 0.1s | Iter Loss 0.0704 | Iter Mean Loss 0.1906\n",
      "2024-11-14 04:42:16,817 - root - INFO - KG Training: Epoch 0001 Iter 1897 / 3136 | Time 0.1s | Iter Loss 0.0823 | Iter Mean Loss 0.1905\n",
      "2024-11-14 04:42:16,880 - root - INFO - KG Training: Epoch 0001 Iter 1898 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.1904\n",
      "2024-11-14 04:42:16,943 - root - INFO - KG Training: Epoch 0001 Iter 1899 / 3136 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.1904\n",
      "2024-11-14 04:42:17,003 - root - INFO - KG Training: Epoch 0001 Iter 1900 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1903\n",
      "2024-11-14 04:42:17,062 - root - INFO - KG Training: Epoch 0001 Iter 1901 / 3136 | Time 0.1s | Iter Loss 0.0785 | Iter Mean Loss 0.1903\n",
      "2024-11-14 04:42:17,120 - root - INFO - KG Training: Epoch 0001 Iter 1902 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1902\n",
      "2024-11-14 04:42:17,181 - root - INFO - KG Training: Epoch 0001 Iter 1903 / 3136 | Time 0.1s | Iter Loss 0.0886 | Iter Mean Loss 0.1901\n",
      "2024-11-14 04:42:17,240 - root - INFO - KG Training: Epoch 0001 Iter 1904 / 3136 | Time 0.1s | Iter Loss 0.0730 | Iter Mean Loss 0.1901\n",
      "2024-11-14 04:42:17,300 - root - INFO - KG Training: Epoch 0001 Iter 1905 / 3136 | Time 0.1s | Iter Loss 0.0779 | Iter Mean Loss 0.1900\n",
      "2024-11-14 04:42:17,358 - root - INFO - KG Training: Epoch 0001 Iter 1906 / 3136 | Time 0.1s | Iter Loss 0.0857 | Iter Mean Loss 0.1900\n",
      "2024-11-14 04:42:17,417 - root - INFO - KG Training: Epoch 0001 Iter 1907 / 3136 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.1899\n",
      "2024-11-14 04:42:17,478 - root - INFO - KG Training: Epoch 0001 Iter 1908 / 3136 | Time 0.1s | Iter Loss 0.0778 | Iter Mean Loss 0.1899\n",
      "2024-11-14 04:42:17,588 - root - INFO - KG Training: Epoch 0001 Iter 1909 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1898\n",
      "2024-11-14 04:42:17,648 - root - INFO - KG Training: Epoch 0001 Iter 1910 / 3136 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1897\n",
      "2024-11-14 04:42:17,706 - root - INFO - KG Training: Epoch 0001 Iter 1911 / 3136 | Time 0.1s | Iter Loss 0.0827 | Iter Mean Loss 0.1897\n",
      "2024-11-14 04:42:17,765 - root - INFO - KG Training: Epoch 0001 Iter 1912 / 3136 | Time 0.1s | Iter Loss 0.0792 | Iter Mean Loss 0.1896\n",
      "2024-11-14 04:42:17,824 - root - INFO - KG Training: Epoch 0001 Iter 1913 / 3136 | Time 0.1s | Iter Loss 0.0742 | Iter Mean Loss 0.1896\n",
      "2024-11-14 04:42:17,883 - root - INFO - KG Training: Epoch 0001 Iter 1914 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.1895\n",
      "2024-11-14 04:42:18,047 - root - INFO - KG Training: Epoch 0001 Iter 1915 / 3136 | Time 0.2s | Iter Loss 0.0669 | Iter Mean Loss 0.1894\n",
      "2024-11-14 04:42:18,110 - root - INFO - KG Training: Epoch 0001 Iter 1916 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1894\n",
      "2024-11-14 04:42:18,172 - root - INFO - KG Training: Epoch 0001 Iter 1917 / 3136 | Time 0.1s | Iter Loss 0.0848 | Iter Mean Loss 0.1893\n",
      "2024-11-14 04:42:18,233 - root - INFO - KG Training: Epoch 0001 Iter 1918 / 3136 | Time 0.1s | Iter Loss 0.0752 | Iter Mean Loss 0.1893\n",
      "2024-11-14 04:42:18,375 - root - INFO - KG Training: Epoch 0001 Iter 1919 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1892\n",
      "2024-11-14 04:42:18,436 - root - INFO - KG Training: Epoch 0001 Iter 1920 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1892\n",
      "2024-11-14 04:42:18,498 - root - INFO - KG Training: Epoch 0001 Iter 1921 / 3136 | Time 0.1s | Iter Loss 0.0754 | Iter Mean Loss 0.1891\n",
      "2024-11-14 04:42:18,554 - root - INFO - KG Training: Epoch 0001 Iter 1922 / 3136 | Time 0.1s | Iter Loss 0.0850 | Iter Mean Loss 0.1890\n",
      "2024-11-14 04:42:18,613 - root - INFO - KG Training: Epoch 0001 Iter 1923 / 3136 | Time 0.1s | Iter Loss 0.0786 | Iter Mean Loss 0.1890\n",
      "2024-11-14 04:42:18,673 - root - INFO - KG Training: Epoch 0001 Iter 1924 / 3136 | Time 0.1s | Iter Loss 0.0735 | Iter Mean Loss 0.1889\n",
      "2024-11-14 04:42:18,739 - root - INFO - KG Training: Epoch 0001 Iter 1925 / 3136 | Time 0.1s | Iter Loss 0.0809 | Iter Mean Loss 0.1889\n",
      "2024-11-14 04:42:18,799 - root - INFO - KG Training: Epoch 0001 Iter 1926 / 3136 | Time 0.1s | Iter Loss 0.0794 | Iter Mean Loss 0.1888\n",
      "2024-11-14 04:42:18,859 - root - INFO - KG Training: Epoch 0001 Iter 1927 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1888\n",
      "2024-11-14 04:42:18,919 - root - INFO - KG Training: Epoch 0001 Iter 1928 / 3136 | Time 0.1s | Iter Loss 0.0763 | Iter Mean Loss 0.1887\n",
      "2024-11-14 04:42:18,981 - root - INFO - KG Training: Epoch 0001 Iter 1929 / 3136 | Time 0.1s | Iter Loss 0.0774 | Iter Mean Loss 0.1886\n",
      "2024-11-14 04:42:19,038 - root - INFO - KG Training: Epoch 0001 Iter 1930 / 3136 | Time 0.1s | Iter Loss 0.0808 | Iter Mean Loss 0.1886\n",
      "2024-11-14 04:42:19,097 - root - INFO - KG Training: Epoch 0001 Iter 1931 / 3136 | Time 0.1s | Iter Loss 0.0692 | Iter Mean Loss 0.1885\n",
      "2024-11-14 04:42:19,161 - root - INFO - KG Training: Epoch 0001 Iter 1932 / 3136 | Time 0.1s | Iter Loss 0.0922 | Iter Mean Loss 0.1885\n",
      "2024-11-14 04:42:19,229 - root - INFO - KG Training: Epoch 0001 Iter 1933 / 3136 | Time 0.1s | Iter Loss 0.0844 | Iter Mean Loss 0.1884\n",
      "2024-11-14 04:42:19,285 - root - INFO - KG Training: Epoch 0001 Iter 1934 / 3136 | Time 0.1s | Iter Loss 0.0779 | Iter Mean Loss 0.1884\n",
      "2024-11-14 04:42:19,496 - root - INFO - KG Training: Epoch 0001 Iter 1935 / 3136 | Time 0.2s | Iter Loss 0.0822 | Iter Mean Loss 0.1883\n",
      "2024-11-14 04:42:19,556 - root - INFO - KG Training: Epoch 0001 Iter 1936 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1882\n",
      "2024-11-14 04:42:19,616 - root - INFO - KG Training: Epoch 0001 Iter 1937 / 3136 | Time 0.1s | Iter Loss 0.0707 | Iter Mean Loss 0.1882\n",
      "2024-11-14 04:42:19,675 - root - INFO - KG Training: Epoch 0001 Iter 1938 / 3136 | Time 0.1s | Iter Loss 0.0782 | Iter Mean Loss 0.1881\n",
      "2024-11-14 04:42:19,736 - root - INFO - KG Training: Epoch 0001 Iter 1939 / 3136 | Time 0.1s | Iter Loss 0.0895 | Iter Mean Loss 0.1881\n",
      "2024-11-14 04:42:19,796 - root - INFO - KG Training: Epoch 0001 Iter 1940 / 3136 | Time 0.1s | Iter Loss 0.0773 | Iter Mean Loss 0.1880\n",
      "2024-11-14 04:42:19,858 - root - INFO - KG Training: Epoch 0001 Iter 1941 / 3136 | Time 0.1s | Iter Loss 0.0786 | Iter Mean Loss 0.1880\n",
      "2024-11-14 04:42:19,922 - root - INFO - KG Training: Epoch 0001 Iter 1942 / 3136 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1879\n",
      "2024-11-14 04:42:19,980 - root - INFO - KG Training: Epoch 0001 Iter 1943 / 3136 | Time 0.1s | Iter Loss 0.0857 | Iter Mean Loss 0.1879\n",
      "2024-11-14 04:42:20,039 - root - INFO - KG Training: Epoch 0001 Iter 1944 / 3136 | Time 0.1s | Iter Loss 0.0734 | Iter Mean Loss 0.1878\n",
      "2024-11-14 04:42:20,102 - root - INFO - KG Training: Epoch 0001 Iter 1945 / 3136 | Time 0.1s | Iter Loss 0.0788 | Iter Mean Loss 0.1877\n",
      "2024-11-14 04:42:20,160 - root - INFO - KG Training: Epoch 0001 Iter 1946 / 3136 | Time 0.1s | Iter Loss 0.0845 | Iter Mean Loss 0.1877\n",
      "2024-11-14 04:42:20,220 - root - INFO - KG Training: Epoch 0001 Iter 1947 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1876\n",
      "2024-11-14 04:42:20,281 - root - INFO - KG Training: Epoch 0001 Iter 1948 / 3136 | Time 0.1s | Iter Loss 0.0815 | Iter Mean Loss 0.1876\n",
      "2024-11-14 04:42:20,432 - root - INFO - KG Training: Epoch 0001 Iter 1949 / 3136 | Time 0.2s | Iter Loss 0.0765 | Iter Mean Loss 0.1875\n",
      "2024-11-14 04:42:20,513 - root - INFO - KG Training: Epoch 0001 Iter 1950 / 3136 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1875\n",
      "2024-11-14 04:42:20,676 - root - INFO - KG Training: Epoch 0001 Iter 1951 / 3136 | Time 0.2s | Iter Loss 0.0760 | Iter Mean Loss 0.1874\n",
      "2024-11-14 04:42:20,736 - root - INFO - KG Training: Epoch 0001 Iter 1952 / 3136 | Time 0.1s | Iter Loss 0.0754 | Iter Mean Loss 0.1873\n",
      "2024-11-14 04:42:20,796 - root - INFO - KG Training: Epoch 0001 Iter 1953 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1873\n",
      "2024-11-14 04:42:20,856 - root - INFO - KG Training: Epoch 0001 Iter 1954 / 3136 | Time 0.1s | Iter Loss 0.0717 | Iter Mean Loss 0.1872\n",
      "2024-11-14 04:42:20,917 - root - INFO - KG Training: Epoch 0001 Iter 1955 / 3136 | Time 0.1s | Iter Loss 0.0756 | Iter Mean Loss 0.1872\n",
      "2024-11-14 04:42:20,978 - root - INFO - KG Training: Epoch 0001 Iter 1956 / 3136 | Time 0.1s | Iter Loss 0.0826 | Iter Mean Loss 0.1871\n",
      "2024-11-14 04:42:21,040 - root - INFO - KG Training: Epoch 0001 Iter 1957 / 3136 | Time 0.1s | Iter Loss 0.0733 | Iter Mean Loss 0.1871\n",
      "2024-11-14 04:42:21,109 - root - INFO - KG Training: Epoch 0001 Iter 1958 / 3136 | Time 0.1s | Iter Loss 0.0757 | Iter Mean Loss 0.1870\n",
      "2024-11-14 04:42:21,175 - root - INFO - KG Training: Epoch 0001 Iter 1959 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1869\n",
      "2024-11-14 04:42:21,242 - root - INFO - KG Training: Epoch 0001 Iter 1960 / 3136 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.1869\n",
      "2024-11-14 04:42:21,324 - root - INFO - KG Training: Epoch 0001 Iter 1961 / 3136 | Time 0.1s | Iter Loss 0.0797 | Iter Mean Loss 0.1868\n",
      "2024-11-14 04:42:21,382 - root - INFO - KG Training: Epoch 0001 Iter 1962 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.1868\n",
      "2024-11-14 04:42:21,440 - root - INFO - KG Training: Epoch 0001 Iter 1963 / 3136 | Time 0.1s | Iter Loss 0.0845 | Iter Mean Loss 0.1867\n",
      "2024-11-14 04:42:21,503 - root - INFO - KG Training: Epoch 0001 Iter 1964 / 3136 | Time 0.1s | Iter Loss 0.1039 | Iter Mean Loss 0.1867\n",
      "2024-11-14 04:42:21,560 - root - INFO - KG Training: Epoch 0001 Iter 1965 / 3136 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1866\n",
      "2024-11-14 04:42:21,619 - root - INFO - KG Training: Epoch 0001 Iter 1966 / 3136 | Time 0.1s | Iter Loss 0.0814 | Iter Mean Loss 0.1866\n",
      "2024-11-14 04:42:21,682 - root - INFO - KG Training: Epoch 0001 Iter 1967 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1865\n",
      "2024-11-14 04:42:21,759 - root - INFO - KG Training: Epoch 0001 Iter 1968 / 3136 | Time 0.1s | Iter Loss 0.0862 | Iter Mean Loss 0.1865\n",
      "2024-11-14 04:42:21,828 - root - INFO - KG Training: Epoch 0001 Iter 1969 / 3136 | Time 0.1s | Iter Loss 0.0864 | Iter Mean Loss 0.1864\n",
      "2024-11-14 04:42:21,897 - root - INFO - KG Training: Epoch 0001 Iter 1970 / 3136 | Time 0.1s | Iter Loss 0.0795 | Iter Mean Loss 0.1864\n",
      "2024-11-14 04:42:21,962 - root - INFO - KG Training: Epoch 0001 Iter 1971 / 3136 | Time 0.1s | Iter Loss 0.0719 | Iter Mean Loss 0.1863\n",
      "2024-11-14 04:42:22,025 - root - INFO - KG Training: Epoch 0001 Iter 1972 / 3136 | Time 0.1s | Iter Loss 0.0730 | Iter Mean Loss 0.1862\n",
      "2024-11-14 04:42:22,106 - root - INFO - KG Training: Epoch 0001 Iter 1973 / 3136 | Time 0.1s | Iter Loss 0.0775 | Iter Mean Loss 0.1862\n",
      "2024-11-14 04:42:22,163 - root - INFO - KG Training: Epoch 0001 Iter 1974 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1861\n",
      "2024-11-14 04:42:22,221 - root - INFO - KG Training: Epoch 0001 Iter 1975 / 3136 | Time 0.1s | Iter Loss 0.0782 | Iter Mean Loss 0.1861\n",
      "2024-11-14 04:42:22,280 - root - INFO - KG Training: Epoch 0001 Iter 1976 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1860\n",
      "2024-11-14 04:42:22,400 - root - INFO - KG Training: Epoch 0001 Iter 1977 / 3136 | Time 0.1s | Iter Loss 0.0773 | Iter Mean Loss 0.1860\n",
      "2024-11-14 04:42:22,458 - root - INFO - KG Training: Epoch 0001 Iter 1978 / 3136 | Time 0.1s | Iter Loss 0.0801 | Iter Mean Loss 0.1859\n",
      "2024-11-14 04:42:22,518 - root - INFO - KG Training: Epoch 0001 Iter 1979 / 3136 | Time 0.1s | Iter Loss 0.0791 | Iter Mean Loss 0.1859\n",
      "2024-11-14 04:42:22,578 - root - INFO - KG Training: Epoch 0001 Iter 1980 / 3136 | Time 0.1s | Iter Loss 0.0796 | Iter Mean Loss 0.1858\n",
      "2024-11-14 04:42:22,640 - root - INFO - KG Training: Epoch 0001 Iter 1981 / 3136 | Time 0.1s | Iter Loss 0.0880 | Iter Mean Loss 0.1858\n",
      "2024-11-14 04:42:22,698 - root - INFO - KG Training: Epoch 0001 Iter 1982 / 3136 | Time 0.1s | Iter Loss 0.0766 | Iter Mean Loss 0.1857\n",
      "2024-11-14 04:42:22,757 - root - INFO - KG Training: Epoch 0001 Iter 1983 / 3136 | Time 0.1s | Iter Loss 0.0752 | Iter Mean Loss 0.1856\n",
      "2024-11-14 04:42:22,816 - root - INFO - KG Training: Epoch 0001 Iter 1984 / 3136 | Time 0.1s | Iter Loss 0.0764 | Iter Mean Loss 0.1856\n",
      "2024-11-14 04:42:22,926 - root - INFO - KG Training: Epoch 0001 Iter 1985 / 3136 | Time 0.1s | Iter Loss 0.0769 | Iter Mean Loss 0.1855\n",
      "2024-11-14 04:42:22,989 - root - INFO - KG Training: Epoch 0001 Iter 1986 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1855\n",
      "2024-11-14 04:42:23,050 - root - INFO - KG Training: Epoch 0001 Iter 1987 / 3136 | Time 0.1s | Iter Loss 0.0760 | Iter Mean Loss 0.1854\n",
      "2024-11-14 04:42:23,110 - root - INFO - KG Training: Epoch 0001 Iter 1988 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.1854\n",
      "2024-11-14 04:42:23,172 - root - INFO - KG Training: Epoch 0001 Iter 1989 / 3136 | Time 0.1s | Iter Loss 0.0929 | Iter Mean Loss 0.1853\n",
      "2024-11-14 04:42:23,231 - root - INFO - KG Training: Epoch 0001 Iter 1990 / 3136 | Time 0.1s | Iter Loss 0.0773 | Iter Mean Loss 0.1853\n",
      "2024-11-14 04:42:23,294 - root - INFO - KG Training: Epoch 0001 Iter 1991 / 3136 | Time 0.1s | Iter Loss 0.0756 | Iter Mean Loss 0.1852\n",
      "2024-11-14 04:42:23,353 - root - INFO - KG Training: Epoch 0001 Iter 1992 / 3136 | Time 0.1s | Iter Loss 0.0771 | Iter Mean Loss 0.1852\n",
      "2024-11-14 04:42:23,413 - root - INFO - KG Training: Epoch 0001 Iter 1993 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1851\n",
      "2024-11-14 04:42:23,472 - root - INFO - KG Training: Epoch 0001 Iter 1994 / 3136 | Time 0.1s | Iter Loss 0.0850 | Iter Mean Loss 0.1850\n",
      "2024-11-14 04:42:23,532 - root - INFO - KG Training: Epoch 0001 Iter 1995 / 3136 | Time 0.1s | Iter Loss 0.0769 | Iter Mean Loss 0.1850\n",
      "2024-11-14 04:42:23,588 - root - INFO - KG Training: Epoch 0001 Iter 1996 / 3136 | Time 0.1s | Iter Loss 0.0737 | Iter Mean Loss 0.1849\n",
      "2024-11-14 04:42:23,650 - root - INFO - KG Training: Epoch 0001 Iter 1997 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.1849\n",
      "2024-11-14 04:42:23,708 - root - INFO - KG Training: Epoch 0001 Iter 1998 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1848\n",
      "2024-11-14 04:42:23,764 - root - INFO - KG Training: Epoch 0001 Iter 1999 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1848\n",
      "2024-11-14 04:42:23,822 - root - INFO - KG Training: Epoch 0001 Iter 2000 / 3136 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1847\n",
      "2024-11-14 04:42:23,880 - root - INFO - KG Training: Epoch 0001 Iter 2001 / 3136 | Time 0.1s | Iter Loss 0.0735 | Iter Mean Loss 0.1847\n",
      "2024-11-14 04:42:23,989 - root - INFO - KG Training: Epoch 0001 Iter 2002 / 3136 | Time 0.1s | Iter Loss 0.0757 | Iter Mean Loss 0.1846\n",
      "2024-11-14 04:42:24,117 - root - INFO - KG Training: Epoch 0001 Iter 2003 / 3136 | Time 0.1s | Iter Loss 0.0730 | Iter Mean Loss 0.1846\n",
      "2024-11-14 04:42:24,175 - root - INFO - KG Training: Epoch 0001 Iter 2004 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1845\n",
      "2024-11-14 04:42:24,236 - root - INFO - KG Training: Epoch 0001 Iter 2005 / 3136 | Time 0.1s | Iter Loss 0.0760 | Iter Mean Loss 0.1844\n",
      "2024-11-14 04:42:24,296 - root - INFO - KG Training: Epoch 0001 Iter 2006 / 3136 | Time 0.1s | Iter Loss 0.0766 | Iter Mean Loss 0.1844\n",
      "2024-11-14 04:42:24,356 - root - INFO - KG Training: Epoch 0001 Iter 2007 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.1843\n",
      "2024-11-14 04:42:24,511 - root - INFO - KG Training: Epoch 0001 Iter 2008 / 3136 | Time 0.2s | Iter Loss 0.0678 | Iter Mean Loss 0.1843\n",
      "2024-11-14 04:42:24,569 - root - INFO - KG Training: Epoch 0001 Iter 2009 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1842\n",
      "2024-11-14 04:42:24,628 - root - INFO - KG Training: Epoch 0001 Iter 2010 / 3136 | Time 0.1s | Iter Loss 0.0725 | Iter Mean Loss 0.1842\n",
      "2024-11-14 04:42:24,685 - root - INFO - KG Training: Epoch 0001 Iter 2011 / 3136 | Time 0.1s | Iter Loss 0.0702 | Iter Mean Loss 0.1841\n",
      "2024-11-14 04:42:24,743 - root - INFO - KG Training: Epoch 0001 Iter 2012 / 3136 | Time 0.1s | Iter Loss 0.0785 | Iter Mean Loss 0.1841\n",
      "2024-11-14 04:42:24,803 - root - INFO - KG Training: Epoch 0001 Iter 2013 / 3136 | Time 0.1s | Iter Loss 0.0739 | Iter Mean Loss 0.1840\n",
      "2024-11-14 04:42:24,860 - root - INFO - KG Training: Epoch 0001 Iter 2014 / 3136 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.1840\n",
      "2024-11-14 04:42:24,919 - root - INFO - KG Training: Epoch 0001 Iter 2015 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1839\n",
      "2024-11-14 04:42:24,978 - root - INFO - KG Training: Epoch 0001 Iter 2016 / 3136 | Time 0.1s | Iter Loss 0.0692 | Iter Mean Loss 0.1839\n",
      "2024-11-14 04:42:25,100 - root - INFO - KG Training: Epoch 0001 Iter 2017 / 3136 | Time 0.1s | Iter Loss 0.0762 | Iter Mean Loss 0.1838\n",
      "2024-11-14 04:42:25,160 - root - INFO - KG Training: Epoch 0001 Iter 2018 / 3136 | Time 0.1s | Iter Loss 0.0708 | Iter Mean Loss 0.1837\n",
      "2024-11-14 04:42:25,222 - root - INFO - KG Training: Epoch 0001 Iter 2019 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1837\n",
      "2024-11-14 04:42:25,281 - root - INFO - KG Training: Epoch 0001 Iter 2020 / 3136 | Time 0.1s | Iter Loss 0.0718 | Iter Mean Loss 0.1836\n",
      "2024-11-14 04:42:25,342 - root - INFO - KG Training: Epoch 0001 Iter 2021 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1836\n",
      "2024-11-14 04:42:25,401 - root - INFO - KG Training: Epoch 0001 Iter 2022 / 3136 | Time 0.1s | Iter Loss 0.0816 | Iter Mean Loss 0.1835\n",
      "2024-11-14 04:42:25,460 - root - INFO - KG Training: Epoch 0001 Iter 2023 / 3136 | Time 0.1s | Iter Loss 0.0790 | Iter Mean Loss 0.1835\n",
      "2024-11-14 04:42:25,521 - root - INFO - KG Training: Epoch 0001 Iter 2024 / 3136 | Time 0.1s | Iter Loss 0.0831 | Iter Mean Loss 0.1834\n",
      "2024-11-14 04:42:25,585 - root - INFO - KG Training: Epoch 0001 Iter 2025 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1834\n",
      "2024-11-14 04:42:25,646 - root - INFO - KG Training: Epoch 0001 Iter 2026 / 3136 | Time 0.1s | Iter Loss 0.0813 | Iter Mean Loss 0.1833\n",
      "2024-11-14 04:42:25,707 - root - INFO - KG Training: Epoch 0001 Iter 2027 / 3136 | Time 0.1s | Iter Loss 0.0697 | Iter Mean Loss 0.1833\n",
      "2024-11-14 04:42:25,775 - root - INFO - KG Training: Epoch 0001 Iter 2028 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1832\n",
      "2024-11-14 04:42:25,837 - root - INFO - KG Training: Epoch 0001 Iter 2029 / 3136 | Time 0.1s | Iter Loss 0.0756 | Iter Mean Loss 0.1832\n",
      "2024-11-14 04:42:25,900 - root - INFO - KG Training: Epoch 0001 Iter 2030 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1831\n",
      "2024-11-14 04:42:25,962 - root - INFO - KG Training: Epoch 0001 Iter 2031 / 3136 | Time 0.1s | Iter Loss 0.0752 | Iter Mean Loss 0.1831\n",
      "2024-11-14 04:42:26,023 - root - INFO - KG Training: Epoch 0001 Iter 2032 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.1830\n",
      "2024-11-14 04:42:26,083 - root - INFO - KG Training: Epoch 0001 Iter 2033 / 3136 | Time 0.1s | Iter Loss 0.0849 | Iter Mean Loss 0.1830\n",
      "2024-11-14 04:42:26,144 - root - INFO - KG Training: Epoch 0001 Iter 2034 / 3136 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.1829\n",
      "2024-11-14 04:42:26,206 - root - INFO - KG Training: Epoch 0001 Iter 2035 / 3136 | Time 0.1s | Iter Loss 0.0766 | Iter Mean Loss 0.1829\n",
      "2024-11-14 04:42:26,267 - root - INFO - KG Training: Epoch 0001 Iter 2036 / 3136 | Time 0.1s | Iter Loss 0.0626 | Iter Mean Loss 0.1828\n",
      "2024-11-14 04:42:26,333 - root - INFO - KG Training: Epoch 0001 Iter 2037 / 3136 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.1827\n",
      "2024-11-14 04:42:26,393 - root - INFO - KG Training: Epoch 0001 Iter 2038 / 3136 | Time 0.1s | Iter Loss 0.0754 | Iter Mean Loss 0.1827\n",
      "2024-11-14 04:42:26,456 - root - INFO - KG Training: Epoch 0001 Iter 2039 / 3136 | Time 0.1s | Iter Loss 0.0777 | Iter Mean Loss 0.1826\n",
      "2024-11-14 04:42:26,520 - root - INFO - KG Training: Epoch 0001 Iter 2040 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1826\n",
      "2024-11-14 04:42:26,582 - root - INFO - KG Training: Epoch 0001 Iter 2041 / 3136 | Time 0.1s | Iter Loss 0.0782 | Iter Mean Loss 0.1825\n",
      "2024-11-14 04:42:26,642 - root - INFO - KG Training: Epoch 0001 Iter 2042 / 3136 | Time 0.1s | Iter Loss 0.0831 | Iter Mean Loss 0.1825\n",
      "2024-11-14 04:42:26,700 - root - INFO - KG Training: Epoch 0001 Iter 2043 / 3136 | Time 0.1s | Iter Loss 0.0741 | Iter Mean Loss 0.1824\n",
      "2024-11-14 04:42:26,762 - root - INFO - KG Training: Epoch 0001 Iter 2044 / 3136 | Time 0.1s | Iter Loss 0.0790 | Iter Mean Loss 0.1824\n",
      "2024-11-14 04:42:26,823 - root - INFO - KG Training: Epoch 0001 Iter 2045 / 3136 | Time 0.1s | Iter Loss 0.0753 | Iter Mean Loss 0.1823\n",
      "2024-11-14 04:42:26,883 - root - INFO - KG Training: Epoch 0001 Iter 2046 / 3136 | Time 0.1s | Iter Loss 0.0908 | Iter Mean Loss 0.1823\n",
      "2024-11-14 04:42:26,946 - root - INFO - KG Training: Epoch 0001 Iter 2047 / 3136 | Time 0.1s | Iter Loss 0.0765 | Iter Mean Loss 0.1822\n",
      "2024-11-14 04:42:27,007 - root - INFO - KG Training: Epoch 0001 Iter 2048 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.1822\n",
      "2024-11-14 04:42:27,124 - root - INFO - KG Training: Epoch 0001 Iter 2049 / 3136 | Time 0.1s | Iter Loss 0.0733 | Iter Mean Loss 0.1821\n",
      "2024-11-14 04:42:27,183 - root - INFO - KG Training: Epoch 0001 Iter 2050 / 3136 | Time 0.1s | Iter Loss 0.0743 | Iter Mean Loss 0.1821\n",
      "2024-11-14 04:42:27,245 - root - INFO - KG Training: Epoch 0001 Iter 2051 / 3136 | Time 0.1s | Iter Loss 0.0727 | Iter Mean Loss 0.1820\n",
      "2024-11-14 04:42:27,310 - root - INFO - KG Training: Epoch 0001 Iter 2052 / 3136 | Time 0.1s | Iter Loss 0.0712 | Iter Mean Loss 0.1820\n",
      "2024-11-14 04:42:27,374 - root - INFO - KG Training: Epoch 0001 Iter 2053 / 3136 | Time 0.1s | Iter Loss 0.0758 | Iter Mean Loss 0.1819\n",
      "2024-11-14 04:42:27,434 - root - INFO - KG Training: Epoch 0001 Iter 2054 / 3136 | Time 0.1s | Iter Loss 0.0814 | Iter Mean Loss 0.1819\n",
      "2024-11-14 04:42:27,494 - root - INFO - KG Training: Epoch 0001 Iter 2055 / 3136 | Time 0.1s | Iter Loss 0.0749 | Iter Mean Loss 0.1818\n",
      "2024-11-14 04:42:27,552 - root - INFO - KG Training: Epoch 0001 Iter 2056 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1818\n",
      "2024-11-14 04:42:27,614 - root - INFO - KG Training: Epoch 0001 Iter 2057 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1817\n",
      "2024-11-14 04:42:27,675 - root - INFO - KG Training: Epoch 0001 Iter 2058 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1817\n",
      "2024-11-14 04:42:27,740 - root - INFO - KG Training: Epoch 0001 Iter 2059 / 3136 | Time 0.1s | Iter Loss 0.0758 | Iter Mean Loss 0.1816\n",
      "2024-11-14 04:42:27,803 - root - INFO - KG Training: Epoch 0001 Iter 2060 / 3136 | Time 0.1s | Iter Loss 0.0779 | Iter Mean Loss 0.1815\n",
      "2024-11-14 04:42:27,862 - root - INFO - KG Training: Epoch 0001 Iter 2061 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1815\n",
      "2024-11-14 04:42:28,053 - root - INFO - KG Training: Epoch 0001 Iter 2062 / 3136 | Time 0.2s | Iter Loss 0.0719 | Iter Mean Loss 0.1814\n",
      "2024-11-14 04:42:28,114 - root - INFO - KG Training: Epoch 0001 Iter 2063 / 3136 | Time 0.1s | Iter Loss 0.0818 | Iter Mean Loss 0.1814\n",
      "2024-11-14 04:42:28,175 - root - INFO - KG Training: Epoch 0001 Iter 2064 / 3136 | Time 0.1s | Iter Loss 0.0879 | Iter Mean Loss 0.1813\n",
      "2024-11-14 04:42:28,236 - root - INFO - KG Training: Epoch 0001 Iter 2065 / 3136 | Time 0.1s | Iter Loss 0.0692 | Iter Mean Loss 0.1813\n",
      "2024-11-14 04:42:28,298 - root - INFO - KG Training: Epoch 0001 Iter 2066 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1812\n",
      "2024-11-14 04:42:28,360 - root - INFO - KG Training: Epoch 0001 Iter 2067 / 3136 | Time 0.1s | Iter Loss 0.0697 | Iter Mean Loss 0.1812\n",
      "2024-11-14 04:42:28,423 - root - INFO - KG Training: Epoch 0001 Iter 2068 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1811\n",
      "2024-11-14 04:42:28,486 - root - INFO - KG Training: Epoch 0001 Iter 2069 / 3136 | Time 0.1s | Iter Loss 0.0747 | Iter Mean Loss 0.1811\n",
      "2024-11-14 04:42:28,549 - root - INFO - KG Training: Epoch 0001 Iter 2070 / 3136 | Time 0.1s | Iter Loss 0.0662 | Iter Mean Loss 0.1810\n",
      "2024-11-14 04:42:28,611 - root - INFO - KG Training: Epoch 0001 Iter 2071 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1810\n",
      "2024-11-14 04:42:28,677 - root - INFO - KG Training: Epoch 0001 Iter 2072 / 3136 | Time 0.1s | Iter Loss 0.0843 | Iter Mean Loss 0.1809\n",
      "2024-11-14 04:42:28,741 - root - INFO - KG Training: Epoch 0001 Iter 2073 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1809\n",
      "2024-11-14 04:42:28,802 - root - INFO - KG Training: Epoch 0001 Iter 2074 / 3136 | Time 0.1s | Iter Loss 0.0765 | Iter Mean Loss 0.1808\n",
      "2024-11-14 04:42:28,864 - root - INFO - KG Training: Epoch 0001 Iter 2075 / 3136 | Time 0.1s | Iter Loss 0.0775 | Iter Mean Loss 0.1808\n",
      "2024-11-14 04:42:28,943 - root - INFO - KG Training: Epoch 0001 Iter 2076 / 3136 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.1807\n",
      "2024-11-14 04:42:29,006 - root - INFO - KG Training: Epoch 0001 Iter 2077 / 3136 | Time 0.1s | Iter Loss 0.0791 | Iter Mean Loss 0.1807\n",
      "2024-11-14 04:42:29,066 - root - INFO - KG Training: Epoch 0001 Iter 2078 / 3136 | Time 0.1s | Iter Loss 0.0782 | Iter Mean Loss 0.1806\n",
      "2024-11-14 04:42:29,130 - root - INFO - KG Training: Epoch 0001 Iter 2079 / 3136 | Time 0.1s | Iter Loss 0.0849 | Iter Mean Loss 0.1806\n",
      "2024-11-14 04:42:29,193 - root - INFO - KG Training: Epoch 0001 Iter 2080 / 3136 | Time 0.1s | Iter Loss 0.0851 | Iter Mean Loss 0.1805\n",
      "2024-11-14 04:42:29,270 - root - INFO - KG Training: Epoch 0001 Iter 2081 / 3136 | Time 0.1s | Iter Loss 0.0720 | Iter Mean Loss 0.1805\n",
      "2024-11-14 04:42:29,337 - root - INFO - KG Training: Epoch 0001 Iter 2082 / 3136 | Time 0.1s | Iter Loss 0.0794 | Iter Mean Loss 0.1804\n",
      "2024-11-14 04:42:29,401 - root - INFO - KG Training: Epoch 0001 Iter 2083 / 3136 | Time 0.1s | Iter Loss 0.0780 | Iter Mean Loss 0.1804\n",
      "2024-11-14 04:42:29,461 - root - INFO - KG Training: Epoch 0001 Iter 2084 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.1803\n",
      "2024-11-14 04:42:29,523 - root - INFO - KG Training: Epoch 0001 Iter 2085 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.1803\n",
      "2024-11-14 04:42:29,584 - root - INFO - KG Training: Epoch 0001 Iter 2086 / 3136 | Time 0.1s | Iter Loss 0.0737 | Iter Mean Loss 0.1802\n",
      "2024-11-14 04:42:29,645 - root - INFO - KG Training: Epoch 0001 Iter 2087 / 3136 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1802\n",
      "2024-11-14 04:42:29,708 - root - INFO - KG Training: Epoch 0001 Iter 2088 / 3136 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1801\n",
      "2024-11-14 04:42:29,772 - root - INFO - KG Training: Epoch 0001 Iter 2089 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1801\n",
      "2024-11-14 04:42:29,834 - root - INFO - KG Training: Epoch 0001 Iter 2090 / 3136 | Time 0.1s | Iter Loss 0.0821 | Iter Mean Loss 0.1800\n",
      "2024-11-14 04:42:29,897 - root - INFO - KG Training: Epoch 0001 Iter 2091 / 3136 | Time 0.1s | Iter Loss 0.0766 | Iter Mean Loss 0.1800\n",
      "2024-11-14 04:42:29,958 - root - INFO - KG Training: Epoch 0001 Iter 2092 / 3136 | Time 0.1s | Iter Loss 0.0845 | Iter Mean Loss 0.1799\n",
      "2024-11-14 04:42:30,021 - root - INFO - KG Training: Epoch 0001 Iter 2093 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1799\n",
      "2024-11-14 04:42:30,083 - root - INFO - KG Training: Epoch 0001 Iter 2094 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1798\n",
      "2024-11-14 04:42:30,148 - root - INFO - KG Training: Epoch 0001 Iter 2095 / 3136 | Time 0.1s | Iter Loss 0.0742 | Iter Mean Loss 0.1798\n",
      "2024-11-14 04:42:30,218 - root - INFO - KG Training: Epoch 0001 Iter 2096 / 3136 | Time 0.1s | Iter Loss 0.0798 | Iter Mean Loss 0.1797\n",
      "2024-11-14 04:42:30,290 - root - INFO - KG Training: Epoch 0001 Iter 2097 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1797\n",
      "2024-11-14 04:42:30,369 - root - INFO - KG Training: Epoch 0001 Iter 2098 / 3136 | Time 0.1s | Iter Loss 0.0830 | Iter Mean Loss 0.1797\n",
      "2024-11-14 04:42:30,448 - root - INFO - KG Training: Epoch 0001 Iter 2099 / 3136 | Time 0.1s | Iter Loss 0.0758 | Iter Mean Loss 0.1796\n",
      "2024-11-14 04:42:30,526 - root - INFO - KG Training: Epoch 0001 Iter 2100 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1796\n",
      "2024-11-14 04:42:30,602 - root - INFO - KG Training: Epoch 0001 Iter 2101 / 3136 | Time 0.1s | Iter Loss 0.0825 | Iter Mean Loss 0.1795\n",
      "2024-11-14 04:42:30,668 - root - INFO - KG Training: Epoch 0001 Iter 2102 / 3136 | Time 0.1s | Iter Loss 0.0804 | Iter Mean Loss 0.1795\n",
      "2024-11-14 04:42:30,726 - root - INFO - KG Training: Epoch 0001 Iter 2103 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1794\n",
      "2024-11-14 04:42:30,838 - root - INFO - KG Training: Epoch 0001 Iter 2104 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1794\n",
      "2024-11-14 04:42:30,900 - root - INFO - KG Training: Epoch 0001 Iter 2105 / 3136 | Time 0.1s | Iter Loss 0.0751 | Iter Mean Loss 0.1793\n",
      "2024-11-14 04:42:30,962 - root - INFO - KG Training: Epoch 0001 Iter 2106 / 3136 | Time 0.1s | Iter Loss 0.0727 | Iter Mean Loss 0.1793\n",
      "2024-11-14 04:42:31,026 - root - INFO - KG Training: Epoch 0001 Iter 2107 / 3136 | Time 0.1s | Iter Loss 0.0788 | Iter Mean Loss 0.1792\n",
      "2024-11-14 04:42:31,107 - root - INFO - KG Training: Epoch 0001 Iter 2108 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1792\n",
      "2024-11-14 04:42:31,164 - root - INFO - KG Training: Epoch 0001 Iter 2109 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1791\n",
      "2024-11-14 04:42:31,240 - root - INFO - KG Training: Epoch 0001 Iter 2110 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1791\n",
      "2024-11-14 04:42:31,317 - root - INFO - KG Training: Epoch 0001 Iter 2111 / 3136 | Time 0.1s | Iter Loss 0.0782 | Iter Mean Loss 0.1790\n",
      "2024-11-14 04:42:31,537 - root - INFO - KG Training: Epoch 0001 Iter 2112 / 3136 | Time 0.2s | Iter Loss 0.0795 | Iter Mean Loss 0.1790\n",
      "2024-11-14 04:42:31,596 - root - INFO - KG Training: Epoch 0001 Iter 2113 / 3136 | Time 0.1s | Iter Loss 0.0778 | Iter Mean Loss 0.1789\n",
      "2024-11-14 04:42:31,671 - root - INFO - KG Training: Epoch 0001 Iter 2114 / 3136 | Time 0.1s | Iter Loss 0.0704 | Iter Mean Loss 0.1789\n",
      "2024-11-14 04:42:31,728 - root - INFO - KG Training: Epoch 0001 Iter 2115 / 3136 | Time 0.1s | Iter Loss 0.0753 | Iter Mean Loss 0.1788\n",
      "2024-11-14 04:42:31,787 - root - INFO - KG Training: Epoch 0001 Iter 2116 / 3136 | Time 0.1s | Iter Loss 0.0785 | Iter Mean Loss 0.1788\n",
      "2024-11-14 04:42:31,861 - root - INFO - KG Training: Epoch 0001 Iter 2117 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1787\n",
      "2024-11-14 04:42:31,932 - root - INFO - KG Training: Epoch 0001 Iter 2118 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1787\n",
      "2024-11-14 04:42:32,007 - root - INFO - KG Training: Epoch 0001 Iter 2119 / 3136 | Time 0.1s | Iter Loss 0.0830 | Iter Mean Loss 0.1786\n",
      "2024-11-14 04:42:32,064 - root - INFO - KG Training: Epoch 0001 Iter 2120 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1786\n",
      "2024-11-14 04:42:32,123 - root - INFO - KG Training: Epoch 0001 Iter 2121 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1785\n",
      "2024-11-14 04:42:32,198 - root - INFO - KG Training: Epoch 0001 Iter 2122 / 3136 | Time 0.1s | Iter Loss 0.0705 | Iter Mean Loss 0.1785\n",
      "2024-11-14 04:42:32,321 - root - INFO - KG Training: Epoch 0001 Iter 2123 / 3136 | Time 0.1s | Iter Loss 0.0687 | Iter Mean Loss 0.1784\n",
      "2024-11-14 04:42:32,382 - root - INFO - KG Training: Epoch 0001 Iter 2124 / 3136 | Time 0.1s | Iter Loss 0.0801 | Iter Mean Loss 0.1784\n",
      "2024-11-14 04:42:32,443 - root - INFO - KG Training: Epoch 0001 Iter 2125 / 3136 | Time 0.1s | Iter Loss 0.0817 | Iter Mean Loss 0.1783\n",
      "2024-11-14 04:42:32,506 - root - INFO - KG Training: Epoch 0001 Iter 2126 / 3136 | Time 0.1s | Iter Loss 0.0792 | Iter Mean Loss 0.1783\n",
      "2024-11-14 04:42:32,564 - root - INFO - KG Training: Epoch 0001 Iter 2127 / 3136 | Time 0.1s | Iter Loss 0.0765 | Iter Mean Loss 0.1782\n",
      "2024-11-14 04:42:32,674 - root - INFO - KG Training: Epoch 0001 Iter 2128 / 3136 | Time 0.1s | Iter Loss 0.0695 | Iter Mean Loss 0.1782\n",
      "2024-11-14 04:42:32,736 - root - INFO - KG Training: Epoch 0001 Iter 2129 / 3136 | Time 0.1s | Iter Loss 0.0657 | Iter Mean Loss 0.1781\n",
      "2024-11-14 04:42:32,799 - root - INFO - KG Training: Epoch 0001 Iter 2130 / 3136 | Time 0.1s | Iter Loss 0.0619 | Iter Mean Loss 0.1781\n",
      "2024-11-14 04:42:32,861 - root - INFO - KG Training: Epoch 0001 Iter 2131 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1780\n",
      "2024-11-14 04:42:32,923 - root - INFO - KG Training: Epoch 0001 Iter 2132 / 3136 | Time 0.1s | Iter Loss 0.0713 | Iter Mean Loss 0.1780\n",
      "2024-11-14 04:42:32,984 - root - INFO - KG Training: Epoch 0001 Iter 2133 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1779\n",
      "2024-11-14 04:42:33,047 - root - INFO - KG Training: Epoch 0001 Iter 2134 / 3136 | Time 0.1s | Iter Loss 0.0807 | Iter Mean Loss 0.1779\n",
      "2024-11-14 04:42:33,107 - root - INFO - KG Training: Epoch 0001 Iter 2135 / 3136 | Time 0.1s | Iter Loss 0.0686 | Iter Mean Loss 0.1778\n",
      "2024-11-14 04:42:33,170 - root - INFO - KG Training: Epoch 0001 Iter 2136 / 3136 | Time 0.1s | Iter Loss 0.0773 | Iter Mean Loss 0.1778\n",
      "2024-11-14 04:42:33,232 - root - INFO - KG Training: Epoch 0001 Iter 2137 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1777\n",
      "2024-11-14 04:42:33,296 - root - INFO - KG Training: Epoch 0001 Iter 2138 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1777\n",
      "2024-11-14 04:42:33,356 - root - INFO - KG Training: Epoch 0001 Iter 2139 / 3136 | Time 0.1s | Iter Loss 0.0747 | Iter Mean Loss 0.1776\n",
      "2024-11-14 04:42:33,418 - root - INFO - KG Training: Epoch 0001 Iter 2140 / 3136 | Time 0.1s | Iter Loss 0.0784 | Iter Mean Loss 0.1776\n",
      "2024-11-14 04:42:33,482 - root - INFO - KG Training: Epoch 0001 Iter 2141 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1775\n",
      "2024-11-14 04:42:33,542 - root - INFO - KG Training: Epoch 0001 Iter 2142 / 3136 | Time 0.1s | Iter Loss 0.0769 | Iter Mean Loss 0.1775\n",
      "2024-11-14 04:42:33,600 - root - INFO - KG Training: Epoch 0001 Iter 2143 / 3136 | Time 0.1s | Iter Loss 0.0734 | Iter Mean Loss 0.1774\n",
      "2024-11-14 04:42:33,660 - root - INFO - KG Training: Epoch 0001 Iter 2144 / 3136 | Time 0.1s | Iter Loss 0.0686 | Iter Mean Loss 0.1774\n",
      "2024-11-14 04:42:33,723 - root - INFO - KG Training: Epoch 0001 Iter 2145 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1773\n",
      "2024-11-14 04:42:33,789 - root - INFO - KG Training: Epoch 0001 Iter 2146 / 3136 | Time 0.1s | Iter Loss 0.0744 | Iter Mean Loss 0.1773\n",
      "2024-11-14 04:42:33,854 - root - INFO - KG Training: Epoch 0001 Iter 2147 / 3136 | Time 0.1s | Iter Loss 0.0753 | Iter Mean Loss 0.1772\n",
      "2024-11-14 04:42:33,916 - root - INFO - KG Training: Epoch 0001 Iter 2148 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1772\n",
      "2024-11-14 04:42:33,978 - root - INFO - KG Training: Epoch 0001 Iter 2149 / 3136 | Time 0.1s | Iter Loss 0.0744 | Iter Mean Loss 0.1771\n",
      "2024-11-14 04:42:34,041 - root - INFO - KG Training: Epoch 0001 Iter 2150 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1771\n",
      "2024-11-14 04:42:34,104 - root - INFO - KG Training: Epoch 0001 Iter 2151 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1771\n",
      "2024-11-14 04:42:34,218 - root - INFO - KG Training: Epoch 0001 Iter 2152 / 3136 | Time 0.1s | Iter Loss 0.0827 | Iter Mean Loss 0.1770\n",
      "2024-11-14 04:42:34,280 - root - INFO - KG Training: Epoch 0001 Iter 2153 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1770\n",
      "2024-11-14 04:42:34,341 - root - INFO - KG Training: Epoch 0001 Iter 2154 / 3136 | Time 0.1s | Iter Loss 0.0689 | Iter Mean Loss 0.1769\n",
      "2024-11-14 04:42:34,405 - root - INFO - KG Training: Epoch 0001 Iter 2155 / 3136 | Time 0.1s | Iter Loss 0.0778 | Iter Mean Loss 0.1769\n",
      "2024-11-14 04:42:34,467 - root - INFO - KG Training: Epoch 0001 Iter 2156 / 3136 | Time 0.1s | Iter Loss 0.0742 | Iter Mean Loss 0.1768\n",
      "2024-11-14 04:42:34,579 - root - INFO - KG Training: Epoch 0001 Iter 2157 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1768\n",
      "2024-11-14 04:42:34,640 - root - INFO - KG Training: Epoch 0001 Iter 2158 / 3136 | Time 0.1s | Iter Loss 0.0750 | Iter Mean Loss 0.1767\n",
      "2024-11-14 04:42:34,702 - root - INFO - KG Training: Epoch 0001 Iter 2159 / 3136 | Time 0.1s | Iter Loss 0.0688 | Iter Mean Loss 0.1767\n",
      "2024-11-14 04:42:34,763 - root - INFO - KG Training: Epoch 0001 Iter 2160 / 3136 | Time 0.1s | Iter Loss 0.0613 | Iter Mean Loss 0.1766\n",
      "2024-11-14 04:42:34,829 - root - INFO - KG Training: Epoch 0001 Iter 2161 / 3136 | Time 0.1s | Iter Loss 0.0661 | Iter Mean Loss 0.1766\n",
      "2024-11-14 04:42:34,891 - root - INFO - KG Training: Epoch 0001 Iter 2162 / 3136 | Time 0.1s | Iter Loss 0.0686 | Iter Mean Loss 0.1765\n",
      "2024-11-14 04:42:34,951 - root - INFO - KG Training: Epoch 0001 Iter 2163 / 3136 | Time 0.1s | Iter Loss 0.0733 | Iter Mean Loss 0.1765\n",
      "2024-11-14 04:42:35,010 - root - INFO - KG Training: Epoch 0001 Iter 2164 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1764\n",
      "2024-11-14 04:42:35,074 - root - INFO - KG Training: Epoch 0001 Iter 2165 / 3136 | Time 0.1s | Iter Loss 0.0694 | Iter Mean Loss 0.1764\n",
      "2024-11-14 04:42:35,137 - root - INFO - KG Training: Epoch 0001 Iter 2166 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1763\n",
      "2024-11-14 04:42:35,200 - root - INFO - KG Training: Epoch 0001 Iter 2167 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1763\n",
      "2024-11-14 04:42:35,258 - root - INFO - KG Training: Epoch 0001 Iter 2168 / 3136 | Time 0.1s | Iter Loss 0.0783 | Iter Mean Loss 0.1762\n",
      "2024-11-14 04:42:35,329 - root - INFO - KG Training: Epoch 0001 Iter 2169 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1762\n",
      "2024-11-14 04:42:35,394 - root - INFO - KG Training: Epoch 0001 Iter 2170 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1761\n",
      "2024-11-14 04:42:35,458 - root - INFO - KG Training: Epoch 0001 Iter 2171 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1761\n",
      "2024-11-14 04:42:35,532 - root - INFO - KG Training: Epoch 0001 Iter 2172 / 3136 | Time 0.1s | Iter Loss 0.0752 | Iter Mean Loss 0.1760\n",
      "2024-11-14 04:42:35,600 - root - INFO - KG Training: Epoch 0001 Iter 2173 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1760\n",
      "2024-11-14 04:42:35,714 - root - INFO - KG Training: Epoch 0001 Iter 2174 / 3136 | Time 0.1s | Iter Loss 0.0812 | Iter Mean Loss 0.1759\n",
      "2024-11-14 04:42:35,775 - root - INFO - KG Training: Epoch 0001 Iter 2175 / 3136 | Time 0.1s | Iter Loss 0.0740 | Iter Mean Loss 0.1759\n",
      "2024-11-14 04:42:35,835 - root - INFO - KG Training: Epoch 0001 Iter 2176 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1758\n",
      "2024-11-14 04:42:35,897 - root - INFO - KG Training: Epoch 0001 Iter 2177 / 3136 | Time 0.1s | Iter Loss 0.0852 | Iter Mean Loss 0.1758\n",
      "2024-11-14 04:42:35,959 - root - INFO - KG Training: Epoch 0001 Iter 2178 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1758\n",
      "2024-11-14 04:42:36,023 - root - INFO - KG Training: Epoch 0001 Iter 2179 / 3136 | Time 0.1s | Iter Loss 0.0772 | Iter Mean Loss 0.1757\n",
      "2024-11-14 04:42:36,085 - root - INFO - KG Training: Epoch 0001 Iter 2180 / 3136 | Time 0.1s | Iter Loss 0.0708 | Iter Mean Loss 0.1757\n",
      "2024-11-14 04:42:36,142 - root - INFO - KG Training: Epoch 0001 Iter 2181 / 3136 | Time 0.1s | Iter Loss 0.0868 | Iter Mean Loss 0.1756\n",
      "2024-11-14 04:42:36,204 - root - INFO - KG Training: Epoch 0001 Iter 2182 / 3136 | Time 0.1s | Iter Loss 0.0670 | Iter Mean Loss 0.1756\n",
      "2024-11-14 04:42:36,263 - root - INFO - KG Training: Epoch 0001 Iter 2183 / 3136 | Time 0.1s | Iter Loss 0.0750 | Iter Mean Loss 0.1755\n",
      "2024-11-14 04:42:36,325 - root - INFO - KG Training: Epoch 0001 Iter 2184 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1755\n",
      "2024-11-14 04:42:36,387 - root - INFO - KG Training: Epoch 0001 Iter 2185 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1754\n",
      "2024-11-14 04:42:36,449 - root - INFO - KG Training: Epoch 0001 Iter 2186 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1754\n",
      "2024-11-14 04:42:36,512 - root - INFO - KG Training: Epoch 0001 Iter 2187 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1753\n",
      "2024-11-14 04:42:36,574 - root - INFO - KG Training: Epoch 0001 Iter 2188 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1753\n",
      "2024-11-14 04:42:36,635 - root - INFO - KG Training: Epoch 0001 Iter 2189 / 3136 | Time 0.1s | Iter Loss 0.0691 | Iter Mean Loss 0.1752\n",
      "2024-11-14 04:42:36,696 - root - INFO - KG Training: Epoch 0001 Iter 2190 / 3136 | Time 0.1s | Iter Loss 0.0778 | Iter Mean Loss 0.1752\n",
      "2024-11-14 04:42:36,757 - root - INFO - KG Training: Epoch 0001 Iter 2191 / 3136 | Time 0.1s | Iter Loss 0.0788 | Iter Mean Loss 0.1751\n",
      "2024-11-14 04:42:36,819 - root - INFO - KG Training: Epoch 0001 Iter 2192 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1751\n",
      "2024-11-14 04:42:36,877 - root - INFO - KG Training: Epoch 0001 Iter 2193 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1751\n",
      "2024-11-14 04:42:36,936 - root - INFO - KG Training: Epoch 0001 Iter 2194 / 3136 | Time 0.1s | Iter Loss 0.0686 | Iter Mean Loss 0.1750\n",
      "2024-11-14 04:42:36,996 - root - INFO - KG Training: Epoch 0001 Iter 2195 / 3136 | Time 0.1s | Iter Loss 0.0810 | Iter Mean Loss 0.1750\n",
      "2024-11-14 04:42:37,056 - root - INFO - KG Training: Epoch 0001 Iter 2196 / 3136 | Time 0.1s | Iter Loss 0.0735 | Iter Mean Loss 0.1749\n",
      "2024-11-14 04:42:37,116 - root - INFO - KG Training: Epoch 0001 Iter 2197 / 3136 | Time 0.1s | Iter Loss 0.0637 | Iter Mean Loss 0.1749\n",
      "2024-11-14 04:42:37,175 - root - INFO - KG Training: Epoch 0001 Iter 2198 / 3136 | Time 0.1s | Iter Loss 0.0692 | Iter Mean Loss 0.1748\n",
      "2024-11-14 04:42:37,242 - root - INFO - KG Training: Epoch 0001 Iter 2199 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1748\n",
      "2024-11-14 04:42:37,304 - root - INFO - KG Training: Epoch 0001 Iter 2200 / 3136 | Time 0.1s | Iter Loss 0.0663 | Iter Mean Loss 0.1747\n",
      "2024-11-14 04:42:37,365 - root - INFO - KG Training: Epoch 0001 Iter 2201 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1747\n",
      "2024-11-14 04:42:37,426 - root - INFO - KG Training: Epoch 0001 Iter 2202 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1746\n",
      "2024-11-14 04:42:37,488 - root - INFO - KG Training: Epoch 0001 Iter 2203 / 3136 | Time 0.1s | Iter Loss 0.0684 | Iter Mean Loss 0.1746\n",
      "2024-11-14 04:42:37,551 - root - INFO - KG Training: Epoch 0001 Iter 2204 / 3136 | Time 0.1s | Iter Loss 0.0786 | Iter Mean Loss 0.1745\n",
      "2024-11-14 04:42:37,612 - root - INFO - KG Training: Epoch 0001 Iter 2205 / 3136 | Time 0.1s | Iter Loss 0.0642 | Iter Mean Loss 0.1745\n",
      "2024-11-14 04:42:37,674 - root - INFO - KG Training: Epoch 0001 Iter 2206 / 3136 | Time 0.1s | Iter Loss 0.0731 | Iter Mean Loss 0.1744\n",
      "2024-11-14 04:42:37,737 - root - INFO - KG Training: Epoch 0001 Iter 2207 / 3136 | Time 0.1s | Iter Loss 0.0784 | Iter Mean Loss 0.1744\n",
      "2024-11-14 04:42:37,798 - root - INFO - KG Training: Epoch 0001 Iter 2208 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1743\n",
      "2024-11-14 04:42:37,870 - root - INFO - KG Training: Epoch 0001 Iter 2209 / 3136 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1743\n",
      "2024-11-14 04:42:37,947 - root - INFO - KG Training: Epoch 0001 Iter 2210 / 3136 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.1743\n",
      "2024-11-14 04:42:38,025 - root - INFO - KG Training: Epoch 0001 Iter 2211 / 3136 | Time 0.1s | Iter Loss 0.0684 | Iter Mean Loss 0.1742\n",
      "2024-11-14 04:42:38,100 - root - INFO - KG Training: Epoch 0001 Iter 2212 / 3136 | Time 0.1s | Iter Loss 0.0763 | Iter Mean Loss 0.1742\n",
      "2024-11-14 04:42:38,158 - root - INFO - KG Training: Epoch 0001 Iter 2213 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1741\n",
      "2024-11-14 04:42:38,215 - root - INFO - KG Training: Epoch 0001 Iter 2214 / 3136 | Time 0.1s | Iter Loss 0.0636 | Iter Mean Loss 0.1741\n",
      "2024-11-14 04:42:38,294 - root - INFO - KG Training: Epoch 0001 Iter 2215 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1740\n",
      "2024-11-14 04:42:38,355 - root - INFO - KG Training: Epoch 0001 Iter 2216 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1740\n",
      "2024-11-14 04:42:38,418 - root - INFO - KG Training: Epoch 0001 Iter 2217 / 3136 | Time 0.1s | Iter Loss 0.0734 | Iter Mean Loss 0.1739\n",
      "2024-11-14 04:42:38,480 - root - INFO - KG Training: Epoch 0001 Iter 2218 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1739\n",
      "2024-11-14 04:42:38,540 - root - INFO - KG Training: Epoch 0001 Iter 2219 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1738\n",
      "2024-11-14 04:42:38,598 - root - INFO - KG Training: Epoch 0001 Iter 2220 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1738\n",
      "2024-11-14 04:42:38,656 - root - INFO - KG Training: Epoch 0001 Iter 2221 / 3136 | Time 0.1s | Iter Loss 0.0712 | Iter Mean Loss 0.1737\n",
      "2024-11-14 04:42:38,716 - root - INFO - KG Training: Epoch 0001 Iter 2222 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1737\n",
      "2024-11-14 04:42:38,781 - root - INFO - KG Training: Epoch 0001 Iter 2223 / 3136 | Time 0.1s | Iter Loss 0.0677 | Iter Mean Loss 0.1736\n",
      "2024-11-14 04:42:38,844 - root - INFO - KG Training: Epoch 0001 Iter 2224 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1736\n",
      "2024-11-14 04:42:38,907 - root - INFO - KG Training: Epoch 0001 Iter 2225 / 3136 | Time 0.1s | Iter Loss 0.0735 | Iter Mean Loss 0.1736\n",
      "2024-11-14 04:42:38,973 - root - INFO - KG Training: Epoch 0001 Iter 2226 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1735\n",
      "2024-11-14 04:42:39,037 - root - INFO - KG Training: Epoch 0001 Iter 2227 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1735\n",
      "2024-11-14 04:42:39,100 - root - INFO - KG Training: Epoch 0001 Iter 2228 / 3136 | Time 0.1s | Iter Loss 0.0671 | Iter Mean Loss 0.1734\n",
      "2024-11-14 04:42:39,188 - root - INFO - KG Training: Epoch 0001 Iter 2229 / 3136 | Time 0.1s | Iter Loss 0.0734 | Iter Mean Loss 0.1734\n",
      "2024-11-14 04:42:39,249 - root - INFO - KG Training: Epoch 0001 Iter 2230 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1733\n",
      "2024-11-14 04:42:39,308 - root - INFO - KG Training: Epoch 0001 Iter 2231 / 3136 | Time 0.1s | Iter Loss 0.0869 | Iter Mean Loss 0.1733\n",
      "2024-11-14 04:42:39,375 - root - INFO - KG Training: Epoch 0001 Iter 2232 / 3136 | Time 0.1s | Iter Loss 0.0777 | Iter Mean Loss 0.1732\n",
      "2024-11-14 04:42:39,438 - root - INFO - KG Training: Epoch 0001 Iter 2233 / 3136 | Time 0.1s | Iter Loss 0.0749 | Iter Mean Loss 0.1732\n",
      "2024-11-14 04:42:39,501 - root - INFO - KG Training: Epoch 0001 Iter 2234 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1732\n",
      "2024-11-14 04:42:39,563 - root - INFO - KG Training: Epoch 0001 Iter 2235 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1731\n",
      "2024-11-14 04:42:39,623 - root - INFO - KG Training: Epoch 0001 Iter 2236 / 3136 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.1731\n",
      "2024-11-14 04:42:39,692 - root - INFO - KG Training: Epoch 0001 Iter 2237 / 3136 | Time 0.1s | Iter Loss 0.0725 | Iter Mean Loss 0.1730\n",
      "2024-11-14 04:42:39,754 - root - INFO - KG Training: Epoch 0001 Iter 2238 / 3136 | Time 0.1s | Iter Loss 0.0652 | Iter Mean Loss 0.1730\n",
      "2024-11-14 04:42:39,867 - root - INFO - KG Training: Epoch 0001 Iter 2239 / 3136 | Time 0.1s | Iter Loss 0.0737 | Iter Mean Loss 0.1729\n",
      "2024-11-14 04:42:39,930 - root - INFO - KG Training: Epoch 0001 Iter 2240 / 3136 | Time 0.1s | Iter Loss 0.0694 | Iter Mean Loss 0.1729\n",
      "2024-11-14 04:42:39,992 - root - INFO - KG Training: Epoch 0001 Iter 2241 / 3136 | Time 0.1s | Iter Loss 0.0657 | Iter Mean Loss 0.1728\n",
      "2024-11-14 04:42:40,054 - root - INFO - KG Training: Epoch 0001 Iter 2242 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1728\n",
      "2024-11-14 04:42:40,123 - root - INFO - KG Training: Epoch 0001 Iter 2243 / 3136 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1727\n",
      "2024-11-14 04:42:40,189 - root - INFO - KG Training: Epoch 0001 Iter 2244 / 3136 | Time 0.1s | Iter Loss 0.0704 | Iter Mean Loss 0.1727\n",
      "2024-11-14 04:42:40,252 - root - INFO - KG Training: Epoch 0001 Iter 2245 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1727\n",
      "2024-11-14 04:42:40,314 - root - INFO - KG Training: Epoch 0001 Iter 2246 / 3136 | Time 0.1s | Iter Loss 0.0720 | Iter Mean Loss 0.1726\n",
      "2024-11-14 04:42:40,381 - root - INFO - KG Training: Epoch 0001 Iter 2247 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1726\n",
      "2024-11-14 04:42:40,444 - root - INFO - KG Training: Epoch 0001 Iter 2248 / 3136 | Time 0.1s | Iter Loss 0.0742 | Iter Mean Loss 0.1725\n",
      "2024-11-14 04:42:40,508 - root - INFO - KG Training: Epoch 0001 Iter 2249 / 3136 | Time 0.1s | Iter Loss 0.0786 | Iter Mean Loss 0.1725\n",
      "2024-11-14 04:42:40,676 - root - INFO - KG Training: Epoch 0001 Iter 2250 / 3136 | Time 0.2s | Iter Loss 0.0684 | Iter Mean Loss 0.1724\n",
      "2024-11-14 04:42:40,740 - root - INFO - KG Training: Epoch 0001 Iter 2251 / 3136 | Time 0.1s | Iter Loss 0.0771 | Iter Mean Loss 0.1724\n",
      "2024-11-14 04:42:40,805 - root - INFO - KG Training: Epoch 0001 Iter 2252 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1723\n",
      "2024-11-14 04:42:40,863 - root - INFO - KG Training: Epoch 0001 Iter 2253 / 3136 | Time 0.1s | Iter Loss 0.0794 | Iter Mean Loss 0.1723\n",
      "2024-11-14 04:42:40,929 - root - INFO - KG Training: Epoch 0001 Iter 2254 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1723\n",
      "2024-11-14 04:42:41,001 - root - INFO - KG Training: Epoch 0001 Iter 2255 / 3136 | Time 0.1s | Iter Loss 0.0705 | Iter Mean Loss 0.1722\n",
      "2024-11-14 04:42:41,059 - root - INFO - KG Training: Epoch 0001 Iter 2256 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.1722\n",
      "2024-11-14 04:42:41,120 - root - INFO - KG Training: Epoch 0001 Iter 2257 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1721\n",
      "2024-11-14 04:42:41,182 - root - INFO - KG Training: Epoch 0001 Iter 2258 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1721\n",
      "2024-11-14 04:42:41,240 - root - INFO - KG Training: Epoch 0001 Iter 2259 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1720\n",
      "2024-11-14 04:42:41,300 - root - INFO - KG Training: Epoch 0001 Iter 2260 / 3136 | Time 0.1s | Iter Loss 0.0727 | Iter Mean Loss 0.1720\n",
      "2024-11-14 04:42:41,359 - root - INFO - KG Training: Epoch 0001 Iter 2261 / 3136 | Time 0.1s | Iter Loss 0.0719 | Iter Mean Loss 0.1719\n",
      "2024-11-14 04:42:41,418 - root - INFO - KG Training: Epoch 0001 Iter 2262 / 3136 | Time 0.1s | Iter Loss 0.0770 | Iter Mean Loss 0.1719\n",
      "2024-11-14 04:42:41,479 - root - INFO - KG Training: Epoch 0001 Iter 2263 / 3136 | Time 0.1s | Iter Loss 0.0680 | Iter Mean Loss 0.1719\n",
      "2024-11-14 04:42:41,616 - root - INFO - KG Training: Epoch 0001 Iter 2264 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1718\n",
      "2024-11-14 04:42:41,678 - root - INFO - KG Training: Epoch 0001 Iter 2265 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1718\n",
      "2024-11-14 04:42:41,740 - root - INFO - KG Training: Epoch 0001 Iter 2266 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1717\n",
      "2024-11-14 04:42:41,802 - root - INFO - KG Training: Epoch 0001 Iter 2267 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1717\n",
      "2024-11-14 04:42:41,865 - root - INFO - KG Training: Epoch 0001 Iter 2268 / 3136 | Time 0.1s | Iter Loss 0.0651 | Iter Mean Loss 0.1716\n",
      "2024-11-14 04:42:41,927 - root - INFO - KG Training: Epoch 0001 Iter 2269 / 3136 | Time 0.1s | Iter Loss 0.0780 | Iter Mean Loss 0.1716\n",
      "2024-11-14 04:42:42,043 - root - INFO - KG Training: Epoch 0001 Iter 2270 / 3136 | Time 0.1s | Iter Loss 0.0672 | Iter Mean Loss 0.1715\n",
      "2024-11-14 04:42:42,108 - root - INFO - KG Training: Epoch 0001 Iter 2271 / 3136 | Time 0.1s | Iter Loss 0.0763 | Iter Mean Loss 0.1715\n",
      "2024-11-14 04:42:42,168 - root - INFO - KG Training: Epoch 0001 Iter 2272 / 3136 | Time 0.1s | Iter Loss 0.0722 | Iter Mean Loss 0.1715\n",
      "2024-11-14 04:42:42,228 - root - INFO - KG Training: Epoch 0001 Iter 2273 / 3136 | Time 0.1s | Iter Loss 0.0668 | Iter Mean Loss 0.1714\n",
      "2024-11-14 04:42:42,288 - root - INFO - KG Training: Epoch 0001 Iter 2274 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1714\n",
      "2024-11-14 04:42:42,347 - root - INFO - KG Training: Epoch 0001 Iter 2275 / 3136 | Time 0.1s | Iter Loss 0.0684 | Iter Mean Loss 0.1713\n",
      "2024-11-14 04:42:42,407 - root - INFO - KG Training: Epoch 0001 Iter 2276 / 3136 | Time 0.1s | Iter Loss 0.0762 | Iter Mean Loss 0.1713\n",
      "2024-11-14 04:42:42,469 - root - INFO - KG Training: Epoch 0001 Iter 2277 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1712\n",
      "2024-11-14 04:42:42,530 - root - INFO - KG Training: Epoch 0001 Iter 2278 / 3136 | Time 0.1s | Iter Loss 0.0806 | Iter Mean Loss 0.1712\n",
      "2024-11-14 04:42:42,588 - root - INFO - KG Training: Epoch 0001 Iter 2279 / 3136 | Time 0.1s | Iter Loss 0.0774 | Iter Mean Loss 0.1711\n",
      "2024-11-14 04:42:42,647 - root - INFO - KG Training: Epoch 0001 Iter 2280 / 3136 | Time 0.1s | Iter Loss 0.0680 | Iter Mean Loss 0.1711\n",
      "2024-11-14 04:42:42,707 - root - INFO - KG Training: Epoch 0001 Iter 2281 / 3136 | Time 0.1s | Iter Loss 0.0714 | Iter Mean Loss 0.1711\n",
      "2024-11-14 04:42:42,767 - root - INFO - KG Training: Epoch 0001 Iter 2282 / 3136 | Time 0.1s | Iter Loss 0.0813 | Iter Mean Loss 0.1710\n",
      "2024-11-14 04:42:42,841 - root - INFO - KG Training: Epoch 0001 Iter 2283 / 3136 | Time 0.1s | Iter Loss 0.0718 | Iter Mean Loss 0.1710\n",
      "2024-11-14 04:42:42,911 - root - INFO - KG Training: Epoch 0001 Iter 2284 / 3136 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1709\n",
      "2024-11-14 04:42:42,976 - root - INFO - KG Training: Epoch 0001 Iter 2285 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1709\n",
      "2024-11-14 04:42:43,045 - root - INFO - KG Training: Epoch 0001 Iter 2286 / 3136 | Time 0.1s | Iter Loss 0.0787 | Iter Mean Loss 0.1708\n",
      "2024-11-14 04:42:43,300 - root - INFO - KG Training: Epoch 0001 Iter 2287 / 3136 | Time 0.3s | Iter Loss 0.0709 | Iter Mean Loss 0.1708\n",
      "2024-11-14 04:42:43,360 - root - INFO - KG Training: Epoch 0001 Iter 2288 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1708\n",
      "2024-11-14 04:42:43,423 - root - INFO - KG Training: Epoch 0001 Iter 2289 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1707\n",
      "2024-11-14 04:42:43,489 - root - INFO - KG Training: Epoch 0001 Iter 2290 / 3136 | Time 0.1s | Iter Loss 0.0606 | Iter Mean Loss 0.1707\n",
      "2024-11-14 04:42:43,549 - root - INFO - KG Training: Epoch 0001 Iter 2291 / 3136 | Time 0.1s | Iter Loss 0.0719 | Iter Mean Loss 0.1706\n",
      "2024-11-14 04:42:43,611 - root - INFO - KG Training: Epoch 0001 Iter 2292 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1706\n",
      "2024-11-14 04:42:43,672 - root - INFO - KG Training: Epoch 0001 Iter 2293 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1705\n",
      "2024-11-14 04:42:43,734 - root - INFO - KG Training: Epoch 0001 Iter 2294 / 3136 | Time 0.1s | Iter Loss 0.0751 | Iter Mean Loss 0.1705\n",
      "2024-11-14 04:42:43,801 - root - INFO - KG Training: Epoch 0001 Iter 2295 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1704\n",
      "2024-11-14 04:42:43,864 - root - INFO - KG Training: Epoch 0001 Iter 2296 / 3136 | Time 0.1s | Iter Loss 0.0768 | Iter Mean Loss 0.1704\n",
      "2024-11-14 04:42:43,926 - root - INFO - KG Training: Epoch 0001 Iter 2297 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1704\n",
      "2024-11-14 04:42:43,989 - root - INFO - KG Training: Epoch 0001 Iter 2298 / 3136 | Time 0.1s | Iter Loss 0.0756 | Iter Mean Loss 0.1703\n",
      "2024-11-14 04:42:44,049 - root - INFO - KG Training: Epoch 0001 Iter 2299 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.1703\n",
      "2024-11-14 04:42:44,107 - root - INFO - KG Training: Epoch 0001 Iter 2300 / 3136 | Time 0.1s | Iter Loss 0.0708 | Iter Mean Loss 0.1702\n",
      "2024-11-14 04:42:44,169 - root - INFO - KG Training: Epoch 0001 Iter 2301 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1702\n",
      "2024-11-14 04:42:44,272 - root - INFO - KG Training: Epoch 0001 Iter 2302 / 3136 | Time 0.1s | Iter Loss 0.0750 | Iter Mean Loss 0.1701\n",
      "2024-11-14 04:42:44,333 - root - INFO - KG Training: Epoch 0001 Iter 2303 / 3136 | Time 0.1s | Iter Loss 0.0642 | Iter Mean Loss 0.1701\n",
      "2024-11-14 04:42:44,394 - root - INFO - KG Training: Epoch 0001 Iter 2304 / 3136 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1701\n",
      "2024-11-14 04:42:44,458 - root - INFO - KG Training: Epoch 0001 Iter 2305 / 3136 | Time 0.1s | Iter Loss 0.0763 | Iter Mean Loss 0.1700\n",
      "2024-11-14 04:42:44,519 - root - INFO - KG Training: Epoch 0001 Iter 2306 / 3136 | Time 0.1s | Iter Loss 0.0842 | Iter Mean Loss 0.1700\n",
      "2024-11-14 04:42:44,579 - root - INFO - KG Training: Epoch 0001 Iter 2307 / 3136 | Time 0.1s | Iter Loss 0.0707 | Iter Mean Loss 0.1699\n",
      "2024-11-14 04:42:44,639 - root - INFO - KG Training: Epoch 0001 Iter 2308 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1699\n",
      "2024-11-14 04:42:44,698 - root - INFO - KG Training: Epoch 0001 Iter 2309 / 3136 | Time 0.1s | Iter Loss 0.0762 | Iter Mean Loss 0.1699\n",
      "2024-11-14 04:42:44,762 - root - INFO - KG Training: Epoch 0001 Iter 2310 / 3136 | Time 0.1s | Iter Loss 0.0687 | Iter Mean Loss 0.1698\n",
      "2024-11-14 04:42:44,826 - root - INFO - KG Training: Epoch 0001 Iter 2311 / 3136 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1698\n",
      "2024-11-14 04:42:44,883 - root - INFO - KG Training: Epoch 0001 Iter 2312 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1697\n",
      "2024-11-14 04:42:44,943 - root - INFO - KG Training: Epoch 0001 Iter 2313 / 3136 | Time 0.1s | Iter Loss 0.0617 | Iter Mean Loss 0.1697\n",
      "2024-11-14 04:42:45,003 - root - INFO - KG Training: Epoch 0001 Iter 2314 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1696\n",
      "2024-11-14 04:42:45,062 - root - INFO - KG Training: Epoch 0001 Iter 2315 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1696\n",
      "2024-11-14 04:42:45,125 - root - INFO - KG Training: Epoch 0001 Iter 2316 / 3136 | Time 0.1s | Iter Loss 0.0683 | Iter Mean Loss 0.1696\n",
      "2024-11-14 04:42:45,188 - root - INFO - KG Training: Epoch 0001 Iter 2317 / 3136 | Time 0.1s | Iter Loss 0.0708 | Iter Mean Loss 0.1695\n",
      "2024-11-14 04:42:45,250 - root - INFO - KG Training: Epoch 0001 Iter 2318 / 3136 | Time 0.1s | Iter Loss 0.0743 | Iter Mean Loss 0.1695\n",
      "2024-11-14 04:42:45,309 - root - INFO - KG Training: Epoch 0001 Iter 2319 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1694\n",
      "2024-11-14 04:42:45,369 - root - INFO - KG Training: Epoch 0001 Iter 2320 / 3136 | Time 0.1s | Iter Loss 0.0708 | Iter Mean Loss 0.1694\n",
      "2024-11-14 04:42:45,430 - root - INFO - KG Training: Epoch 0001 Iter 2321 / 3136 | Time 0.1s | Iter Loss 0.0607 | Iter Mean Loss 0.1693\n",
      "2024-11-14 04:42:45,491 - root - INFO - KG Training: Epoch 0001 Iter 2322 / 3136 | Time 0.1s | Iter Loss 0.0634 | Iter Mean Loss 0.1693\n",
      "2024-11-14 04:42:45,553 - root - INFO - KG Training: Epoch 0001 Iter 2323 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1692\n",
      "2024-11-14 04:42:45,619 - root - INFO - KG Training: Epoch 0001 Iter 2324 / 3136 | Time 0.1s | Iter Loss 0.0727 | Iter Mean Loss 0.1692\n",
      "2024-11-14 04:42:45,678 - root - INFO - KG Training: Epoch 0001 Iter 2325 / 3136 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.1692\n",
      "2024-11-14 04:42:45,740 - root - INFO - KG Training: Epoch 0001 Iter 2326 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1691\n",
      "2024-11-14 04:42:45,797 - root - INFO - KG Training: Epoch 0001 Iter 2327 / 3136 | Time 0.1s | Iter Loss 0.0722 | Iter Mean Loss 0.1691\n",
      "2024-11-14 04:42:45,858 - root - INFO - KG Training: Epoch 0001 Iter 2328 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1690\n",
      "2024-11-14 04:42:45,916 - root - INFO - KG Training: Epoch 0001 Iter 2329 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1690\n",
      "2024-11-14 04:42:45,976 - root - INFO - KG Training: Epoch 0001 Iter 2330 / 3136 | Time 0.1s | Iter Loss 0.0681 | Iter Mean Loss 0.1689\n",
      "2024-11-14 04:42:46,034 - root - INFO - KG Training: Epoch 0001 Iter 2331 / 3136 | Time 0.1s | Iter Loss 0.0687 | Iter Mean Loss 0.1689\n",
      "2024-11-14 04:42:46,151 - root - INFO - KG Training: Epoch 0001 Iter 2332 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1689\n",
      "2024-11-14 04:42:46,211 - root - INFO - KG Training: Epoch 0001 Iter 2333 / 3136 | Time 0.1s | Iter Loss 0.0763 | Iter Mean Loss 0.1688\n",
      "2024-11-14 04:42:46,272 - root - INFO - KG Training: Epoch 0001 Iter 2334 / 3136 | Time 0.1s | Iter Loss 0.0733 | Iter Mean Loss 0.1688\n",
      "2024-11-14 04:42:46,335 - root - INFO - KG Training: Epoch 0001 Iter 2335 / 3136 | Time 0.1s | Iter Loss 0.0740 | Iter Mean Loss 0.1687\n",
      "2024-11-14 04:42:46,397 - root - INFO - KG Training: Epoch 0001 Iter 2336 / 3136 | Time 0.1s | Iter Loss 0.0796 | Iter Mean Loss 0.1687\n",
      "2024-11-14 04:42:46,459 - root - INFO - KG Training: Epoch 0001 Iter 2337 / 3136 | Time 0.1s | Iter Loss 0.0625 | Iter Mean Loss 0.1687\n",
      "2024-11-14 04:42:46,520 - root - INFO - KG Training: Epoch 0001 Iter 2338 / 3136 | Time 0.1s | Iter Loss 0.0743 | Iter Mean Loss 0.1686\n",
      "2024-11-14 04:42:46,587 - root - INFO - KG Training: Epoch 0001 Iter 2339 / 3136 | Time 0.1s | Iter Loss 0.0694 | Iter Mean Loss 0.1686\n",
      "2024-11-14 04:42:46,648 - root - INFO - KG Training: Epoch 0001 Iter 2340 / 3136 | Time 0.1s | Iter Loss 0.0795 | Iter Mean Loss 0.1685\n",
      "2024-11-14 04:42:46,707 - root - INFO - KG Training: Epoch 0001 Iter 2341 / 3136 | Time 0.1s | Iter Loss 0.0642 | Iter Mean Loss 0.1685\n",
      "2024-11-14 04:42:46,770 - root - INFO - KG Training: Epoch 0001 Iter 2342 / 3136 | Time 0.1s | Iter Loss 0.0704 | Iter Mean Loss 0.1684\n",
      "2024-11-14 04:42:46,829 - root - INFO - KG Training: Epoch 0001 Iter 2343 / 3136 | Time 0.1s | Iter Loss 0.0643 | Iter Mean Loss 0.1684\n",
      "2024-11-14 04:42:46,891 - root - INFO - KG Training: Epoch 0001 Iter 2344 / 3136 | Time 0.1s | Iter Loss 0.0679 | Iter Mean Loss 0.1684\n",
      "2024-11-14 04:42:46,954 - root - INFO - KG Training: Epoch 0001 Iter 2345 / 3136 | Time 0.1s | Iter Loss 0.0609 | Iter Mean Loss 0.1683\n",
      "2024-11-14 04:42:47,017 - root - INFO - KG Training: Epoch 0001 Iter 2346 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1683\n",
      "2024-11-14 04:42:47,080 - root - INFO - KG Training: Epoch 0001 Iter 2347 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1682\n",
      "2024-11-14 04:42:47,142 - root - INFO - KG Training: Epoch 0001 Iter 2348 / 3136 | Time 0.1s | Iter Loss 0.0739 | Iter Mean Loss 0.1682\n",
      "2024-11-14 04:42:47,203 - root - INFO - KG Training: Epoch 0001 Iter 2349 / 3136 | Time 0.1s | Iter Loss 0.0713 | Iter Mean Loss 0.1681\n",
      "2024-11-14 04:42:47,342 - root - INFO - KG Training: Epoch 0001 Iter 2350 / 3136 | Time 0.1s | Iter Loss 0.0608 | Iter Mean Loss 0.1681\n",
      "2024-11-14 04:42:47,401 - root - INFO - KG Training: Epoch 0001 Iter 2351 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1681\n",
      "2024-11-14 04:42:47,460 - root - INFO - KG Training: Epoch 0001 Iter 2352 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1680\n",
      "2024-11-14 04:42:47,520 - root - INFO - KG Training: Epoch 0001 Iter 2353 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1680\n",
      "2024-11-14 04:42:47,580 - root - INFO - KG Training: Epoch 0001 Iter 2354 / 3136 | Time 0.1s | Iter Loss 0.0677 | Iter Mean Loss 0.1679\n",
      "2024-11-14 04:42:47,642 - root - INFO - KG Training: Epoch 0001 Iter 2355 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1679\n",
      "2024-11-14 04:42:47,702 - root - INFO - KG Training: Epoch 0001 Iter 2356 / 3136 | Time 0.1s | Iter Loss 0.0683 | Iter Mean Loss 0.1678\n",
      "2024-11-14 04:42:47,763 - root - INFO - KG Training: Epoch 0001 Iter 2357 / 3136 | Time 0.1s | Iter Loss 0.0653 | Iter Mean Loss 0.1678\n",
      "2024-11-14 04:42:47,823 - root - INFO - KG Training: Epoch 0001 Iter 2358 / 3136 | Time 0.1s | Iter Loss 0.0712 | Iter Mean Loss 0.1678\n",
      "2024-11-14 04:42:47,883 - root - INFO - KG Training: Epoch 0001 Iter 2359 / 3136 | Time 0.1s | Iter Loss 0.0720 | Iter Mean Loss 0.1677\n",
      "2024-11-14 04:42:47,943 - root - INFO - KG Training: Epoch 0001 Iter 2360 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.1677\n",
      "2024-11-14 04:42:48,005 - root - INFO - KG Training: Epoch 0001 Iter 2361 / 3136 | Time 0.1s | Iter Loss 0.0776 | Iter Mean Loss 0.1676\n",
      "2024-11-14 04:42:48,067 - root - INFO - KG Training: Epoch 0001 Iter 2362 / 3136 | Time 0.1s | Iter Loss 0.0809 | Iter Mean Loss 0.1676\n",
      "2024-11-14 04:42:48,130 - root - INFO - KG Training: Epoch 0001 Iter 2363 / 3136 | Time 0.1s | Iter Loss 0.0763 | Iter Mean Loss 0.1676\n",
      "2024-11-14 04:42:48,200 - root - INFO - KG Training: Epoch 0001 Iter 2364 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1675\n",
      "2024-11-14 04:42:48,261 - root - INFO - KG Training: Epoch 0001 Iter 2365 / 3136 | Time 0.1s | Iter Loss 0.0801 | Iter Mean Loss 0.1675\n",
      "2024-11-14 04:42:48,335 - root - INFO - KG Training: Epoch 0001 Iter 2366 / 3136 | Time 0.1s | Iter Loss 0.0804 | Iter Mean Loss 0.1674\n",
      "2024-11-14 04:42:48,391 - root - INFO - KG Training: Epoch 0001 Iter 2367 / 3136 | Time 0.1s | Iter Loss 0.0721 | Iter Mean Loss 0.1674\n",
      "2024-11-14 04:42:48,467 - root - INFO - KG Training: Epoch 0001 Iter 2368 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1674\n",
      "2024-11-14 04:42:48,543 - root - INFO - KG Training: Epoch 0001 Iter 2369 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1673\n",
      "2024-11-14 04:42:48,601 - root - INFO - KG Training: Epoch 0001 Iter 2370 / 3136 | Time 0.1s | Iter Loss 0.0665 | Iter Mean Loss 0.1673\n",
      "2024-11-14 04:42:48,659 - root - INFO - KG Training: Epoch 0001 Iter 2371 / 3136 | Time 0.1s | Iter Loss 0.0740 | Iter Mean Loss 0.1672\n",
      "2024-11-14 04:42:48,718 - root - INFO - KG Training: Epoch 0001 Iter 2372 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1672\n",
      "2024-11-14 04:42:48,779 - root - INFO - KG Training: Epoch 0001 Iter 2373 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1672\n",
      "2024-11-14 04:42:48,839 - root - INFO - KG Training: Epoch 0001 Iter 2374 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1671\n",
      "2024-11-14 04:42:48,898 - root - INFO - KG Training: Epoch 0001 Iter 2375 / 3136 | Time 0.1s | Iter Loss 0.0737 | Iter Mean Loss 0.1671\n",
      "2024-11-14 04:42:48,958 - root - INFO - KG Training: Epoch 0001 Iter 2376 / 3136 | Time 0.1s | Iter Loss 0.0752 | Iter Mean Loss 0.1670\n",
      "2024-11-14 04:42:49,019 - root - INFO - KG Training: Epoch 0001 Iter 2377 / 3136 | Time 0.1s | Iter Loss 0.0678 | Iter Mean Loss 0.1670\n",
      "2024-11-14 04:42:49,079 - root - INFO - KG Training: Epoch 0001 Iter 2378 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.1670\n",
      "2024-11-14 04:42:49,138 - root - INFO - KG Training: Epoch 0001 Iter 2379 / 3136 | Time 0.1s | Iter Loss 0.0744 | Iter Mean Loss 0.1669\n",
      "2024-11-14 04:42:49,199 - root - INFO - KG Training: Epoch 0001 Iter 2380 / 3136 | Time 0.1s | Iter Loss 0.0640 | Iter Mean Loss 0.1669\n",
      "2024-11-14 04:42:49,259 - root - INFO - KG Training: Epoch 0001 Iter 2381 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1668\n",
      "2024-11-14 04:42:49,320 - root - INFO - KG Training: Epoch 0001 Iter 2382 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1668\n",
      "2024-11-14 04:42:49,380 - root - INFO - KG Training: Epoch 0001 Iter 2383 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1667\n",
      "2024-11-14 04:42:49,439 - root - INFO - KG Training: Epoch 0001 Iter 2384 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1667\n",
      "2024-11-14 04:42:49,501 - root - INFO - KG Training: Epoch 0001 Iter 2385 / 3136 | Time 0.1s | Iter Loss 0.0683 | Iter Mean Loss 0.1667\n",
      "2024-11-14 04:42:49,562 - root - INFO - KG Training: Epoch 0001 Iter 2386 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1666\n",
      "2024-11-14 04:42:49,622 - root - INFO - KG Training: Epoch 0001 Iter 2387 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1666\n",
      "2024-11-14 04:42:49,684 - root - INFO - KG Training: Epoch 0001 Iter 2388 / 3136 | Time 0.1s | Iter Loss 0.0747 | Iter Mean Loss 0.1665\n",
      "2024-11-14 04:42:49,743 - root - INFO - KG Training: Epoch 0001 Iter 2389 / 3136 | Time 0.1s | Iter Loss 0.0766 | Iter Mean Loss 0.1665\n",
      "2024-11-14 04:42:49,805 - root - INFO - KG Training: Epoch 0001 Iter 2390 / 3136 | Time 0.1s | Iter Loss 0.0573 | Iter Mean Loss 0.1665\n",
      "2024-11-14 04:42:49,864 - root - INFO - KG Training: Epoch 0001 Iter 2391 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1664\n",
      "2024-11-14 04:42:49,924 - root - INFO - KG Training: Epoch 0001 Iter 2392 / 3136 | Time 0.1s | Iter Loss 0.0611 | Iter Mean Loss 0.1664\n",
      "2024-11-14 04:42:49,983 - root - INFO - KG Training: Epoch 0001 Iter 2393 / 3136 | Time 0.1s | Iter Loss 0.0796 | Iter Mean Loss 0.1663\n",
      "2024-11-14 04:42:50,043 - root - INFO - KG Training: Epoch 0001 Iter 2394 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1663\n",
      "2024-11-14 04:42:50,104 - root - INFO - KG Training: Epoch 0001 Iter 2395 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1663\n",
      "2024-11-14 04:42:50,165 - root - INFO - KG Training: Epoch 0001 Iter 2396 / 3136 | Time 0.1s | Iter Loss 0.0668 | Iter Mean Loss 0.1662\n",
      "2024-11-14 04:42:50,228 - root - INFO - KG Training: Epoch 0001 Iter 2397 / 3136 | Time 0.1s | Iter Loss 0.0665 | Iter Mean Loss 0.1662\n",
      "2024-11-14 04:42:50,289 - root - INFO - KG Training: Epoch 0001 Iter 2398 / 3136 | Time 0.1s | Iter Loss 0.0643 | Iter Mean Loss 0.1661\n",
      "2024-11-14 04:42:50,351 - root - INFO - KG Training: Epoch 0001 Iter 2399 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1661\n",
      "2024-11-14 04:42:50,415 - root - INFO - KG Training: Epoch 0001 Iter 2400 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1660\n",
      "2024-11-14 04:42:50,475 - root - INFO - KG Training: Epoch 0001 Iter 2401 / 3136 | Time 0.1s | Iter Loss 0.0803 | Iter Mean Loss 0.1660\n",
      "2024-11-14 04:42:50,539 - root - INFO - KG Training: Epoch 0001 Iter 2402 / 3136 | Time 0.1s | Iter Loss 0.0705 | Iter Mean Loss 0.1660\n",
      "2024-11-14 04:42:50,653 - root - INFO - KG Training: Epoch 0001 Iter 2403 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1659\n",
      "2024-11-14 04:42:50,717 - root - INFO - KG Training: Epoch 0001 Iter 2404 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1659\n",
      "2024-11-14 04:42:50,776 - root - INFO - KG Training: Epoch 0001 Iter 2405 / 3136 | Time 0.1s | Iter Loss 0.0694 | Iter Mean Loss 0.1658\n",
      "2024-11-14 04:42:50,838 - root - INFO - KG Training: Epoch 0001 Iter 2406 / 3136 | Time 0.1s | Iter Loss 0.0635 | Iter Mean Loss 0.1658\n",
      "2024-11-14 04:42:50,899 - root - INFO - KG Training: Epoch 0001 Iter 2407 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1658\n",
      "2024-11-14 04:42:50,960 - root - INFO - KG Training: Epoch 0001 Iter 2408 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1657\n",
      "2024-11-14 04:42:51,189 - root - INFO - KG Training: Epoch 0001 Iter 2409 / 3136 | Time 0.2s | Iter Loss 0.0657 | Iter Mean Loss 0.1657\n",
      "2024-11-14 04:42:51,254 - root - INFO - KG Training: Epoch 0001 Iter 2410 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1656\n",
      "2024-11-14 04:42:51,317 - root - INFO - KG Training: Epoch 0001 Iter 2411 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1656\n",
      "2024-11-14 04:42:51,676 - root - INFO - KG Training: Epoch 0001 Iter 2412 / 3136 | Time 0.4s | Iter Loss 0.0780 | Iter Mean Loss 0.1656\n",
      "2024-11-14 04:42:51,738 - root - INFO - KG Training: Epoch 0001 Iter 2413 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1655\n",
      "2024-11-14 04:42:51,960 - root - INFO - KG Training: Epoch 0001 Iter 2414 / 3136 | Time 0.2s | Iter Loss 0.0771 | Iter Mean Loss 0.1655\n",
      "2024-11-14 04:42:52,022 - root - INFO - KG Training: Epoch 0001 Iter 2415 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1654\n",
      "2024-11-14 04:42:52,085 - root - INFO - KG Training: Epoch 0001 Iter 2416 / 3136 | Time 0.1s | Iter Loss 0.0655 | Iter Mean Loss 0.1654\n",
      "2024-11-14 04:42:52,144 - root - INFO - KG Training: Epoch 0001 Iter 2417 / 3136 | Time 0.1s | Iter Loss 0.0598 | Iter Mean Loss 0.1654\n",
      "2024-11-14 04:42:52,205 - root - INFO - KG Training: Epoch 0001 Iter 2418 / 3136 | Time 0.1s | Iter Loss 0.0540 | Iter Mean Loss 0.1653\n",
      "2024-11-14 04:42:52,265 - root - INFO - KG Training: Epoch 0001 Iter 2419 / 3136 | Time 0.1s | Iter Loss 0.0725 | Iter Mean Loss 0.1653\n",
      "2024-11-14 04:42:52,328 - root - INFO - KG Training: Epoch 0001 Iter 2420 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1652\n",
      "2024-11-14 04:42:52,389 - root - INFO - KG Training: Epoch 0001 Iter 2421 / 3136 | Time 0.1s | Iter Loss 0.0719 | Iter Mean Loss 0.1652\n",
      "2024-11-14 04:42:52,450 - root - INFO - KG Training: Epoch 0001 Iter 2422 / 3136 | Time 0.1s | Iter Loss 0.0647 | Iter Mean Loss 0.1652\n",
      "2024-11-14 04:42:52,511 - root - INFO - KG Training: Epoch 0001 Iter 2423 / 3136 | Time 0.1s | Iter Loss 0.0649 | Iter Mean Loss 0.1651\n",
      "2024-11-14 04:42:52,573 - root - INFO - KG Training: Epoch 0001 Iter 2424 / 3136 | Time 0.1s | Iter Loss 0.0591 | Iter Mean Loss 0.1651\n",
      "2024-11-14 04:42:52,639 - root - INFO - KG Training: Epoch 0001 Iter 2425 / 3136 | Time 0.1s | Iter Loss 0.0795 | Iter Mean Loss 0.1650\n",
      "2024-11-14 04:42:52,709 - root - INFO - KG Training: Epoch 0001 Iter 2426 / 3136 | Time 0.1s | Iter Loss 0.0644 | Iter Mean Loss 0.1650\n",
      "2024-11-14 04:42:52,768 - root - INFO - KG Training: Epoch 0001 Iter 2427 / 3136 | Time 0.1s | Iter Loss 0.0721 | Iter Mean Loss 0.1650\n",
      "2024-11-14 04:42:52,826 - root - INFO - KG Training: Epoch 0001 Iter 2428 / 3136 | Time 0.1s | Iter Loss 0.0667 | Iter Mean Loss 0.1649\n",
      "2024-11-14 04:42:52,889 - root - INFO - KG Training: Epoch 0001 Iter 2429 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1649\n",
      "2024-11-14 04:42:52,948 - root - INFO - KG Training: Epoch 0001 Iter 2430 / 3136 | Time 0.1s | Iter Loss 0.0751 | Iter Mean Loss 0.1648\n",
      "2024-11-14 04:42:53,007 - root - INFO - KG Training: Epoch 0001 Iter 2431 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1648\n",
      "2024-11-14 04:42:53,067 - root - INFO - KG Training: Epoch 0001 Iter 2432 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1648\n",
      "2024-11-14 04:42:53,125 - root - INFO - KG Training: Epoch 0001 Iter 2433 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.1647\n",
      "2024-11-14 04:42:53,186 - root - INFO - KG Training: Epoch 0001 Iter 2434 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1647\n",
      "2024-11-14 04:42:53,246 - root - INFO - KG Training: Epoch 0001 Iter 2435 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1646\n",
      "2024-11-14 04:42:53,306 - root - INFO - KG Training: Epoch 0001 Iter 2436 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1646\n",
      "2024-11-14 04:42:53,364 - root - INFO - KG Training: Epoch 0001 Iter 2437 / 3136 | Time 0.1s | Iter Loss 0.0712 | Iter Mean Loss 0.1646\n",
      "2024-11-14 04:42:53,425 - root - INFO - KG Training: Epoch 0001 Iter 2438 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1645\n",
      "2024-11-14 04:42:53,487 - root - INFO - KG Training: Epoch 0001 Iter 2439 / 3136 | Time 0.1s | Iter Loss 0.0641 | Iter Mean Loss 0.1645\n",
      "2024-11-14 04:42:53,547 - root - INFO - KG Training: Epoch 0001 Iter 2440 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1644\n",
      "2024-11-14 04:42:53,609 - root - INFO - KG Training: Epoch 0001 Iter 2441 / 3136 | Time 0.1s | Iter Loss 0.0537 | Iter Mean Loss 0.1644\n",
      "2024-11-14 04:42:53,671 - root - INFO - KG Training: Epoch 0001 Iter 2442 / 3136 | Time 0.1s | Iter Loss 0.0571 | Iter Mean Loss 0.1643\n",
      "2024-11-14 04:42:53,736 - root - INFO - KG Training: Epoch 0001 Iter 2443 / 3136 | Time 0.1s | Iter Loss 0.0616 | Iter Mean Loss 0.1643\n",
      "2024-11-14 04:42:53,797 - root - INFO - KG Training: Epoch 0001 Iter 2444 / 3136 | Time 0.1s | Iter Loss 0.0614 | Iter Mean Loss 0.1643\n",
      "2024-11-14 04:42:53,869 - root - INFO - KG Training: Epoch 0001 Iter 2445 / 3136 | Time 0.1s | Iter Loss 0.0704 | Iter Mean Loss 0.1642\n",
      "2024-11-14 04:42:53,929 - root - INFO - KG Training: Epoch 0001 Iter 2446 / 3136 | Time 0.1s | Iter Loss 0.0602 | Iter Mean Loss 0.1642\n",
      "2024-11-14 04:42:54,002 - root - INFO - KG Training: Epoch 0001 Iter 2447 / 3136 | Time 0.1s | Iter Loss 0.0734 | Iter Mean Loss 0.1641\n",
      "2024-11-14 04:42:54,073 - root - INFO - KG Training: Epoch 0001 Iter 2448 / 3136 | Time 0.1s | Iter Loss 0.0613 | Iter Mean Loss 0.1641\n",
      "2024-11-14 04:42:54,151 - root - INFO - KG Training: Epoch 0001 Iter 2449 / 3136 | Time 0.1s | Iter Loss 0.0707 | Iter Mean Loss 0.1641\n",
      "2024-11-14 04:42:54,209 - root - INFO - KG Training: Epoch 0001 Iter 2450 / 3136 | Time 0.1s | Iter Loss 0.0563 | Iter Mean Loss 0.1640\n",
      "2024-11-14 04:42:54,277 - root - INFO - KG Training: Epoch 0001 Iter 2451 / 3136 | Time 0.1s | Iter Loss 0.0580 | Iter Mean Loss 0.1640\n",
      "2024-11-14 04:42:54,410 - root - INFO - KG Training: Epoch 0001 Iter 2452 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1639\n",
      "2024-11-14 04:42:54,470 - root - INFO - KG Training: Epoch 0001 Iter 2453 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1639\n",
      "2024-11-14 04:42:54,531 - root - INFO - KG Training: Epoch 0001 Iter 2454 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1639\n",
      "2024-11-14 04:42:54,590 - root - INFO - KG Training: Epoch 0001 Iter 2455 / 3136 | Time 0.1s | Iter Loss 0.0699 | Iter Mean Loss 0.1638\n",
      "2024-11-14 04:42:54,650 - root - INFO - KG Training: Epoch 0001 Iter 2456 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1638\n",
      "2024-11-14 04:42:54,711 - root - INFO - KG Training: Epoch 0001 Iter 2457 / 3136 | Time 0.1s | Iter Loss 0.0722 | Iter Mean Loss 0.1637\n",
      "2024-11-14 04:42:54,771 - root - INFO - KG Training: Epoch 0001 Iter 2458 / 3136 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1637\n",
      "2024-11-14 04:42:54,828 - root - INFO - KG Training: Epoch 0001 Iter 2459 / 3136 | Time 0.1s | Iter Loss 0.0553 | Iter Mean Loss 0.1637\n",
      "2024-11-14 04:42:54,887 - root - INFO - KG Training: Epoch 0001 Iter 2460 / 3136 | Time 0.1s | Iter Loss 0.0595 | Iter Mean Loss 0.1636\n",
      "2024-11-14 04:42:54,948 - root - INFO - KG Training: Epoch 0001 Iter 2461 / 3136 | Time 0.1s | Iter Loss 0.0706 | Iter Mean Loss 0.1636\n",
      "2024-11-14 04:42:55,012 - root - INFO - KG Training: Epoch 0001 Iter 2462 / 3136 | Time 0.1s | Iter Loss 0.0750 | Iter Mean Loss 0.1635\n",
      "2024-11-14 04:42:55,076 - root - INFO - KG Training: Epoch 0001 Iter 2463 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1635\n",
      "2024-11-14 04:42:55,134 - root - INFO - KG Training: Epoch 0001 Iter 2464 / 3136 | Time 0.1s | Iter Loss 0.0640 | Iter Mean Loss 0.1635\n",
      "2024-11-14 04:42:55,194 - root - INFO - KG Training: Epoch 0001 Iter 2465 / 3136 | Time 0.1s | Iter Loss 0.0574 | Iter Mean Loss 0.1634\n",
      "2024-11-14 04:42:55,255 - root - INFO - KG Training: Epoch 0001 Iter 2466 / 3136 | Time 0.1s | Iter Loss 0.0724 | Iter Mean Loss 0.1634\n",
      "2024-11-14 04:42:55,314 - root - INFO - KG Training: Epoch 0001 Iter 2467 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1633\n",
      "2024-11-14 04:42:55,375 - root - INFO - KG Training: Epoch 0001 Iter 2468 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1633\n",
      "2024-11-14 04:42:55,437 - root - INFO - KG Training: Epoch 0001 Iter 2469 / 3136 | Time 0.1s | Iter Loss 0.0670 | Iter Mean Loss 0.1633\n",
      "2024-11-14 04:42:55,497 - root - INFO - KG Training: Epoch 0001 Iter 2470 / 3136 | Time 0.1s | Iter Loss 0.0589 | Iter Mean Loss 0.1632\n",
      "2024-11-14 04:42:55,559 - root - INFO - KG Training: Epoch 0001 Iter 2471 / 3136 | Time 0.1s | Iter Loss 0.0655 | Iter Mean Loss 0.1632\n",
      "2024-11-14 04:42:55,630 - root - INFO - KG Training: Epoch 0001 Iter 2472 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1631\n",
      "2024-11-14 04:42:55,691 - root - INFO - KG Training: Epoch 0001 Iter 2473 / 3136 | Time 0.1s | Iter Loss 0.0631 | Iter Mean Loss 0.1631\n",
      "2024-11-14 04:42:55,759 - root - INFO - KG Training: Epoch 0001 Iter 2474 / 3136 | Time 0.1s | Iter Loss 0.0681 | Iter Mean Loss 0.1631\n",
      "2024-11-14 04:42:55,825 - root - INFO - KG Training: Epoch 0001 Iter 2475 / 3136 | Time 0.1s | Iter Loss 0.0688 | Iter Mean Loss 0.1630\n",
      "2024-11-14 04:42:55,884 - root - INFO - KG Training: Epoch 0001 Iter 2476 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1630\n",
      "2024-11-14 04:42:55,941 - root - INFO - KG Training: Epoch 0001 Iter 2477 / 3136 | Time 0.1s | Iter Loss 0.0634 | Iter Mean Loss 0.1629\n",
      "2024-11-14 04:42:56,018 - root - INFO - KG Training: Epoch 0001 Iter 2478 / 3136 | Time 0.1s | Iter Loss 0.0609 | Iter Mean Loss 0.1629\n",
      "2024-11-14 04:42:56,093 - root - INFO - KG Training: Epoch 0001 Iter 2479 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1629\n",
      "2024-11-14 04:42:56,171 - root - INFO - KG Training: Epoch 0001 Iter 2480 / 3136 | Time 0.1s | Iter Loss 0.0671 | Iter Mean Loss 0.1628\n",
      "2024-11-14 04:42:56,229 - root - INFO - KG Training: Epoch 0001 Iter 2481 / 3136 | Time 0.1s | Iter Loss 0.0600 | Iter Mean Loss 0.1628\n",
      "2024-11-14 04:42:56,304 - root - INFO - KG Training: Epoch 0001 Iter 2482 / 3136 | Time 0.1s | Iter Loss 0.0680 | Iter Mean Loss 0.1628\n",
      "2024-11-14 04:42:56,371 - root - INFO - KG Training: Epoch 0001 Iter 2483 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1627\n",
      "2024-11-14 04:42:56,429 - root - INFO - KG Training: Epoch 0001 Iter 2484 / 3136 | Time 0.1s | Iter Loss 0.0738 | Iter Mean Loss 0.1627\n",
      "2024-11-14 04:42:56,489 - root - INFO - KG Training: Epoch 0001 Iter 2485 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1626\n",
      "2024-11-14 04:42:56,550 - root - INFO - KG Training: Epoch 0001 Iter 2486 / 3136 | Time 0.1s | Iter Loss 0.0765 | Iter Mean Loss 0.1626\n",
      "2024-11-14 04:42:56,611 - root - INFO - KG Training: Epoch 0001 Iter 2487 / 3136 | Time 0.1s | Iter Loss 0.0670 | Iter Mean Loss 0.1626\n",
      "2024-11-14 04:42:56,674 - root - INFO - KG Training: Epoch 0001 Iter 2488 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1625\n",
      "2024-11-14 04:42:56,735 - root - INFO - KG Training: Epoch 0001 Iter 2489 / 3136 | Time 0.1s | Iter Loss 0.0658 | Iter Mean Loss 0.1625\n",
      "2024-11-14 04:42:56,797 - root - INFO - KG Training: Epoch 0001 Iter 2490 / 3136 | Time 0.1s | Iter Loss 0.0676 | Iter Mean Loss 0.1624\n",
      "2024-11-14 04:42:56,857 - root - INFO - KG Training: Epoch 0001 Iter 2491 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1624\n",
      "2024-11-14 04:42:56,916 - root - INFO - KG Training: Epoch 0001 Iter 2492 / 3136 | Time 0.1s | Iter Loss 0.0644 | Iter Mean Loss 0.1624\n",
      "2024-11-14 04:42:56,976 - root - INFO - KG Training: Epoch 0001 Iter 2493 / 3136 | Time 0.1s | Iter Loss 0.0745 | Iter Mean Loss 0.1623\n",
      "2024-11-14 04:42:57,035 - root - INFO - KG Training: Epoch 0001 Iter 2494 / 3136 | Time 0.1s | Iter Loss 0.0625 | Iter Mean Loss 0.1623\n",
      "2024-11-14 04:42:57,093 - root - INFO - KG Training: Epoch 0001 Iter 2495 / 3136 | Time 0.1s | Iter Loss 0.0663 | Iter Mean Loss 0.1623\n",
      "2024-11-14 04:42:57,150 - root - INFO - KG Training: Epoch 0001 Iter 2496 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1622\n",
      "2024-11-14 04:42:57,212 - root - INFO - KG Training: Epoch 0001 Iter 2497 / 3136 | Time 0.1s | Iter Loss 0.0653 | Iter Mean Loss 0.1622\n",
      "2024-11-14 04:42:57,269 - root - INFO - KG Training: Epoch 0001 Iter 2498 / 3136 | Time 0.1s | Iter Loss 0.0617 | Iter Mean Loss 0.1621\n",
      "2024-11-14 04:42:57,326 - root - INFO - KG Training: Epoch 0001 Iter 2499 / 3136 | Time 0.1s | Iter Loss 0.0735 | Iter Mean Loss 0.1621\n",
      "2024-11-14 04:42:57,386 - root - INFO - KG Training: Epoch 0001 Iter 2500 / 3136 | Time 0.1s | Iter Loss 0.0709 | Iter Mean Loss 0.1621\n",
      "2024-11-14 04:42:57,443 - root - INFO - KG Training: Epoch 0001 Iter 2501 / 3136 | Time 0.1s | Iter Loss 0.0741 | Iter Mean Loss 0.1620\n",
      "2024-11-14 04:42:57,514 - root - INFO - KG Training: Epoch 0001 Iter 2502 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1620\n",
      "2024-11-14 04:42:57,574 - root - INFO - KG Training: Epoch 0001 Iter 2503 / 3136 | Time 0.1s | Iter Loss 0.0606 | Iter Mean Loss 0.1620\n",
      "2024-11-14 04:42:57,631 - root - INFO - KG Training: Epoch 0001 Iter 2504 / 3136 | Time 0.1s | Iter Loss 0.0609 | Iter Mean Loss 0.1619\n",
      "2024-11-14 04:42:57,691 - root - INFO - KG Training: Epoch 0001 Iter 2505 / 3136 | Time 0.1s | Iter Loss 0.0651 | Iter Mean Loss 0.1619\n",
      "2024-11-14 04:42:57,749 - root - INFO - KG Training: Epoch 0001 Iter 2506 / 3136 | Time 0.1s | Iter Loss 0.0533 | Iter Mean Loss 0.1618\n",
      "2024-11-14 04:42:57,808 - root - INFO - KG Training: Epoch 0001 Iter 2507 / 3136 | Time 0.1s | Iter Loss 0.0593 | Iter Mean Loss 0.1618\n",
      "2024-11-14 04:42:57,867 - root - INFO - KG Training: Epoch 0001 Iter 2508 / 3136 | Time 0.1s | Iter Loss 0.0688 | Iter Mean Loss 0.1618\n",
      "2024-11-14 04:42:57,984 - root - INFO - KG Training: Epoch 0001 Iter 2509 / 3136 | Time 0.1s | Iter Loss 0.0686 | Iter Mean Loss 0.1617\n",
      "2024-11-14 04:42:58,042 - root - INFO - KG Training: Epoch 0001 Iter 2510 / 3136 | Time 0.1s | Iter Loss 0.0593 | Iter Mean Loss 0.1617\n",
      "2024-11-14 04:42:58,100 - root - INFO - KG Training: Epoch 0001 Iter 2511 / 3136 | Time 0.1s | Iter Loss 0.0751 | Iter Mean Loss 0.1616\n",
      "2024-11-14 04:42:58,219 - root - INFO - KG Training: Epoch 0001 Iter 2512 / 3136 | Time 0.1s | Iter Loss 0.0677 | Iter Mean Loss 0.1616\n",
      "2024-11-14 04:42:58,278 - root - INFO - KG Training: Epoch 0001 Iter 2513 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1616\n",
      "2024-11-14 04:42:58,335 - root - INFO - KG Training: Epoch 0001 Iter 2514 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1615\n",
      "2024-11-14 04:42:58,393 - root - INFO - KG Training: Epoch 0001 Iter 2515 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1615\n",
      "2024-11-14 04:42:58,453 - root - INFO - KG Training: Epoch 0001 Iter 2516 / 3136 | Time 0.1s | Iter Loss 0.0657 | Iter Mean Loss 0.1615\n",
      "2024-11-14 04:42:58,510 - root - INFO - KG Training: Epoch 0001 Iter 2517 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1614\n",
      "2024-11-14 04:42:58,568 - root - INFO - KG Training: Epoch 0001 Iter 2518 / 3136 | Time 0.1s | Iter Loss 0.0640 | Iter Mean Loss 0.1614\n",
      "2024-11-14 04:42:58,629 - root - INFO - KG Training: Epoch 0001 Iter 2519 / 3136 | Time 0.1s | Iter Loss 0.0621 | Iter Mean Loss 0.1613\n",
      "2024-11-14 04:42:58,688 - root - INFO - KG Training: Epoch 0001 Iter 2520 / 3136 | Time 0.1s | Iter Loss 0.0713 | Iter Mean Loss 0.1613\n",
      "2024-11-14 04:42:58,747 - root - INFO - KG Training: Epoch 0001 Iter 2521 / 3136 | Time 0.1s | Iter Loss 0.0580 | Iter Mean Loss 0.1613\n",
      "2024-11-14 04:42:58,806 - root - INFO - KG Training: Epoch 0001 Iter 2522 / 3136 | Time 0.1s | Iter Loss 0.0727 | Iter Mean Loss 0.1612\n",
      "2024-11-14 04:42:58,863 - root - INFO - KG Training: Epoch 0001 Iter 2523 / 3136 | Time 0.1s | Iter Loss 0.0717 | Iter Mean Loss 0.1612\n",
      "2024-11-14 04:42:58,923 - root - INFO - KG Training: Epoch 0001 Iter 2524 / 3136 | Time 0.1s | Iter Loss 0.0617 | Iter Mean Loss 0.1612\n",
      "2024-11-14 04:42:58,982 - root - INFO - KG Training: Epoch 0001 Iter 2525 / 3136 | Time 0.1s | Iter Loss 0.0672 | Iter Mean Loss 0.1611\n",
      "2024-11-14 04:42:59,040 - root - INFO - KG Training: Epoch 0001 Iter 2526 / 3136 | Time 0.1s | Iter Loss 0.0661 | Iter Mean Loss 0.1611\n",
      "2024-11-14 04:42:59,097 - root - INFO - KG Training: Epoch 0001 Iter 2527 / 3136 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1610\n",
      "2024-11-14 04:42:59,168 - root - INFO - KG Training: Epoch 0001 Iter 2528 / 3136 | Time 0.1s | Iter Loss 0.0644 | Iter Mean Loss 0.1610\n",
      "2024-11-14 04:42:59,225 - root - INFO - KG Training: Epoch 0001 Iter 2529 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1610\n",
      "2024-11-14 04:42:59,287 - root - INFO - KG Training: Epoch 0001 Iter 2530 / 3136 | Time 0.1s | Iter Loss 0.0643 | Iter Mean Loss 0.1609\n",
      "2024-11-14 04:42:59,344 - root - INFO - KG Training: Epoch 0001 Iter 2531 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1609\n",
      "2024-11-14 04:42:59,404 - root - INFO - KG Training: Epoch 0001 Iter 2532 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1609\n",
      "2024-11-14 04:42:59,461 - root - INFO - KG Training: Epoch 0001 Iter 2533 / 3136 | Time 0.1s | Iter Loss 0.0662 | Iter Mean Loss 0.1608\n",
      "2024-11-14 04:42:59,521 - root - INFO - KG Training: Epoch 0001 Iter 2534 / 3136 | Time 0.1s | Iter Loss 0.0702 | Iter Mean Loss 0.1608\n",
      "2024-11-14 04:42:59,740 - root - INFO - KG Training: Epoch 0001 Iter 2535 / 3136 | Time 0.2s | Iter Loss 0.0596 | Iter Mean Loss 0.1607\n",
      "2024-11-14 04:42:59,796 - root - INFO - KG Training: Epoch 0001 Iter 2536 / 3136 | Time 0.1s | Iter Loss 0.0546 | Iter Mean Loss 0.1607\n",
      "2024-11-14 04:42:59,865 - root - INFO - KG Training: Epoch 0001 Iter 2537 / 3136 | Time 0.1s | Iter Loss 0.0737 | Iter Mean Loss 0.1607\n",
      "2024-11-14 04:42:59,927 - root - INFO - KG Training: Epoch 0001 Iter 2538 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1606\n",
      "2024-11-14 04:42:59,987 - root - INFO - KG Training: Epoch 0001 Iter 2539 / 3136 | Time 0.1s | Iter Loss 0.0631 | Iter Mean Loss 0.1606\n",
      "2024-11-14 04:43:00,046 - root - INFO - KG Training: Epoch 0001 Iter 2540 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1606\n",
      "2024-11-14 04:43:00,105 - root - INFO - KG Training: Epoch 0001 Iter 2541 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1605\n",
      "2024-11-14 04:43:00,165 - root - INFO - KG Training: Epoch 0001 Iter 2542 / 3136 | Time 0.1s | Iter Loss 0.0624 | Iter Mean Loss 0.1605\n",
      "2024-11-14 04:43:00,224 - root - INFO - KG Training: Epoch 0001 Iter 2543 / 3136 | Time 0.1s | Iter Loss 0.0603 | Iter Mean Loss 0.1604\n",
      "2024-11-14 04:43:00,285 - root - INFO - KG Training: Epoch 0001 Iter 2544 / 3136 | Time 0.1s | Iter Loss 0.0635 | Iter Mean Loss 0.1604\n",
      "2024-11-14 04:43:00,344 - root - INFO - KG Training: Epoch 0001 Iter 2545 / 3136 | Time 0.1s | Iter Loss 0.0651 | Iter Mean Loss 0.1604\n",
      "2024-11-14 04:43:00,454 - root - INFO - KG Training: Epoch 0001 Iter 2546 / 3136 | Time 0.1s | Iter Loss 0.0562 | Iter Mean Loss 0.1603\n",
      "2024-11-14 04:43:00,514 - root - INFO - KG Training: Epoch 0001 Iter 2547 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1603\n",
      "2024-11-14 04:43:00,572 - root - INFO - KG Training: Epoch 0001 Iter 2548 / 3136 | Time 0.1s | Iter Loss 0.0624 | Iter Mean Loss 0.1602\n",
      "2024-11-14 04:43:00,634 - root - INFO - KG Training: Epoch 0001 Iter 2549 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1602\n",
      "2024-11-14 04:43:00,692 - root - INFO - KG Training: Epoch 0001 Iter 2550 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1602\n",
      "2024-11-14 04:43:00,751 - root - INFO - KG Training: Epoch 0001 Iter 2551 / 3136 | Time 0.1s | Iter Loss 0.0596 | Iter Mean Loss 0.1601\n",
      "2024-11-14 04:43:00,811 - root - INFO - KG Training: Epoch 0001 Iter 2552 / 3136 | Time 0.1s | Iter Loss 0.0578 | Iter Mean Loss 0.1601\n",
      "2024-11-14 04:43:00,872 - root - INFO - KG Training: Epoch 0001 Iter 2553 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1601\n",
      "2024-11-14 04:43:01,006 - root - INFO - KG Training: Epoch 0001 Iter 2554 / 3136 | Time 0.1s | Iter Loss 0.0582 | Iter Mean Loss 0.1600\n",
      "2024-11-14 04:43:01,065 - root - INFO - KG Training: Epoch 0001 Iter 2555 / 3136 | Time 0.1s | Iter Loss 0.0665 | Iter Mean Loss 0.1600\n",
      "2024-11-14 04:43:01,136 - root - INFO - KG Training: Epoch 0001 Iter 2556 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1599\n",
      "2024-11-14 04:43:01,480 - root - INFO - KG Training: Epoch 0001 Iter 2557 / 3136 | Time 0.3s | Iter Loss 0.0647 | Iter Mean Loss 0.1599\n",
      "2024-11-14 04:43:01,540 - root - INFO - KG Training: Epoch 0001 Iter 2558 / 3136 | Time 0.1s | Iter Loss 0.0739 | Iter Mean Loss 0.1599\n",
      "2024-11-14 04:43:01,603 - root - INFO - KG Training: Epoch 0001 Iter 2559 / 3136 | Time 0.1s | Iter Loss 0.0528 | Iter Mean Loss 0.1598\n",
      "2024-11-14 04:43:01,662 - root - INFO - KG Training: Epoch 0001 Iter 2560 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1598\n",
      "2024-11-14 04:43:01,727 - root - INFO - KG Training: Epoch 0001 Iter 2561 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1598\n",
      "2024-11-14 04:43:01,789 - root - INFO - KG Training: Epoch 0001 Iter 2562 / 3136 | Time 0.1s | Iter Loss 0.0634 | Iter Mean Loss 0.1597\n",
      "2024-11-14 04:43:01,852 - root - INFO - KG Training: Epoch 0001 Iter 2563 / 3136 | Time 0.1s | Iter Loss 0.0738 | Iter Mean Loss 0.1597\n",
      "2024-11-14 04:43:01,981 - root - INFO - KG Training: Epoch 0001 Iter 2564 / 3136 | Time 0.1s | Iter Loss 0.0714 | Iter Mean Loss 0.1597\n",
      "2024-11-14 04:43:02,101 - root - INFO - KG Training: Epoch 0001 Iter 2565 / 3136 | Time 0.1s | Iter Loss 0.0822 | Iter Mean Loss 0.1596\n",
      "2024-11-14 04:43:02,161 - root - INFO - KG Training: Epoch 0001 Iter 2566 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1596\n",
      "2024-11-14 04:43:02,225 - root - INFO - KG Training: Epoch 0001 Iter 2567 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1596\n",
      "2024-11-14 04:43:02,283 - root - INFO - KG Training: Epoch 0001 Iter 2568 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1595\n",
      "2024-11-14 04:43:02,344 - root - INFO - KG Training: Epoch 0001 Iter 2569 / 3136 | Time 0.1s | Iter Loss 0.0603 | Iter Mean Loss 0.1595\n",
      "2024-11-14 04:43:02,407 - root - INFO - KG Training: Epoch 0001 Iter 2570 / 3136 | Time 0.1s | Iter Loss 0.0613 | Iter Mean Loss 0.1594\n",
      "2024-11-14 04:43:02,467 - root - INFO - KG Training: Epoch 0001 Iter 2571 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1594\n",
      "2024-11-14 04:43:02,524 - root - INFO - KG Training: Epoch 0001 Iter 2572 / 3136 | Time 0.1s | Iter Loss 0.0671 | Iter Mean Loss 0.1594\n",
      "2024-11-14 04:43:02,585 - root - INFO - KG Training: Epoch 0001 Iter 2573 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1593\n",
      "2024-11-14 04:43:02,645 - root - INFO - KG Training: Epoch 0001 Iter 2574 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1593\n",
      "2024-11-14 04:43:02,710 - root - INFO - KG Training: Epoch 0001 Iter 2575 / 3136 | Time 0.1s | Iter Loss 0.0662 | Iter Mean Loss 0.1593\n",
      "2024-11-14 04:43:02,772 - root - INFO - KG Training: Epoch 0001 Iter 2576 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1592\n",
      "2024-11-14 04:43:02,835 - root - INFO - KG Training: Epoch 0001 Iter 2577 / 3136 | Time 0.1s | Iter Loss 0.0611 | Iter Mean Loss 0.1592\n",
      "2024-11-14 04:43:02,897 - root - INFO - KG Training: Epoch 0001 Iter 2578 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1591\n",
      "2024-11-14 04:43:02,955 - root - INFO - KG Training: Epoch 0001 Iter 2579 / 3136 | Time 0.1s | Iter Loss 0.0692 | Iter Mean Loss 0.1591\n",
      "2024-11-14 04:43:03,014 - root - INFO - KG Training: Epoch 0001 Iter 2580 / 3136 | Time 0.1s | Iter Loss 0.0631 | Iter Mean Loss 0.1591\n",
      "2024-11-14 04:43:03,077 - root - INFO - KG Training: Epoch 0001 Iter 2581 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1590\n",
      "2024-11-14 04:43:03,135 - root - INFO - KG Training: Epoch 0001 Iter 2582 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1590\n",
      "2024-11-14 04:43:03,195 - root - INFO - KG Training: Epoch 0001 Iter 2583 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1590\n",
      "2024-11-14 04:43:03,258 - root - INFO - KG Training: Epoch 0001 Iter 2584 / 3136 | Time 0.1s | Iter Loss 0.0605 | Iter Mean Loss 0.1589\n",
      "2024-11-14 04:43:03,324 - root - INFO - KG Training: Epoch 0001 Iter 2585 / 3136 | Time 0.1s | Iter Loss 0.0517 | Iter Mean Loss 0.1589\n",
      "2024-11-14 04:43:03,388 - root - INFO - KG Training: Epoch 0001 Iter 2586 / 3136 | Time 0.1s | Iter Loss 0.0561 | Iter Mean Loss 0.1588\n",
      "2024-11-14 04:43:03,450 - root - INFO - KG Training: Epoch 0001 Iter 2587 / 3136 | Time 0.1s | Iter Loss 0.0573 | Iter Mean Loss 0.1588\n",
      "2024-11-14 04:43:03,511 - root - INFO - KG Training: Epoch 0001 Iter 2588 / 3136 | Time 0.1s | Iter Loss 0.0574 | Iter Mean Loss 0.1588\n",
      "2024-11-14 04:43:03,574 - root - INFO - KG Training: Epoch 0001 Iter 2589 / 3136 | Time 0.1s | Iter Loss 0.0739 | Iter Mean Loss 0.1587\n",
      "2024-11-14 04:43:03,635 - root - INFO - KG Training: Epoch 0001 Iter 2590 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1587\n",
      "2024-11-14 04:43:03,695 - root - INFO - KG Training: Epoch 0001 Iter 2591 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1587\n",
      "2024-11-14 04:43:03,751 - root - INFO - KG Training: Epoch 0001 Iter 2592 / 3136 | Time 0.1s | Iter Loss 0.0668 | Iter Mean Loss 0.1586\n",
      "2024-11-14 04:43:03,811 - root - INFO - KG Training: Epoch 0001 Iter 2593 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1586\n",
      "2024-11-14 04:43:03,872 - root - INFO - KG Training: Epoch 0001 Iter 2594 / 3136 | Time 0.1s | Iter Loss 0.0562 | Iter Mean Loss 0.1586\n",
      "2024-11-14 04:43:03,933 - root - INFO - KG Training: Epoch 0001 Iter 2595 / 3136 | Time 0.1s | Iter Loss 0.0740 | Iter Mean Loss 0.1585\n",
      "2024-11-14 04:43:03,993 - root - INFO - KG Training: Epoch 0001 Iter 2596 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1585\n",
      "2024-11-14 04:43:04,053 - root - INFO - KG Training: Epoch 0001 Iter 2597 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1584\n",
      "2024-11-14 04:43:04,117 - root - INFO - KG Training: Epoch 0001 Iter 2598 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1584\n",
      "2024-11-14 04:43:04,178 - root - INFO - KG Training: Epoch 0001 Iter 2599 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1584\n",
      "2024-11-14 04:43:04,239 - root - INFO - KG Training: Epoch 0001 Iter 2600 / 3136 | Time 0.1s | Iter Loss 0.0770 | Iter Mean Loss 0.1583\n",
      "2024-11-14 04:43:04,300 - root - INFO - KG Training: Epoch 0001 Iter 2601 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1583\n",
      "2024-11-14 04:43:04,361 - root - INFO - KG Training: Epoch 0001 Iter 2602 / 3136 | Time 0.1s | Iter Loss 0.0695 | Iter Mean Loss 0.1583\n",
      "2024-11-14 04:43:04,424 - root - INFO - KG Training: Epoch 0001 Iter 2603 / 3136 | Time 0.1s | Iter Loss 0.0668 | Iter Mean Loss 0.1582\n",
      "2024-11-14 04:43:04,487 - root - INFO - KG Training: Epoch 0001 Iter 2604 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1582\n",
      "2024-11-14 04:43:04,549 - root - INFO - KG Training: Epoch 0001 Iter 2605 / 3136 | Time 0.1s | Iter Loss 0.0564 | Iter Mean Loss 0.1582\n",
      "2024-11-14 04:43:04,612 - root - INFO - KG Training: Epoch 0001 Iter 2606 / 3136 | Time 0.1s | Iter Loss 0.0636 | Iter Mean Loss 0.1581\n",
      "2024-11-14 04:43:04,672 - root - INFO - KG Training: Epoch 0001 Iter 2607 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1581\n",
      "2024-11-14 04:43:04,738 - root - INFO - KG Training: Epoch 0001 Iter 2608 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1581\n",
      "2024-11-14 04:43:04,797 - root - INFO - KG Training: Epoch 0001 Iter 2609 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1580\n",
      "2024-11-14 04:43:04,868 - root - INFO - KG Training: Epoch 0001 Iter 2610 / 3136 | Time 0.1s | Iter Loss 0.0596 | Iter Mean Loss 0.1580\n",
      "2024-11-14 04:43:04,926 - root - INFO - KG Training: Epoch 0001 Iter 2611 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1580\n",
      "2024-11-14 04:43:04,985 - root - INFO - KG Training: Epoch 0001 Iter 2612 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1579\n",
      "2024-11-14 04:43:05,048 - root - INFO - KG Training: Epoch 0001 Iter 2613 / 3136 | Time 0.1s | Iter Loss 0.0591 | Iter Mean Loss 0.1579\n",
      "2024-11-14 04:43:05,109 - root - INFO - KG Training: Epoch 0001 Iter 2614 / 3136 | Time 0.1s | Iter Loss 0.0584 | Iter Mean Loss 0.1578\n",
      "2024-11-14 04:43:05,170 - root - INFO - KG Training: Epoch 0001 Iter 2615 / 3136 | Time 0.1s | Iter Loss 0.0528 | Iter Mean Loss 0.1578\n",
      "2024-11-14 04:43:05,229 - root - INFO - KG Training: Epoch 0001 Iter 2616 / 3136 | Time 0.1s | Iter Loss 0.0687 | Iter Mean Loss 0.1578\n",
      "2024-11-14 04:43:05,287 - root - INFO - KG Training: Epoch 0001 Iter 2617 / 3136 | Time 0.1s | Iter Loss 0.0685 | Iter Mean Loss 0.1577\n",
      "2024-11-14 04:43:05,350 - root - INFO - KG Training: Epoch 0001 Iter 2618 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1577\n",
      "2024-11-14 04:43:05,471 - root - INFO - KG Training: Epoch 0001 Iter 2619 / 3136 | Time 0.1s | Iter Loss 0.0670 | Iter Mean Loss 0.1577\n",
      "2024-11-14 04:43:05,532 - root - INFO - KG Training: Epoch 0001 Iter 2620 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1576\n",
      "2024-11-14 04:43:05,592 - root - INFO - KG Training: Epoch 0001 Iter 2621 / 3136 | Time 0.1s | Iter Loss 0.0663 | Iter Mean Loss 0.1576\n",
      "2024-11-14 04:43:05,655 - root - INFO - KG Training: Epoch 0001 Iter 2622 / 3136 | Time 0.1s | Iter Loss 0.0633 | Iter Mean Loss 0.1576\n",
      "2024-11-14 04:43:05,718 - root - INFO - KG Training: Epoch 0001 Iter 2623 / 3136 | Time 0.1s | Iter Loss 0.0592 | Iter Mean Loss 0.1575\n",
      "2024-11-14 04:43:05,778 - root - INFO - KG Training: Epoch 0001 Iter 2624 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1575\n",
      "2024-11-14 04:43:05,847 - root - INFO - KG Training: Epoch 0001 Iter 2625 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1574\n",
      "2024-11-14 04:43:05,906 - root - INFO - KG Training: Epoch 0001 Iter 2626 / 3136 | Time 0.1s | Iter Loss 0.0657 | Iter Mean Loss 0.1574\n",
      "2024-11-14 04:43:05,970 - root - INFO - KG Training: Epoch 0001 Iter 2627 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1574\n",
      "2024-11-14 04:43:06,035 - root - INFO - KG Training: Epoch 0001 Iter 2628 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1573\n",
      "2024-11-14 04:43:06,099 - root - INFO - KG Training: Epoch 0001 Iter 2629 / 3136 | Time 0.1s | Iter Loss 0.0626 | Iter Mean Loss 0.1573\n",
      "2024-11-14 04:43:06,159 - root - INFO - KG Training: Epoch 0001 Iter 2630 / 3136 | Time 0.1s | Iter Loss 0.0587 | Iter Mean Loss 0.1573\n",
      "2024-11-14 04:43:06,218 - root - INFO - KG Training: Epoch 0001 Iter 2631 / 3136 | Time 0.1s | Iter Loss 0.0618 | Iter Mean Loss 0.1572\n",
      "2024-11-14 04:43:06,279 - root - INFO - KG Training: Epoch 0001 Iter 2632 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1572\n",
      "2024-11-14 04:43:06,341 - root - INFO - KG Training: Epoch 0001 Iter 2633 / 3136 | Time 0.1s | Iter Loss 0.0683 | Iter Mean Loss 0.1572\n",
      "2024-11-14 04:43:06,400 - root - INFO - KG Training: Epoch 0001 Iter 2634 / 3136 | Time 0.1s | Iter Loss 0.0716 | Iter Mean Loss 0.1571\n",
      "2024-11-14 04:43:06,462 - root - INFO - KG Training: Epoch 0001 Iter 2635 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1571\n",
      "2024-11-14 04:43:06,522 - root - INFO - KG Training: Epoch 0001 Iter 2636 / 3136 | Time 0.1s | Iter Loss 0.0760 | Iter Mean Loss 0.1571\n",
      "2024-11-14 04:43:06,652 - root - INFO - KG Training: Epoch 0001 Iter 2637 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1570\n",
      "2024-11-14 04:43:06,711 - root - INFO - KG Training: Epoch 0001 Iter 2638 / 3136 | Time 0.1s | Iter Loss 0.0640 | Iter Mean Loss 0.1570\n",
      "2024-11-14 04:43:06,772 - root - INFO - KG Training: Epoch 0001 Iter 2639 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1570\n",
      "2024-11-14 04:43:06,832 - root - INFO - KG Training: Epoch 0001 Iter 2640 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1569\n",
      "2024-11-14 04:43:06,892 - root - INFO - KG Training: Epoch 0001 Iter 2641 / 3136 | Time 0.1s | Iter Loss 0.0677 | Iter Mean Loss 0.1569\n",
      "2024-11-14 04:43:06,955 - root - INFO - KG Training: Epoch 0001 Iter 2642 / 3136 | Time 0.1s | Iter Loss 0.0490 | Iter Mean Loss 0.1569\n",
      "2024-11-14 04:43:07,016 - root - INFO - KG Training: Epoch 0001 Iter 2643 / 3136 | Time 0.1s | Iter Loss 0.0705 | Iter Mean Loss 0.1568\n",
      "2024-11-14 04:43:07,079 - root - INFO - KG Training: Epoch 0001 Iter 2644 / 3136 | Time 0.1s | Iter Loss 0.0593 | Iter Mean Loss 0.1568\n",
      "2024-11-14 04:43:07,143 - root - INFO - KG Training: Epoch 0001 Iter 2645 / 3136 | Time 0.1s | Iter Loss 0.0631 | Iter Mean Loss 0.1568\n",
      "2024-11-14 04:43:07,205 - root - INFO - KG Training: Epoch 0001 Iter 2646 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1567\n",
      "2024-11-14 04:43:07,266 - root - INFO - KG Training: Epoch 0001 Iter 2647 / 3136 | Time 0.1s | Iter Loss 0.0613 | Iter Mean Loss 0.1567\n",
      "2024-11-14 04:43:07,328 - root - INFO - KG Training: Epoch 0001 Iter 2648 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1566\n",
      "2024-11-14 04:43:07,388 - root - INFO - KG Training: Epoch 0001 Iter 2649 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1566\n",
      "2024-11-14 04:43:07,564 - root - INFO - KG Training: Epoch 0001 Iter 2650 / 3136 | Time 0.2s | Iter Loss 0.0634 | Iter Mean Loss 0.1566\n",
      "2024-11-14 04:43:07,625 - root - INFO - KG Training: Epoch 0001 Iter 2651 / 3136 | Time 0.1s | Iter Loss 0.0518 | Iter Mean Loss 0.1565\n",
      "2024-11-14 04:43:07,689 - root - INFO - KG Training: Epoch 0001 Iter 2652 / 3136 | Time 0.1s | Iter Loss 0.0746 | Iter Mean Loss 0.1565\n",
      "2024-11-14 04:43:07,750 - root - INFO - KG Training: Epoch 0001 Iter 2653 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1565\n",
      "2024-11-14 04:43:07,810 - root - INFO - KG Training: Epoch 0001 Iter 2654 / 3136 | Time 0.1s | Iter Loss 0.0603 | Iter Mean Loss 0.1564\n",
      "2024-11-14 04:43:07,872 - root - INFO - KG Training: Epoch 0001 Iter 2655 / 3136 | Time 0.1s | Iter Loss 0.0753 | Iter Mean Loss 0.1564\n",
      "2024-11-14 04:43:07,933 - root - INFO - KG Training: Epoch 0001 Iter 2656 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1564\n",
      "2024-11-14 04:43:07,997 - root - INFO - KG Training: Epoch 0001 Iter 2657 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1563\n",
      "2024-11-14 04:43:08,060 - root - INFO - KG Training: Epoch 0001 Iter 2658 / 3136 | Time 0.1s | Iter Loss 0.0691 | Iter Mean Loss 0.1563\n",
      "2024-11-14 04:43:08,122 - root - INFO - KG Training: Epoch 0001 Iter 2659 / 3136 | Time 0.1s | Iter Loss 0.0588 | Iter Mean Loss 0.1563\n",
      "2024-11-14 04:43:08,183 - root - INFO - KG Training: Epoch 0001 Iter 2660 / 3136 | Time 0.1s | Iter Loss 0.0719 | Iter Mean Loss 0.1562\n",
      "2024-11-14 04:43:08,247 - root - INFO - KG Training: Epoch 0001 Iter 2661 / 3136 | Time 0.1s | Iter Loss 0.0599 | Iter Mean Loss 0.1562\n",
      "2024-11-14 04:43:08,309 - root - INFO - KG Training: Epoch 0001 Iter 2662 / 3136 | Time 0.1s | Iter Loss 0.0680 | Iter Mean Loss 0.1562\n",
      "2024-11-14 04:43:08,475 - root - INFO - KG Training: Epoch 0001 Iter 2663 / 3136 | Time 0.2s | Iter Loss 0.0658 | Iter Mean Loss 0.1561\n",
      "2024-11-14 04:43:08,636 - root - INFO - KG Training: Epoch 0001 Iter 2664 / 3136 | Time 0.2s | Iter Loss 0.0696 | Iter Mean Loss 0.1561\n",
      "2024-11-14 04:43:08,695 - root - INFO - KG Training: Epoch 0001 Iter 2665 / 3136 | Time 0.1s | Iter Loss 0.0637 | Iter Mean Loss 0.1561\n",
      "2024-11-14 04:43:08,754 - root - INFO - KG Training: Epoch 0001 Iter 2666 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1560\n",
      "2024-11-14 04:43:08,814 - root - INFO - KG Training: Epoch 0001 Iter 2667 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1560\n",
      "2024-11-14 04:43:08,880 - root - INFO - KG Training: Epoch 0001 Iter 2668 / 3136 | Time 0.1s | Iter Loss 0.0712 | Iter Mean Loss 0.1560\n",
      "2024-11-14 04:43:08,941 - root - INFO - KG Training: Epoch 0001 Iter 2669 / 3136 | Time 0.1s | Iter Loss 0.0649 | Iter Mean Loss 0.1559\n",
      "2024-11-14 04:43:09,002 - root - INFO - KG Training: Epoch 0001 Iter 2670 / 3136 | Time 0.1s | Iter Loss 0.0599 | Iter Mean Loss 0.1559\n",
      "2024-11-14 04:43:09,062 - root - INFO - KG Training: Epoch 0001 Iter 2671 / 3136 | Time 0.1s | Iter Loss 0.0642 | Iter Mean Loss 0.1559\n",
      "2024-11-14 04:43:09,122 - root - INFO - KG Training: Epoch 0001 Iter 2672 / 3136 | Time 0.1s | Iter Loss 0.0581 | Iter Mean Loss 0.1558\n",
      "2024-11-14 04:43:09,239 - root - INFO - KG Training: Epoch 0001 Iter 2673 / 3136 | Time 0.1s | Iter Loss 0.0642 | Iter Mean Loss 0.1558\n",
      "2024-11-14 04:43:09,355 - root - INFO - KG Training: Epoch 0001 Iter 2674 / 3136 | Time 0.1s | Iter Loss 0.0681 | Iter Mean Loss 0.1557\n",
      "2024-11-14 04:43:09,414 - root - INFO - KG Training: Epoch 0001 Iter 2675 / 3136 | Time 0.1s | Iter Loss 0.0619 | Iter Mean Loss 0.1557\n",
      "2024-11-14 04:43:09,477 - root - INFO - KG Training: Epoch 0001 Iter 2676 / 3136 | Time 0.1s | Iter Loss 0.0599 | Iter Mean Loss 0.1557\n",
      "2024-11-14 04:43:09,538 - root - INFO - KG Training: Epoch 0001 Iter 2677 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1556\n",
      "2024-11-14 04:43:09,609 - root - INFO - KG Training: Epoch 0001 Iter 2678 / 3136 | Time 0.1s | Iter Loss 0.0580 | Iter Mean Loss 0.1556\n",
      "2024-11-14 04:43:09,687 - root - INFO - KG Training: Epoch 0001 Iter 2679 / 3136 | Time 0.1s | Iter Loss 0.0722 | Iter Mean Loss 0.1556\n",
      "2024-11-14 04:43:09,843 - root - INFO - KG Training: Epoch 0001 Iter 2680 / 3136 | Time 0.2s | Iter Loss 0.0664 | Iter Mean Loss 0.1555\n",
      "2024-11-14 04:43:09,976 - root - INFO - KG Training: Epoch 0001 Iter 2681 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1555\n",
      "2024-11-14 04:43:10,038 - root - INFO - KG Training: Epoch 0001 Iter 2682 / 3136 | Time 0.1s | Iter Loss 0.0609 | Iter Mean Loss 0.1555\n",
      "2024-11-14 04:43:10,099 - root - INFO - KG Training: Epoch 0001 Iter 2683 / 3136 | Time 0.1s | Iter Loss 0.0607 | Iter Mean Loss 0.1554\n",
      "2024-11-14 04:43:10,261 - root - INFO - KG Training: Epoch 0001 Iter 2684 / 3136 | Time 0.2s | Iter Loss 0.0682 | Iter Mean Loss 0.1554\n",
      "2024-11-14 04:43:10,322 - root - INFO - KG Training: Epoch 0001 Iter 2685 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1554\n",
      "2024-11-14 04:43:10,387 - root - INFO - KG Training: Epoch 0001 Iter 2686 / 3136 | Time 0.1s | Iter Loss 0.0753 | Iter Mean Loss 0.1553\n",
      "2024-11-14 04:43:10,446 - root - INFO - KG Training: Epoch 0001 Iter 2687 / 3136 | Time 0.1s | Iter Loss 0.0618 | Iter Mean Loss 0.1553\n",
      "2024-11-14 04:43:10,508 - root - INFO - KG Training: Epoch 0001 Iter 2688 / 3136 | Time 0.1s | Iter Loss 0.0644 | Iter Mean Loss 0.1553\n",
      "2024-11-14 04:43:10,571 - root - INFO - KG Training: Epoch 0001 Iter 2689 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1552\n",
      "2024-11-14 04:43:10,634 - root - INFO - KG Training: Epoch 0001 Iter 2690 / 3136 | Time 0.1s | Iter Loss 0.0614 | Iter Mean Loss 0.1552\n",
      "2024-11-14 04:43:10,694 - root - INFO - KG Training: Epoch 0001 Iter 2691 / 3136 | Time 0.1s | Iter Loss 0.0717 | Iter Mean Loss 0.1552\n",
      "2024-11-14 04:43:10,755 - root - INFO - KG Training: Epoch 0001 Iter 2692 / 3136 | Time 0.1s | Iter Loss 0.0595 | Iter Mean Loss 0.1551\n",
      "2024-11-14 04:43:10,815 - root - INFO - KG Training: Epoch 0001 Iter 2693 / 3136 | Time 0.1s | Iter Loss 0.0581 | Iter Mean Loss 0.1551\n",
      "2024-11-14 04:43:10,875 - root - INFO - KG Training: Epoch 0001 Iter 2694 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1551\n",
      "2024-11-14 04:43:10,940 - root - INFO - KG Training: Epoch 0001 Iter 2695 / 3136 | Time 0.1s | Iter Loss 0.0707 | Iter Mean Loss 0.1550\n",
      "2024-11-14 04:43:11,015 - root - INFO - KG Training: Epoch 0001 Iter 2696 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1550\n",
      "2024-11-14 04:43:11,092 - root - INFO - KG Training: Epoch 0001 Iter 2697 / 3136 | Time 0.1s | Iter Loss 0.0602 | Iter Mean Loss 0.1550\n",
      "2024-11-14 04:43:11,199 - root - INFO - KG Training: Epoch 0001 Iter 2698 / 3136 | Time 0.1s | Iter Loss 0.0568 | Iter Mean Loss 0.1549\n",
      "2024-11-14 04:43:11,257 - root - INFO - KG Training: Epoch 0001 Iter 2699 / 3136 | Time 0.1s | Iter Loss 0.0579 | Iter Mean Loss 0.1549\n",
      "2024-11-14 04:43:11,318 - root - INFO - KG Training: Epoch 0001 Iter 2700 / 3136 | Time 0.1s | Iter Loss 0.0560 | Iter Mean Loss 0.1549\n",
      "2024-11-14 04:43:11,377 - root - INFO - KG Training: Epoch 0001 Iter 2701 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1548\n",
      "2024-11-14 04:43:11,435 - root - INFO - KG Training: Epoch 0001 Iter 2702 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1548\n",
      "2024-11-14 04:43:11,493 - root - INFO - KG Training: Epoch 0001 Iter 2703 / 3136 | Time 0.1s | Iter Loss 0.0581 | Iter Mean Loss 0.1548\n",
      "2024-11-14 04:43:11,551 - root - INFO - KG Training: Epoch 0001 Iter 2704 / 3136 | Time 0.1s | Iter Loss 0.0637 | Iter Mean Loss 0.1547\n",
      "2024-11-14 04:43:11,610 - root - INFO - KG Training: Epoch 0001 Iter 2705 / 3136 | Time 0.1s | Iter Loss 0.0682 | Iter Mean Loss 0.1547\n",
      "2024-11-14 04:43:11,667 - root - INFO - KG Training: Epoch 0001 Iter 2706 / 3136 | Time 0.1s | Iter Loss 0.0661 | Iter Mean Loss 0.1547\n",
      "2024-11-14 04:43:11,726 - root - INFO - KG Training: Epoch 0001 Iter 2707 / 3136 | Time 0.1s | Iter Loss 0.0608 | Iter Mean Loss 0.1546\n",
      "2024-11-14 04:43:11,787 - root - INFO - KG Training: Epoch 0001 Iter 2708 / 3136 | Time 0.1s | Iter Loss 0.0517 | Iter Mean Loss 0.1546\n",
      "2024-11-14 04:43:11,847 - root - INFO - KG Training: Epoch 0001 Iter 2709 / 3136 | Time 0.1s | Iter Loss 0.0560 | Iter Mean Loss 0.1546\n",
      "2024-11-14 04:43:11,908 - root - INFO - KG Training: Epoch 0001 Iter 2710 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1545\n",
      "2024-11-14 04:43:11,970 - root - INFO - KG Training: Epoch 0001 Iter 2711 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1545\n",
      "2024-11-14 04:43:12,029 - root - INFO - KG Training: Epoch 0001 Iter 2712 / 3136 | Time 0.1s | Iter Loss 0.0631 | Iter Mean Loss 0.1545\n",
      "2024-11-14 04:43:12,104 - root - INFO - KG Training: Epoch 0001 Iter 2713 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1544\n",
      "2024-11-14 04:43:12,179 - root - INFO - KG Training: Epoch 0001 Iter 2714 / 3136 | Time 0.1s | Iter Loss 0.0734 | Iter Mean Loss 0.1544\n",
      "2024-11-14 04:43:12,254 - root - INFO - KG Training: Epoch 0001 Iter 2715 / 3136 | Time 0.1s | Iter Loss 0.0619 | Iter Mean Loss 0.1544\n",
      "2024-11-14 04:43:12,365 - root - INFO - KG Training: Epoch 0001 Iter 2716 / 3136 | Time 0.1s | Iter Loss 0.0610 | Iter Mean Loss 0.1543\n",
      "2024-11-14 04:43:12,426 - root - INFO - KG Training: Epoch 0001 Iter 2717 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1543\n",
      "2024-11-14 04:43:12,485 - root - INFO - KG Training: Epoch 0001 Iter 2718 / 3136 | Time 0.1s | Iter Loss 0.0642 | Iter Mean Loss 0.1543\n",
      "2024-11-14 04:43:12,545 - root - INFO - KG Training: Epoch 0001 Iter 2719 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1542\n",
      "2024-11-14 04:43:12,607 - root - INFO - KG Training: Epoch 0001 Iter 2720 / 3136 | Time 0.1s | Iter Loss 0.0748 | Iter Mean Loss 0.1542\n",
      "2024-11-14 04:43:12,676 - root - INFO - KG Training: Epoch 0001 Iter 2721 / 3136 | Time 0.1s | Iter Loss 0.0583 | Iter Mean Loss 0.1542\n",
      "2024-11-14 04:43:12,739 - root - INFO - KG Training: Epoch 0001 Iter 2722 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1541\n",
      "2024-11-14 04:43:12,801 - root - INFO - KG Training: Epoch 0001 Iter 2723 / 3136 | Time 0.1s | Iter Loss 0.0579 | Iter Mean Loss 0.1541\n",
      "2024-11-14 04:43:12,861 - root - INFO - KG Training: Epoch 0001 Iter 2724 / 3136 | Time 0.1s | Iter Loss 0.0481 | Iter Mean Loss 0.1541\n",
      "2024-11-14 04:43:12,926 - root - INFO - KG Training: Epoch 0001 Iter 2725 / 3136 | Time 0.1s | Iter Loss 0.0677 | Iter Mean Loss 0.1540\n",
      "2024-11-14 04:43:12,988 - root - INFO - KG Training: Epoch 0001 Iter 2726 / 3136 | Time 0.1s | Iter Loss 0.0714 | Iter Mean Loss 0.1540\n",
      "2024-11-14 04:43:13,051 - root - INFO - KG Training: Epoch 0001 Iter 2727 / 3136 | Time 0.1s | Iter Loss 0.0717 | Iter Mean Loss 0.1540\n",
      "2024-11-14 04:43:13,114 - root - INFO - KG Training: Epoch 0001 Iter 2728 / 3136 | Time 0.1s | Iter Loss 0.0574 | Iter Mean Loss 0.1539\n",
      "2024-11-14 04:43:13,173 - root - INFO - KG Training: Epoch 0001 Iter 2729 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1539\n",
      "2024-11-14 04:43:13,234 - root - INFO - KG Training: Epoch 0001 Iter 2730 / 3136 | Time 0.1s | Iter Loss 0.0580 | Iter Mean Loss 0.1539\n",
      "2024-11-14 04:43:13,297 - root - INFO - KG Training: Epoch 0001 Iter 2731 / 3136 | Time 0.1s | Iter Loss 0.0570 | Iter Mean Loss 0.1538\n",
      "2024-11-14 04:43:13,364 - root - INFO - KG Training: Epoch 0001 Iter 2732 / 3136 | Time 0.1s | Iter Loss 0.0593 | Iter Mean Loss 0.1538\n",
      "2024-11-14 04:43:13,428 - root - INFO - KG Training: Epoch 0001 Iter 2733 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1538\n",
      "2024-11-14 04:43:13,490 - root - INFO - KG Training: Epoch 0001 Iter 2734 / 3136 | Time 0.1s | Iter Loss 0.0603 | Iter Mean Loss 0.1537\n",
      "2024-11-14 04:43:13,550 - root - INFO - KG Training: Epoch 0001 Iter 2735 / 3136 | Time 0.1s | Iter Loss 0.0586 | Iter Mean Loss 0.1537\n",
      "2024-11-14 04:43:13,612 - root - INFO - KG Training: Epoch 0001 Iter 2736 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1537\n",
      "2024-11-14 04:43:13,673 - root - INFO - KG Training: Epoch 0001 Iter 2737 / 3136 | Time 0.1s | Iter Loss 0.0541 | Iter Mean Loss 0.1536\n",
      "2024-11-14 04:43:13,738 - root - INFO - KG Training: Epoch 0001 Iter 2738 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1536\n",
      "2024-11-14 04:43:13,801 - root - INFO - KG Training: Epoch 0001 Iter 2739 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1535\n",
      "2024-11-14 04:43:13,860 - root - INFO - KG Training: Epoch 0001 Iter 2740 / 3136 | Time 0.1s | Iter Loss 0.0600 | Iter Mean Loss 0.1535\n",
      "2024-11-14 04:43:13,923 - root - INFO - KG Training: Epoch 0001 Iter 2741 / 3136 | Time 0.1s | Iter Loss 0.0604 | Iter Mean Loss 0.1535\n",
      "2024-11-14 04:43:13,982 - root - INFO - KG Training: Epoch 0001 Iter 2742 / 3136 | Time 0.1s | Iter Loss 0.0560 | Iter Mean Loss 0.1534\n",
      "2024-11-14 04:43:14,042 - root - INFO - KG Training: Epoch 0001 Iter 2743 / 3136 | Time 0.1s | Iter Loss 0.0527 | Iter Mean Loss 0.1534\n",
      "2024-11-14 04:43:14,104 - root - INFO - KG Training: Epoch 0001 Iter 2744 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1534\n",
      "2024-11-14 04:43:14,162 - root - INFO - KG Training: Epoch 0001 Iter 2745 / 3136 | Time 0.1s | Iter Loss 0.0625 | Iter Mean Loss 0.1533\n",
      "2024-11-14 04:43:14,225 - root - INFO - KG Training: Epoch 0001 Iter 2746 / 3136 | Time 0.1s | Iter Loss 0.0575 | Iter Mean Loss 0.1533\n",
      "2024-11-14 04:43:14,286 - root - INFO - KG Training: Epoch 0001 Iter 2747 / 3136 | Time 0.1s | Iter Loss 0.0578 | Iter Mean Loss 0.1533\n",
      "2024-11-14 04:43:14,346 - root - INFO - KG Training: Epoch 0001 Iter 2748 / 3136 | Time 0.1s | Iter Loss 0.0518 | Iter Mean Loss 0.1532\n",
      "2024-11-14 04:43:14,404 - root - INFO - KG Training: Epoch 0001 Iter 2749 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1532\n",
      "2024-11-14 04:43:14,466 - root - INFO - KG Training: Epoch 0001 Iter 2750 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1532\n",
      "2024-11-14 04:43:14,526 - root - INFO - KG Training: Epoch 0001 Iter 2751 / 3136 | Time 0.1s | Iter Loss 0.0524 | Iter Mean Loss 0.1531\n",
      "2024-11-14 04:43:14,587 - root - INFO - KG Training: Epoch 0001 Iter 2752 / 3136 | Time 0.1s | Iter Loss 0.0702 | Iter Mean Loss 0.1531\n",
      "2024-11-14 04:43:14,649 - root - INFO - KG Training: Epoch 0001 Iter 2753 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1531\n",
      "2024-11-14 04:43:14,709 - root - INFO - KG Training: Epoch 0001 Iter 2754 / 3136 | Time 0.1s | Iter Loss 0.0587 | Iter Mean Loss 0.1530\n",
      "2024-11-14 04:43:14,768 - root - INFO - KG Training: Epoch 0001 Iter 2755 / 3136 | Time 0.1s | Iter Loss 0.0629 | Iter Mean Loss 0.1530\n",
      "2024-11-14 04:43:14,828 - root - INFO - KG Training: Epoch 0001 Iter 2756 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1530\n",
      "2024-11-14 04:43:14,888 - root - INFO - KG Training: Epoch 0001 Iter 2757 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1529\n",
      "2024-11-14 04:43:14,950 - root - INFO - KG Training: Epoch 0001 Iter 2758 / 3136 | Time 0.1s | Iter Loss 0.0589 | Iter Mean Loss 0.1529\n",
      "2024-11-14 04:43:15,010 - root - INFO - KG Training: Epoch 0001 Iter 2759 / 3136 | Time 0.1s | Iter Loss 0.0649 | Iter Mean Loss 0.1529\n",
      "2024-11-14 04:43:15,070 - root - INFO - KG Training: Epoch 0001 Iter 2760 / 3136 | Time 0.1s | Iter Loss 0.0543 | Iter Mean Loss 0.1528\n",
      "2024-11-14 04:43:15,421 - root - INFO - KG Training: Epoch 0001 Iter 2761 / 3136 | Time 0.4s | Iter Loss 0.0657 | Iter Mean Loss 0.1528\n",
      "2024-11-14 04:43:15,480 - root - INFO - KG Training: Epoch 0001 Iter 2762 / 3136 | Time 0.1s | Iter Loss 0.0700 | Iter Mean Loss 0.1528\n",
      "2024-11-14 04:43:15,639 - root - INFO - KG Training: Epoch 0001 Iter 2763 / 3136 | Time 0.2s | Iter Loss 0.0628 | Iter Mean Loss 0.1527\n",
      "2024-11-14 04:43:15,700 - root - INFO - KG Training: Epoch 0001 Iter 2764 / 3136 | Time 0.1s | Iter Loss 0.0588 | Iter Mean Loss 0.1527\n",
      "2024-11-14 04:43:15,760 - root - INFO - KG Training: Epoch 0001 Iter 2765 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1527\n",
      "2024-11-14 04:43:15,818 - root - INFO - KG Training: Epoch 0001 Iter 2766 / 3136 | Time 0.1s | Iter Loss 0.0647 | Iter Mean Loss 0.1526\n",
      "2024-11-14 04:43:15,876 - root - INFO - KG Training: Epoch 0001 Iter 2767 / 3136 | Time 0.1s | Iter Loss 0.0611 | Iter Mean Loss 0.1526\n",
      "2024-11-14 04:43:15,936 - root - INFO - KG Training: Epoch 0001 Iter 2768 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1526\n",
      "2024-11-14 04:43:15,994 - root - INFO - KG Training: Epoch 0001 Iter 2769 / 3136 | Time 0.1s | Iter Loss 0.0602 | Iter Mean Loss 0.1525\n",
      "2024-11-14 04:43:16,055 - root - INFO - KG Training: Epoch 0001 Iter 2770 / 3136 | Time 0.1s | Iter Loss 0.0687 | Iter Mean Loss 0.1525\n",
      "2024-11-14 04:43:16,114 - root - INFO - KG Training: Epoch 0001 Iter 2771 / 3136 | Time 0.1s | Iter Loss 0.0590 | Iter Mean Loss 0.1525\n",
      "2024-11-14 04:43:16,173 - root - INFO - KG Training: Epoch 0001 Iter 2772 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1525\n",
      "2024-11-14 04:43:16,233 - root - INFO - KG Training: Epoch 0001 Iter 2773 / 3136 | Time 0.1s | Iter Loss 0.0619 | Iter Mean Loss 0.1524\n",
      "2024-11-14 04:43:16,297 - root - INFO - KG Training: Epoch 0001 Iter 2774 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1524\n",
      "2024-11-14 04:43:16,357 - root - INFO - KG Training: Epoch 0001 Iter 2775 / 3136 | Time 0.1s | Iter Loss 0.0569 | Iter Mean Loss 0.1524\n",
      "2024-11-14 04:43:16,417 - root - INFO - KG Training: Epoch 0001 Iter 2776 / 3136 | Time 0.1s | Iter Loss 0.0541 | Iter Mean Loss 0.1523\n",
      "2024-11-14 04:43:16,478 - root - INFO - KG Training: Epoch 0001 Iter 2777 / 3136 | Time 0.1s | Iter Loss 0.0566 | Iter Mean Loss 0.1523\n",
      "2024-11-14 04:43:16,539 - root - INFO - KG Training: Epoch 0001 Iter 2778 / 3136 | Time 0.1s | Iter Loss 0.0552 | Iter Mean Loss 0.1522\n",
      "2024-11-14 04:43:16,605 - root - INFO - KG Training: Epoch 0001 Iter 2779 / 3136 | Time 0.1s | Iter Loss 0.0621 | Iter Mean Loss 0.1522\n",
      "2024-11-14 04:43:16,664 - root - INFO - KG Training: Epoch 0001 Iter 2780 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1522\n",
      "2024-11-14 04:43:16,723 - root - INFO - KG Training: Epoch 0001 Iter 2781 / 3136 | Time 0.1s | Iter Loss 0.0726 | Iter Mean Loss 0.1522\n",
      "2024-11-14 04:43:16,921 - root - INFO - KG Training: Epoch 0001 Iter 2782 / 3136 | Time 0.2s | Iter Loss 0.0664 | Iter Mean Loss 0.1521\n",
      "2024-11-14 04:43:16,981 - root - INFO - KG Training: Epoch 0001 Iter 2783 / 3136 | Time 0.1s | Iter Loss 0.0657 | Iter Mean Loss 0.1521\n",
      "2024-11-14 04:43:17,040 - root - INFO - KG Training: Epoch 0001 Iter 2784 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1521\n",
      "2024-11-14 04:43:17,102 - root - INFO - KG Training: Epoch 0001 Iter 2785 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1520\n",
      "2024-11-14 04:43:17,162 - root - INFO - KG Training: Epoch 0001 Iter 2786 / 3136 | Time 0.1s | Iter Loss 0.0625 | Iter Mean Loss 0.1520\n",
      "2024-11-14 04:43:17,223 - root - INFO - KG Training: Epoch 0001 Iter 2787 / 3136 | Time 0.1s | Iter Loss 0.0612 | Iter Mean Loss 0.1520\n",
      "2024-11-14 04:43:17,284 - root - INFO - KG Training: Epoch 0001 Iter 2788 / 3136 | Time 0.1s | Iter Loss 0.0590 | Iter Mean Loss 0.1519\n",
      "2024-11-14 04:43:17,344 - root - INFO - KG Training: Epoch 0001 Iter 2789 / 3136 | Time 0.1s | Iter Loss 0.0578 | Iter Mean Loss 0.1519\n",
      "2024-11-14 04:43:17,403 - root - INFO - KG Training: Epoch 0001 Iter 2790 / 3136 | Time 0.1s | Iter Loss 0.0739 | Iter Mean Loss 0.1519\n",
      "2024-11-14 04:43:17,462 - root - INFO - KG Training: Epoch 0001 Iter 2791 / 3136 | Time 0.1s | Iter Loss 0.0583 | Iter Mean Loss 0.1518\n",
      "2024-11-14 04:43:17,521 - root - INFO - KG Training: Epoch 0001 Iter 2792 / 3136 | Time 0.1s | Iter Loss 0.0660 | Iter Mean Loss 0.1518\n",
      "2024-11-14 04:43:17,582 - root - INFO - KG Training: Epoch 0001 Iter 2793 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1518\n",
      "2024-11-14 04:43:17,642 - root - INFO - KG Training: Epoch 0001 Iter 2794 / 3136 | Time 0.1s | Iter Loss 0.0583 | Iter Mean Loss 0.1517\n",
      "2024-11-14 04:43:17,699 - root - INFO - KG Training: Epoch 0001 Iter 2795 / 3136 | Time 0.1s | Iter Loss 0.0641 | Iter Mean Loss 0.1517\n",
      "2024-11-14 04:43:17,759 - root - INFO - KG Training: Epoch 0001 Iter 2796 / 3136 | Time 0.1s | Iter Loss 0.0730 | Iter Mean Loss 0.1517\n",
      "2024-11-14 04:43:17,818 - root - INFO - KG Training: Epoch 0001 Iter 2797 / 3136 | Time 0.1s | Iter Loss 0.0574 | Iter Mean Loss 0.1516\n",
      "2024-11-14 04:43:17,876 - root - INFO - KG Training: Epoch 0001 Iter 2798 / 3136 | Time 0.1s | Iter Loss 0.0565 | Iter Mean Loss 0.1516\n",
      "2024-11-14 04:43:17,939 - root - INFO - KG Training: Epoch 0001 Iter 2799 / 3136 | Time 0.1s | Iter Loss 0.0584 | Iter Mean Loss 0.1516\n",
      "2024-11-14 04:43:18,000 - root - INFO - KG Training: Epoch 0001 Iter 2800 / 3136 | Time 0.1s | Iter Loss 0.0553 | Iter Mean Loss 0.1515\n",
      "2024-11-14 04:43:18,060 - root - INFO - KG Training: Epoch 0001 Iter 2801 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1515\n",
      "2024-11-14 04:43:18,120 - root - INFO - KG Training: Epoch 0001 Iter 2802 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1515\n",
      "2024-11-14 04:43:18,181 - root - INFO - KG Training: Epoch 0001 Iter 2803 / 3136 | Time 0.1s | Iter Loss 0.0637 | Iter Mean Loss 0.1515\n",
      "2024-11-14 04:43:18,240 - root - INFO - KG Training: Epoch 0001 Iter 2804 / 3136 | Time 0.1s | Iter Loss 0.0577 | Iter Mean Loss 0.1514\n",
      "2024-11-14 04:43:18,301 - root - INFO - KG Training: Epoch 0001 Iter 2805 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1514\n",
      "2024-11-14 04:43:18,360 - root - INFO - KG Training: Epoch 0001 Iter 2806 / 3136 | Time 0.1s | Iter Loss 0.0541 | Iter Mean Loss 0.1513\n",
      "2024-11-14 04:43:18,422 - root - INFO - KG Training: Epoch 0001 Iter 2807 / 3136 | Time 0.1s | Iter Loss 0.0609 | Iter Mean Loss 0.1513\n",
      "2024-11-14 04:43:18,484 - root - INFO - KG Training: Epoch 0001 Iter 2808 / 3136 | Time 0.1s | Iter Loss 0.0600 | Iter Mean Loss 0.1513\n",
      "2024-11-14 04:43:18,543 - root - INFO - KG Training: Epoch 0001 Iter 2809 / 3136 | Time 0.1s | Iter Loss 0.0652 | Iter Mean Loss 0.1513\n",
      "2024-11-14 04:43:18,702 - root - INFO - KG Training: Epoch 0001 Iter 2810 / 3136 | Time 0.2s | Iter Loss 0.0668 | Iter Mean Loss 0.1512\n",
      "2024-11-14 04:43:18,762 - root - INFO - KG Training: Epoch 0001 Iter 2811 / 3136 | Time 0.1s | Iter Loss 0.0584 | Iter Mean Loss 0.1512\n",
      "2024-11-14 04:43:18,877 - root - INFO - KG Training: Epoch 0001 Iter 2812 / 3136 | Time 0.1s | Iter Loss 0.0616 | Iter Mean Loss 0.1512\n",
      "2024-11-14 04:43:18,940 - root - INFO - KG Training: Epoch 0001 Iter 2813 / 3136 | Time 0.1s | Iter Loss 0.0635 | Iter Mean Loss 0.1511\n",
      "2024-11-14 04:43:19,000 - root - INFO - KG Training: Epoch 0001 Iter 2814 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1511\n",
      "2024-11-14 04:43:19,059 - root - INFO - KG Training: Epoch 0001 Iter 2815 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1511\n",
      "2024-11-14 04:43:19,121 - root - INFO - KG Training: Epoch 0001 Iter 2816 / 3136 | Time 0.1s | Iter Loss 0.0557 | Iter Mean Loss 0.1510\n",
      "2024-11-14 04:43:19,183 - root - INFO - KG Training: Epoch 0001 Iter 2817 / 3136 | Time 0.1s | Iter Loss 0.0643 | Iter Mean Loss 0.1510\n",
      "2024-11-14 04:43:19,241 - root - INFO - KG Training: Epoch 0001 Iter 2818 / 3136 | Time 0.1s | Iter Loss 0.0561 | Iter Mean Loss 0.1510\n",
      "2024-11-14 04:43:19,303 - root - INFO - KG Training: Epoch 0001 Iter 2819 / 3136 | Time 0.1s | Iter Loss 0.0692 | Iter Mean Loss 0.1509\n",
      "2024-11-14 04:43:19,364 - root - INFO - KG Training: Epoch 0001 Iter 2820 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1509\n",
      "2024-11-14 04:43:19,423 - root - INFO - KG Training: Epoch 0001 Iter 2821 / 3136 | Time 0.1s | Iter Loss 0.0528 | Iter Mean Loss 0.1509\n",
      "2024-11-14 04:43:19,483 - root - INFO - KG Training: Epoch 0001 Iter 2822 / 3136 | Time 0.1s | Iter Loss 0.0641 | Iter Mean Loss 0.1508\n",
      "2024-11-14 04:43:19,594 - root - INFO - KG Training: Epoch 0001 Iter 2823 / 3136 | Time 0.1s | Iter Loss 0.0575 | Iter Mean Loss 0.1508\n",
      "2024-11-14 04:43:19,654 - root - INFO - KG Training: Epoch 0001 Iter 2824 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1508\n",
      "2024-11-14 04:43:19,716 - root - INFO - KG Training: Epoch 0001 Iter 2825 / 3136 | Time 0.1s | Iter Loss 0.0550 | Iter Mean Loss 0.1507\n",
      "2024-11-14 04:43:19,775 - root - INFO - KG Training: Epoch 0001 Iter 2826 / 3136 | Time 0.1s | Iter Loss 0.0499 | Iter Mean Loss 0.1507\n",
      "2024-11-14 04:43:19,835 - root - INFO - KG Training: Epoch 0001 Iter 2827 / 3136 | Time 0.1s | Iter Loss 0.0717 | Iter Mean Loss 0.1507\n",
      "2024-11-14 04:43:19,899 - root - INFO - KG Training: Epoch 0001 Iter 2828 / 3136 | Time 0.1s | Iter Loss 0.0598 | Iter Mean Loss 0.1506\n",
      "2024-11-14 04:43:19,956 - root - INFO - KG Training: Epoch 0001 Iter 2829 / 3136 | Time 0.1s | Iter Loss 0.0618 | Iter Mean Loss 0.1506\n",
      "2024-11-14 04:43:20,015 - root - INFO - KG Training: Epoch 0001 Iter 2830 / 3136 | Time 0.1s | Iter Loss 0.0678 | Iter Mean Loss 0.1506\n",
      "2024-11-14 04:43:20,075 - root - INFO - KG Training: Epoch 0001 Iter 2831 / 3136 | Time 0.1s | Iter Loss 0.0604 | Iter Mean Loss 0.1506\n",
      "2024-11-14 04:43:20,134 - root - INFO - KG Training: Epoch 0001 Iter 2832 / 3136 | Time 0.1s | Iter Loss 0.0527 | Iter Mean Loss 0.1505\n",
      "2024-11-14 04:43:20,195 - root - INFO - KG Training: Epoch 0001 Iter 2833 / 3136 | Time 0.1s | Iter Loss 0.0684 | Iter Mean Loss 0.1505\n",
      "2024-11-14 04:43:20,254 - root - INFO - KG Training: Epoch 0001 Iter 2834 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1505\n",
      "2024-11-14 04:43:20,455 - root - INFO - KG Training: Epoch 0001 Iter 2835 / 3136 | Time 0.2s | Iter Loss 0.0634 | Iter Mean Loss 0.1504\n",
      "2024-11-14 04:43:20,514 - root - INFO - KG Training: Epoch 0001 Iter 2836 / 3136 | Time 0.1s | Iter Loss 0.0577 | Iter Mean Loss 0.1504\n",
      "2024-11-14 04:43:20,574 - root - INFO - KG Training: Epoch 0001 Iter 2837 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1504\n",
      "2024-11-14 04:43:20,635 - root - INFO - KG Training: Epoch 0001 Iter 2838 / 3136 | Time 0.1s | Iter Loss 0.0672 | Iter Mean Loss 0.1503\n",
      "2024-11-14 04:43:20,813 - root - INFO - KG Training: Epoch 0001 Iter 2839 / 3136 | Time 0.2s | Iter Loss 0.0599 | Iter Mean Loss 0.1503\n",
      "2024-11-14 04:43:20,871 - root - INFO - KG Training: Epoch 0001 Iter 2840 / 3136 | Time 0.1s | Iter Loss 0.0754 | Iter Mean Loss 0.1503\n",
      "2024-11-14 04:43:20,928 - root - INFO - KG Training: Epoch 0001 Iter 2841 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1503\n",
      "2024-11-14 04:43:20,987 - root - INFO - KG Training: Epoch 0001 Iter 2842 / 3136 | Time 0.1s | Iter Loss 0.0538 | Iter Mean Loss 0.1502\n",
      "2024-11-14 04:43:21,098 - root - INFO - KG Training: Epoch 0001 Iter 2843 / 3136 | Time 0.1s | Iter Loss 0.0574 | Iter Mean Loss 0.1502\n",
      "2024-11-14 04:43:21,157 - root - INFO - KG Training: Epoch 0001 Iter 2844 / 3136 | Time 0.1s | Iter Loss 0.0625 | Iter Mean Loss 0.1502\n",
      "2024-11-14 04:43:21,220 - root - INFO - KG Training: Epoch 0001 Iter 2845 / 3136 | Time 0.1s | Iter Loss 0.0604 | Iter Mean Loss 0.1501\n",
      "2024-11-14 04:43:21,280 - root - INFO - KG Training: Epoch 0001 Iter 2846 / 3136 | Time 0.1s | Iter Loss 0.0659 | Iter Mean Loss 0.1501\n",
      "2024-11-14 04:43:21,341 - root - INFO - KG Training: Epoch 0001 Iter 2847 / 3136 | Time 0.1s | Iter Loss 0.0614 | Iter Mean Loss 0.1501\n",
      "2024-11-14 04:43:21,399 - root - INFO - KG Training: Epoch 0001 Iter 2848 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1500\n",
      "2024-11-14 04:43:21,459 - root - INFO - KG Training: Epoch 0001 Iter 2849 / 3136 | Time 0.1s | Iter Loss 0.0540 | Iter Mean Loss 0.1500\n",
      "2024-11-14 04:43:21,522 - root - INFO - KG Training: Epoch 0001 Iter 2850 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1500\n",
      "2024-11-14 04:43:21,641 - root - INFO - KG Training: Epoch 0001 Iter 2851 / 3136 | Time 0.1s | Iter Loss 0.0742 | Iter Mean Loss 0.1499\n",
      "2024-11-14 04:43:21,700 - root - INFO - KG Training: Epoch 0001 Iter 2852 / 3136 | Time 0.1s | Iter Loss 0.0602 | Iter Mean Loss 0.1499\n",
      "2024-11-14 04:43:21,788 - root - INFO - KG Training: Epoch 0001 Iter 2853 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1499\n",
      "2024-11-14 04:43:21,848 - root - INFO - KG Training: Epoch 0001 Iter 2854 / 3136 | Time 0.1s | Iter Loss 0.0647 | Iter Mean Loss 0.1498\n",
      "2024-11-14 04:43:21,911 - root - INFO - KG Training: Epoch 0001 Iter 2855 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1498\n",
      "2024-11-14 04:43:21,967 - root - INFO - KG Training: Epoch 0001 Iter 2856 / 3136 | Time 0.1s | Iter Loss 0.0703 | Iter Mean Loss 0.1498\n",
      "2024-11-14 04:43:22,025 - root - INFO - KG Training: Epoch 0001 Iter 2857 / 3136 | Time 0.1s | Iter Loss 0.0693 | Iter Mean Loss 0.1498\n",
      "2024-11-14 04:43:22,083 - root - INFO - KG Training: Epoch 0001 Iter 2858 / 3136 | Time 0.1s | Iter Loss 0.0587 | Iter Mean Loss 0.1497\n",
      "2024-11-14 04:43:22,143 - root - INFO - KG Training: Epoch 0001 Iter 2859 / 3136 | Time 0.1s | Iter Loss 0.0691 | Iter Mean Loss 0.1497\n",
      "2024-11-14 04:43:22,204 - root - INFO - KG Training: Epoch 0001 Iter 2860 / 3136 | Time 0.1s | Iter Loss 0.0695 | Iter Mean Loss 0.1497\n",
      "2024-11-14 04:43:22,265 - root - INFO - KG Training: Epoch 0001 Iter 2861 / 3136 | Time 0.1s | Iter Loss 0.0654 | Iter Mean Loss 0.1496\n",
      "2024-11-14 04:43:22,325 - root - INFO - KG Training: Epoch 0001 Iter 2862 / 3136 | Time 0.1s | Iter Loss 0.0665 | Iter Mean Loss 0.1496\n",
      "2024-11-14 04:43:22,386 - root - INFO - KG Training: Epoch 0001 Iter 2863 / 3136 | Time 0.1s | Iter Loss 0.0588 | Iter Mean Loss 0.1496\n",
      "2024-11-14 04:43:22,444 - root - INFO - KG Training: Epoch 0001 Iter 2864 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1496\n",
      "2024-11-14 04:43:22,505 - root - INFO - KG Training: Epoch 0001 Iter 2865 / 3136 | Time 0.1s | Iter Loss 0.0578 | Iter Mean Loss 0.1495\n",
      "2024-11-14 04:43:22,565 - root - INFO - KG Training: Epoch 0001 Iter 2866 / 3136 | Time 0.1s | Iter Loss 0.0618 | Iter Mean Loss 0.1495\n",
      "2024-11-14 04:43:22,625 - root - INFO - KG Training: Epoch 0001 Iter 2867 / 3136 | Time 0.1s | Iter Loss 0.0644 | Iter Mean Loss 0.1495\n",
      "2024-11-14 04:43:22,686 - root - INFO - KG Training: Epoch 0001 Iter 2868 / 3136 | Time 0.1s | Iter Loss 0.0679 | Iter Mean Loss 0.1494\n",
      "2024-11-14 04:43:22,745 - root - INFO - KG Training: Epoch 0001 Iter 2869 / 3136 | Time 0.1s | Iter Loss 0.0550 | Iter Mean Loss 0.1494\n",
      "2024-11-14 04:43:22,805 - root - INFO - KG Training: Epoch 0001 Iter 2870 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1494\n",
      "2024-11-14 04:43:22,865 - root - INFO - KG Training: Epoch 0001 Iter 2871 / 3136 | Time 0.1s | Iter Loss 0.0590 | Iter Mean Loss 0.1493\n",
      "2024-11-14 04:43:22,926 - root - INFO - KG Training: Epoch 0001 Iter 2872 / 3136 | Time 0.1s | Iter Loss 0.0606 | Iter Mean Loss 0.1493\n",
      "2024-11-14 04:43:22,989 - root - INFO - KG Training: Epoch 0001 Iter 2873 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1493\n",
      "2024-11-14 04:43:23,048 - root - INFO - KG Training: Epoch 0001 Iter 2874 / 3136 | Time 0.1s | Iter Loss 0.0618 | Iter Mean Loss 0.1492\n",
      "2024-11-14 04:43:23,108 - root - INFO - KG Training: Epoch 0001 Iter 2875 / 3136 | Time 0.1s | Iter Loss 0.0614 | Iter Mean Loss 0.1492\n",
      "2024-11-14 04:43:23,170 - root - INFO - KG Training: Epoch 0001 Iter 2876 / 3136 | Time 0.1s | Iter Loss 0.0591 | Iter Mean Loss 0.1492\n",
      "2024-11-14 04:43:23,231 - root - INFO - KG Training: Epoch 0001 Iter 2877 / 3136 | Time 0.1s | Iter Loss 0.0532 | Iter Mean Loss 0.1492\n",
      "2024-11-14 04:43:23,290 - root - INFO - KG Training: Epoch 0001 Iter 2878 / 3136 | Time 0.1s | Iter Loss 0.0525 | Iter Mean Loss 0.1491\n",
      "2024-11-14 04:43:23,349 - root - INFO - KG Training: Epoch 0001 Iter 2879 / 3136 | Time 0.1s | Iter Loss 0.0671 | Iter Mean Loss 0.1491\n",
      "2024-11-14 04:43:23,409 - root - INFO - KG Training: Epoch 0001 Iter 2880 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1491\n",
      "2024-11-14 04:43:23,471 - root - INFO - KG Training: Epoch 0001 Iter 2881 / 3136 | Time 0.1s | Iter Loss 0.0571 | Iter Mean Loss 0.1490\n",
      "2024-11-14 04:43:23,530 - root - INFO - KG Training: Epoch 0001 Iter 2882 / 3136 | Time 0.1s | Iter Loss 0.0597 | Iter Mean Loss 0.1490\n",
      "2024-11-14 04:43:23,640 - root - INFO - KG Training: Epoch 0001 Iter 2883 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1490\n",
      "2024-11-14 04:43:23,699 - root - INFO - KG Training: Epoch 0001 Iter 2884 / 3136 | Time 0.1s | Iter Loss 0.0547 | Iter Mean Loss 0.1489\n",
      "2024-11-14 04:43:23,762 - root - INFO - KG Training: Epoch 0001 Iter 2885 / 3136 | Time 0.1s | Iter Loss 0.0598 | Iter Mean Loss 0.1489\n",
      "2024-11-14 04:43:23,823 - root - INFO - KG Training: Epoch 0001 Iter 2886 / 3136 | Time 0.1s | Iter Loss 0.0723 | Iter Mean Loss 0.1489\n",
      "2024-11-14 04:43:23,887 - root - INFO - KG Training: Epoch 0001 Iter 2887 / 3136 | Time 0.1s | Iter Loss 0.0689 | Iter Mean Loss 0.1488\n",
      "2024-11-14 04:43:23,948 - root - INFO - KG Training: Epoch 0001 Iter 2888 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1488\n",
      "2024-11-14 04:43:24,009 - root - INFO - KG Training: Epoch 0001 Iter 2889 / 3136 | Time 0.1s | Iter Loss 0.0651 | Iter Mean Loss 0.1488\n",
      "2024-11-14 04:43:24,069 - root - INFO - KG Training: Epoch 0001 Iter 2890 / 3136 | Time 0.1s | Iter Loss 0.0550 | Iter Mean Loss 0.1488\n",
      "2024-11-14 04:43:24,180 - root - INFO - KG Training: Epoch 0001 Iter 2891 / 3136 | Time 0.1s | Iter Loss 0.0678 | Iter Mean Loss 0.1487\n",
      "2024-11-14 04:43:24,243 - root - INFO - KG Training: Epoch 0001 Iter 2892 / 3136 | Time 0.1s | Iter Loss 0.0577 | Iter Mean Loss 0.1487\n",
      "2024-11-14 04:43:24,307 - root - INFO - KG Training: Epoch 0001 Iter 2893 / 3136 | Time 0.1s | Iter Loss 0.0578 | Iter Mean Loss 0.1487\n",
      "2024-11-14 04:43:24,371 - root - INFO - KG Training: Epoch 0001 Iter 2894 / 3136 | Time 0.1s | Iter Loss 0.0546 | Iter Mean Loss 0.1486\n",
      "2024-11-14 04:43:24,433 - root - INFO - KG Training: Epoch 0001 Iter 2895 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1486\n",
      "2024-11-14 04:43:24,497 - root - INFO - KG Training: Epoch 0001 Iter 2896 / 3136 | Time 0.1s | Iter Loss 0.0592 | Iter Mean Loss 0.1486\n",
      "2024-11-14 04:43:24,560 - root - INFO - KG Training: Epoch 0001 Iter 2897 / 3136 | Time 0.1s | Iter Loss 0.0626 | Iter Mean Loss 0.1485\n",
      "2024-11-14 04:43:24,624 - root - INFO - KG Training: Epoch 0001 Iter 2898 / 3136 | Time 0.1s | Iter Loss 0.0592 | Iter Mean Loss 0.1485\n",
      "2024-11-14 04:43:24,688 - root - INFO - KG Training: Epoch 0001 Iter 2899 / 3136 | Time 0.1s | Iter Loss 0.0563 | Iter Mean Loss 0.1485\n",
      "2024-11-14 04:43:24,752 - root - INFO - KG Training: Epoch 0001 Iter 2900 / 3136 | Time 0.1s | Iter Loss 0.0681 | Iter Mean Loss 0.1485\n",
      "2024-11-14 04:43:24,814 - root - INFO - KG Training: Epoch 0001 Iter 2901 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1484\n",
      "2024-11-14 04:43:24,878 - root - INFO - KG Training: Epoch 0001 Iter 2902 / 3136 | Time 0.1s | Iter Loss 0.0622 | Iter Mean Loss 0.1484\n",
      "2024-11-14 04:43:24,942 - root - INFO - KG Training: Epoch 0001 Iter 2903 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1484\n",
      "2024-11-14 04:43:25,003 - root - INFO - KG Training: Epoch 0001 Iter 2904 / 3136 | Time 0.1s | Iter Loss 0.0667 | Iter Mean Loss 0.1483\n",
      "2024-11-14 04:43:25,070 - root - INFO - KG Training: Epoch 0001 Iter 2905 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1483\n",
      "2024-11-14 04:43:25,133 - root - INFO - KG Training: Epoch 0001 Iter 2906 / 3136 | Time 0.1s | Iter Loss 0.0583 | Iter Mean Loss 0.1483\n",
      "2024-11-14 04:43:25,193 - root - INFO - KG Training: Epoch 0001 Iter 2907 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1483\n",
      "2024-11-14 04:43:25,254 - root - INFO - KG Training: Epoch 0001 Iter 2908 / 3136 | Time 0.1s | Iter Loss 0.0589 | Iter Mean Loss 0.1482\n",
      "2024-11-14 04:43:25,316 - root - INFO - KG Training: Epoch 0001 Iter 2909 / 3136 | Time 0.1s | Iter Loss 0.0558 | Iter Mean Loss 0.1482\n",
      "2024-11-14 04:43:25,377 - root - INFO - KG Training: Epoch 0001 Iter 2910 / 3136 | Time 0.1s | Iter Loss 0.0626 | Iter Mean Loss 0.1482\n",
      "2024-11-14 04:43:25,439 - root - INFO - KG Training: Epoch 0001 Iter 2911 / 3136 | Time 0.1s | Iter Loss 0.0546 | Iter Mean Loss 0.1481\n",
      "2024-11-14 04:43:25,499 - root - INFO - KG Training: Epoch 0001 Iter 2912 / 3136 | Time 0.1s | Iter Loss 0.0655 | Iter Mean Loss 0.1481\n",
      "2024-11-14 04:43:25,559 - root - INFO - KG Training: Epoch 0001 Iter 2913 / 3136 | Time 0.1s | Iter Loss 0.0651 | Iter Mean Loss 0.1481\n",
      "2024-11-14 04:43:25,621 - root - INFO - KG Training: Epoch 0001 Iter 2914 / 3136 | Time 0.1s | Iter Loss 0.0502 | Iter Mean Loss 0.1480\n",
      "2024-11-14 04:43:25,682 - root - INFO - KG Training: Epoch 0001 Iter 2915 / 3136 | Time 0.1s | Iter Loss 0.0663 | Iter Mean Loss 0.1480\n",
      "2024-11-14 04:43:25,749 - root - INFO - KG Training: Epoch 0001 Iter 2916 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1480\n",
      "2024-11-14 04:43:25,810 - root - INFO - KG Training: Epoch 0001 Iter 2917 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1480\n",
      "2024-11-14 04:43:25,872 - root - INFO - KG Training: Epoch 0001 Iter 2918 / 3136 | Time 0.1s | Iter Loss 0.0565 | Iter Mean Loss 0.1479\n",
      "2024-11-14 04:43:25,933 - root - INFO - KG Training: Epoch 0001 Iter 2919 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1479\n",
      "2024-11-14 04:43:25,996 - root - INFO - KG Training: Epoch 0001 Iter 2920 / 3136 | Time 0.1s | Iter Loss 0.0605 | Iter Mean Loss 0.1479\n",
      "2024-11-14 04:43:26,062 - root - INFO - KG Training: Epoch 0001 Iter 2921 / 3136 | Time 0.1s | Iter Loss 0.0561 | Iter Mean Loss 0.1478\n",
      "2024-11-14 04:43:26,122 - root - INFO - KG Training: Epoch 0001 Iter 2922 / 3136 | Time 0.1s | Iter Loss 0.0547 | Iter Mean Loss 0.1478\n",
      "2024-11-14 04:43:26,183 - root - INFO - KG Training: Epoch 0001 Iter 2923 / 3136 | Time 0.1s | Iter Loss 0.0562 | Iter Mean Loss 0.1478\n",
      "2024-11-14 04:43:26,242 - root - INFO - KG Training: Epoch 0001 Iter 2924 / 3136 | Time 0.1s | Iter Loss 0.0590 | Iter Mean Loss 0.1477\n",
      "2024-11-14 04:43:26,305 - root - INFO - KG Training: Epoch 0001 Iter 2925 / 3136 | Time 0.1s | Iter Loss 0.0537 | Iter Mean Loss 0.1477\n",
      "2024-11-14 04:43:26,365 - root - INFO - KG Training: Epoch 0001 Iter 2926 / 3136 | Time 0.1s | Iter Loss 0.0582 | Iter Mean Loss 0.1477\n",
      "2024-11-14 04:43:26,425 - root - INFO - KG Training: Epoch 0001 Iter 2927 / 3136 | Time 0.1s | Iter Loss 0.0613 | Iter Mean Loss 0.1476\n",
      "2024-11-14 04:43:26,486 - root - INFO - KG Training: Epoch 0001 Iter 2928 / 3136 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1476\n",
      "2024-11-14 04:43:26,546 - root - INFO - KG Training: Epoch 0001 Iter 2929 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1476\n",
      "2024-11-14 04:43:26,608 - root - INFO - KG Training: Epoch 0001 Iter 2930 / 3136 | Time 0.1s | Iter Loss 0.0671 | Iter Mean Loss 0.1476\n",
      "2024-11-14 04:43:26,668 - root - INFO - KG Training: Epoch 0001 Iter 2931 / 3136 | Time 0.1s | Iter Loss 0.0636 | Iter Mean Loss 0.1475\n",
      "2024-11-14 04:43:26,730 - root - INFO - KG Training: Epoch 0001 Iter 2932 / 3136 | Time 0.1s | Iter Loss 0.0641 | Iter Mean Loss 0.1475\n",
      "2024-11-14 04:43:26,791 - root - INFO - KG Training: Epoch 0001 Iter 2933 / 3136 | Time 0.1s | Iter Loss 0.0574 | Iter Mean Loss 0.1475\n",
      "2024-11-14 04:43:26,851 - root - INFO - KG Training: Epoch 0001 Iter 2934 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1474\n",
      "2024-11-14 04:43:26,913 - root - INFO - KG Training: Epoch 0001 Iter 2935 / 3136 | Time 0.1s | Iter Loss 0.0562 | Iter Mean Loss 0.1474\n",
      "2024-11-14 04:43:26,973 - root - INFO - KG Training: Epoch 0001 Iter 2936 / 3136 | Time 0.1s | Iter Loss 0.0563 | Iter Mean Loss 0.1474\n",
      "2024-11-14 04:43:27,036 - root - INFO - KG Training: Epoch 0001 Iter 2937 / 3136 | Time 0.1s | Iter Loss 0.0688 | Iter Mean Loss 0.1474\n",
      "2024-11-14 04:43:27,098 - root - INFO - KG Training: Epoch 0001 Iter 2938 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1473\n",
      "2024-11-14 04:43:27,158 - root - INFO - KG Training: Epoch 0001 Iter 2939 / 3136 | Time 0.1s | Iter Loss 0.0573 | Iter Mean Loss 0.1473\n",
      "2024-11-14 04:43:27,221 - root - INFO - KG Training: Epoch 0001 Iter 2940 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1473\n",
      "2024-11-14 04:43:27,279 - root - INFO - KG Training: Epoch 0001 Iter 2941 / 3136 | Time 0.1s | Iter Loss 0.0589 | Iter Mean Loss 0.1472\n",
      "2024-11-14 04:43:27,350 - root - INFO - KG Training: Epoch 0001 Iter 2942 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1472\n",
      "2024-11-14 04:43:27,425 - root - INFO - KG Training: Epoch 0001 Iter 2943 / 3136 | Time 0.1s | Iter Loss 0.0637 | Iter Mean Loss 0.1472\n",
      "2024-11-14 04:43:27,490 - root - INFO - KG Training: Epoch 0001 Iter 2944 / 3136 | Time 0.1s | Iter Loss 0.0568 | Iter Mean Loss 0.1471\n",
      "2024-11-14 04:43:27,548 - root - INFO - KG Training: Epoch 0001 Iter 2945 / 3136 | Time 0.1s | Iter Loss 0.0606 | Iter Mean Loss 0.1471\n",
      "2024-11-14 04:43:27,625 - root - INFO - KG Training: Epoch 0001 Iter 2946 / 3136 | Time 0.1s | Iter Loss 0.0604 | Iter Mean Loss 0.1471\n",
      "2024-11-14 04:43:27,683 - root - INFO - KG Training: Epoch 0001 Iter 2947 / 3136 | Time 0.1s | Iter Loss 0.0668 | Iter Mean Loss 0.1471\n",
      "2024-11-14 04:43:27,759 - root - INFO - KG Training: Epoch 0001 Iter 2948 / 3136 | Time 0.1s | Iter Loss 0.0681 | Iter Mean Loss 0.1470\n",
      "2024-11-14 04:43:27,816 - root - INFO - KG Training: Epoch 0001 Iter 2949 / 3136 | Time 0.1s | Iter Loss 0.0559 | Iter Mean Loss 0.1470\n",
      "2024-11-14 04:43:27,886 - root - INFO - KG Training: Epoch 0001 Iter 2950 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1470\n",
      "2024-11-14 04:43:27,947 - root - INFO - KG Training: Epoch 0001 Iter 2951 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1469\n",
      "2024-11-14 04:43:28,009 - root - INFO - KG Training: Epoch 0001 Iter 2952 / 3136 | Time 0.1s | Iter Loss 0.0647 | Iter Mean Loss 0.1469\n",
      "2024-11-14 04:43:28,070 - root - INFO - KG Training: Epoch 0001 Iter 2953 / 3136 | Time 0.1s | Iter Loss 0.0573 | Iter Mean Loss 0.1469\n",
      "2024-11-14 04:43:28,132 - root - INFO - KG Training: Epoch 0001 Iter 2954 / 3136 | Time 0.1s | Iter Loss 0.0600 | Iter Mean Loss 0.1469\n",
      "2024-11-14 04:43:28,193 - root - INFO - KG Training: Epoch 0001 Iter 2955 / 3136 | Time 0.1s | Iter Loss 0.0513 | Iter Mean Loss 0.1468\n",
      "2024-11-14 04:43:28,254 - root - INFO - KG Training: Epoch 0001 Iter 2956 / 3136 | Time 0.1s | Iter Loss 0.0607 | Iter Mean Loss 0.1468\n",
      "2024-11-14 04:43:28,315 - root - INFO - KG Training: Epoch 0001 Iter 2957 / 3136 | Time 0.1s | Iter Loss 0.0587 | Iter Mean Loss 0.1468\n",
      "2024-11-14 04:43:28,376 - root - INFO - KG Training: Epoch 0001 Iter 2958 / 3136 | Time 0.1s | Iter Loss 0.0698 | Iter Mean Loss 0.1467\n",
      "2024-11-14 04:43:28,437 - root - INFO - KG Training: Epoch 0001 Iter 2959 / 3136 | Time 0.1s | Iter Loss 0.0603 | Iter Mean Loss 0.1467\n",
      "2024-11-14 04:43:28,497 - root - INFO - KG Training: Epoch 0001 Iter 2960 / 3136 | Time 0.1s | Iter Loss 0.0668 | Iter Mean Loss 0.1467\n",
      "2024-11-14 04:43:28,558 - root - INFO - KG Training: Epoch 0001 Iter 2961 / 3136 | Time 0.1s | Iter Loss 0.0581 | Iter Mean Loss 0.1467\n",
      "2024-11-14 04:43:28,618 - root - INFO - KG Training: Epoch 0001 Iter 2962 / 3136 | Time 0.1s | Iter Loss 0.0559 | Iter Mean Loss 0.1466\n",
      "2024-11-14 04:43:28,680 - root - INFO - KG Training: Epoch 0001 Iter 2963 / 3136 | Time 0.1s | Iter Loss 0.0628 | Iter Mean Loss 0.1466\n",
      "2024-11-14 04:43:28,743 - root - INFO - KG Training: Epoch 0001 Iter 2964 / 3136 | Time 0.1s | Iter Loss 0.0626 | Iter Mean Loss 0.1466\n",
      "2024-11-14 04:43:28,804 - root - INFO - KG Training: Epoch 0001 Iter 2965 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1465\n",
      "2024-11-14 04:43:28,864 - root - INFO - KG Training: Epoch 0001 Iter 2966 / 3136 | Time 0.1s | Iter Loss 0.0665 | Iter Mean Loss 0.1465\n",
      "2024-11-14 04:43:28,926 - root - INFO - KG Training: Epoch 0001 Iter 2967 / 3136 | Time 0.1s | Iter Loss 0.0673 | Iter Mean Loss 0.1465\n",
      "2024-11-14 04:43:28,986 - root - INFO - KG Training: Epoch 0001 Iter 2968 / 3136 | Time 0.1s | Iter Loss 0.0536 | Iter Mean Loss 0.1465\n",
      "2024-11-14 04:43:29,049 - root - INFO - KG Training: Epoch 0001 Iter 2969 / 3136 | Time 0.1s | Iter Loss 0.0544 | Iter Mean Loss 0.1464\n",
      "2024-11-14 04:43:29,109 - root - INFO - KG Training: Epoch 0001 Iter 2970 / 3136 | Time 0.1s | Iter Loss 0.0610 | Iter Mean Loss 0.1464\n",
      "2024-11-14 04:43:29,172 - root - INFO - KG Training: Epoch 0001 Iter 2971 / 3136 | Time 0.1s | Iter Loss 0.0589 | Iter Mean Loss 0.1464\n",
      "2024-11-14 04:43:29,232 - root - INFO - KG Training: Epoch 0001 Iter 2972 / 3136 | Time 0.1s | Iter Loss 0.0611 | Iter Mean Loss 0.1463\n",
      "2024-11-14 04:43:29,289 - root - INFO - KG Training: Epoch 0001 Iter 2973 / 3136 | Time 0.1s | Iter Loss 0.0506 | Iter Mean Loss 0.1463\n",
      "2024-11-14 04:43:29,353 - root - INFO - KG Training: Epoch 0001 Iter 2974 / 3136 | Time 0.1s | Iter Loss 0.0617 | Iter Mean Loss 0.1463\n",
      "2024-11-14 04:43:29,418 - root - INFO - KG Training: Epoch 0001 Iter 2975 / 3136 | Time 0.1s | Iter Loss 0.0513 | Iter Mean Loss 0.1462\n",
      "2024-11-14 04:43:29,478 - root - INFO - KG Training: Epoch 0001 Iter 2976 / 3136 | Time 0.1s | Iter Loss 0.0612 | Iter Mean Loss 0.1462\n",
      "2024-11-14 04:43:29,540 - root - INFO - KG Training: Epoch 0001 Iter 2977 / 3136 | Time 0.1s | Iter Loss 0.0616 | Iter Mean Loss 0.1462\n",
      "2024-11-14 04:43:29,602 - root - INFO - KG Training: Epoch 0001 Iter 2978 / 3136 | Time 0.1s | Iter Loss 0.0611 | Iter Mean Loss 0.1462\n",
      "2024-11-14 04:43:29,663 - root - INFO - KG Training: Epoch 0001 Iter 2979 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1461\n",
      "2024-11-14 04:43:29,724 - root - INFO - KG Training: Epoch 0001 Iter 2980 / 3136 | Time 0.1s | Iter Loss 0.0586 | Iter Mean Loss 0.1461\n",
      "2024-11-14 04:43:29,892 - root - INFO - KG Training: Epoch 0001 Iter 2981 / 3136 | Time 0.2s | Iter Loss 0.0570 | Iter Mean Loss 0.1461\n",
      "2024-11-14 04:43:29,953 - root - INFO - KG Training: Epoch 0001 Iter 2982 / 3136 | Time 0.1s | Iter Loss 0.0560 | Iter Mean Loss 0.1460\n",
      "2024-11-14 04:43:30,010 - root - INFO - KG Training: Epoch 0001 Iter 2983 / 3136 | Time 0.1s | Iter Loss 0.0576 | Iter Mean Loss 0.1460\n",
      "2024-11-14 04:43:30,070 - root - INFO - KG Training: Epoch 0001 Iter 2984 / 3136 | Time 0.1s | Iter Loss 0.0674 | Iter Mean Loss 0.1460\n",
      "2024-11-14 04:43:30,131 - root - INFO - KG Training: Epoch 0001 Iter 2985 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1460\n",
      "2024-11-14 04:43:30,194 - root - INFO - KG Training: Epoch 0001 Iter 2986 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1459\n",
      "2024-11-14 04:43:30,255 - root - INFO - KG Training: Epoch 0001 Iter 2987 / 3136 | Time 0.1s | Iter Loss 0.0541 | Iter Mean Loss 0.1459\n",
      "2024-11-14 04:43:30,314 - root - INFO - KG Training: Epoch 0001 Iter 2988 / 3136 | Time 0.1s | Iter Loss 0.0512 | Iter Mean Loss 0.1459\n",
      "2024-11-14 04:43:30,373 - root - INFO - KG Training: Epoch 0001 Iter 2989 / 3136 | Time 0.1s | Iter Loss 0.0624 | Iter Mean Loss 0.1458\n",
      "2024-11-14 04:43:30,434 - root - INFO - KG Training: Epoch 0001 Iter 2990 / 3136 | Time 0.1s | Iter Loss 0.0559 | Iter Mean Loss 0.1458\n",
      "2024-11-14 04:43:30,492 - root - INFO - KG Training: Epoch 0001 Iter 2991 / 3136 | Time 0.1s | Iter Loss 0.0570 | Iter Mean Loss 0.1458\n",
      "2024-11-14 04:43:30,555 - root - INFO - KG Training: Epoch 0001 Iter 2992 / 3136 | Time 0.1s | Iter Loss 0.0562 | Iter Mean Loss 0.1458\n",
      "2024-11-14 04:43:30,614 - root - INFO - KG Training: Epoch 0001 Iter 2993 / 3136 | Time 0.1s | Iter Loss 0.0607 | Iter Mean Loss 0.1457\n",
      "2024-11-14 04:43:30,677 - root - INFO - KG Training: Epoch 0001 Iter 2994 / 3136 | Time 0.1s | Iter Loss 0.0647 | Iter Mean Loss 0.1457\n",
      "2024-11-14 04:43:30,738 - root - INFO - KG Training: Epoch 0001 Iter 2995 / 3136 | Time 0.1s | Iter Loss 0.0615 | Iter Mean Loss 0.1457\n",
      "2024-11-14 04:43:30,861 - root - INFO - KG Training: Epoch 0001 Iter 2996 / 3136 | Time 0.1s | Iter Loss 0.0614 | Iter Mean Loss 0.1456\n",
      "2024-11-14 04:43:30,980 - root - INFO - KG Training: Epoch 0001 Iter 2997 / 3136 | Time 0.1s | Iter Loss 0.0567 | Iter Mean Loss 0.1456\n",
      "2024-11-14 04:43:31,045 - root - INFO - KG Training: Epoch 0001 Iter 2998 / 3136 | Time 0.1s | Iter Loss 0.0554 | Iter Mean Loss 0.1456\n",
      "2024-11-14 04:43:31,104 - root - INFO - KG Training: Epoch 0001 Iter 2999 / 3136 | Time 0.1s | Iter Loss 0.0531 | Iter Mean Loss 0.1455\n",
      "2024-11-14 04:43:31,161 - root - INFO - KG Training: Epoch 0001 Iter 3000 / 3136 | Time 0.1s | Iter Loss 0.0646 | Iter Mean Loss 0.1455\n",
      "2024-11-14 04:43:31,224 - root - INFO - KG Training: Epoch 0001 Iter 3001 / 3136 | Time 0.1s | Iter Loss 0.0715 | Iter Mean Loss 0.1455\n",
      "2024-11-14 04:43:31,285 - root - INFO - KG Training: Epoch 0001 Iter 3002 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1455\n",
      "2024-11-14 04:43:31,345 - root - INFO - KG Training: Epoch 0001 Iter 3003 / 3136 | Time 0.1s | Iter Loss 0.0680 | Iter Mean Loss 0.1454\n",
      "2024-11-14 04:43:31,404 - root - INFO - KG Training: Epoch 0001 Iter 3004 / 3136 | Time 0.1s | Iter Loss 0.0551 | Iter Mean Loss 0.1454\n",
      "2024-11-14 04:43:31,464 - root - INFO - KG Training: Epoch 0001 Iter 3005 / 3136 | Time 0.1s | Iter Loss 0.0601 | Iter Mean Loss 0.1454\n",
      "2024-11-14 04:43:31,524 - root - INFO - KG Training: Epoch 0001 Iter 3006 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1454\n",
      "2024-11-14 04:43:31,662 - root - INFO - KG Training: Epoch 0001 Iter 3007 / 3136 | Time 0.1s | Iter Loss 0.0702 | Iter Mean Loss 0.1453\n",
      "2024-11-14 04:43:31,720 - root - INFO - KG Training: Epoch 0001 Iter 3008 / 3136 | Time 0.1s | Iter Loss 0.0610 | Iter Mean Loss 0.1453\n",
      "2024-11-14 04:43:31,779 - root - INFO - KG Training: Epoch 0001 Iter 3009 / 3136 | Time 0.1s | Iter Loss 0.0596 | Iter Mean Loss 0.1453\n",
      "2024-11-14 04:43:31,839 - root - INFO - KG Training: Epoch 0001 Iter 3010 / 3136 | Time 0.1s | Iter Loss 0.0597 | Iter Mean Loss 0.1453\n",
      "2024-11-14 04:43:31,907 - root - INFO - KG Training: Epoch 0001 Iter 3011 / 3136 | Time 0.1s | Iter Loss 0.0648 | Iter Mean Loss 0.1452\n",
      "2024-11-14 04:43:31,965 - root - INFO - KG Training: Epoch 0001 Iter 3012 / 3136 | Time 0.1s | Iter Loss 0.0602 | Iter Mean Loss 0.1452\n",
      "2024-11-14 04:43:32,030 - root - INFO - KG Training: Epoch 0001 Iter 3013 / 3136 | Time 0.1s | Iter Loss 0.0711 | Iter Mean Loss 0.1452\n",
      "2024-11-14 04:43:32,092 - root - INFO - KG Training: Epoch 0001 Iter 3014 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1451\n",
      "2024-11-14 04:43:32,153 - root - INFO - KG Training: Epoch 0001 Iter 3015 / 3136 | Time 0.1s | Iter Loss 0.0621 | Iter Mean Loss 0.1451\n",
      "2024-11-14 04:43:32,213 - root - INFO - KG Training: Epoch 0001 Iter 3016 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1451\n",
      "2024-11-14 04:43:32,325 - root - INFO - KG Training: Epoch 0001 Iter 3017 / 3136 | Time 0.1s | Iter Loss 0.0603 | Iter Mean Loss 0.1451\n",
      "2024-11-14 04:43:32,386 - root - INFO - KG Training: Epoch 0001 Iter 3018 / 3136 | Time 0.1s | Iter Loss 0.0592 | Iter Mean Loss 0.1450\n",
      "2024-11-14 04:43:32,446 - root - INFO - KG Training: Epoch 0001 Iter 3019 / 3136 | Time 0.1s | Iter Loss 0.0483 | Iter Mean Loss 0.1450\n",
      "2024-11-14 04:43:32,505 - root - INFO - KG Training: Epoch 0001 Iter 3020 / 3136 | Time 0.1s | Iter Loss 0.0584 | Iter Mean Loss 0.1450\n",
      "2024-11-14 04:43:32,564 - root - INFO - KG Training: Epoch 0001 Iter 3021 / 3136 | Time 0.1s | Iter Loss 0.0640 | Iter Mean Loss 0.1449\n",
      "2024-11-14 04:43:32,627 - root - INFO - KG Training: Epoch 0001 Iter 3022 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1449\n",
      "2024-11-14 04:43:32,690 - root - INFO - KG Training: Epoch 0001 Iter 3023 / 3136 | Time 0.1s | Iter Loss 0.0510 | Iter Mean Loss 0.1449\n",
      "2024-11-14 04:43:32,751 - root - INFO - KG Training: Epoch 0001 Iter 3024 / 3136 | Time 0.1s | Iter Loss 0.0533 | Iter Mean Loss 0.1449\n",
      "2024-11-14 04:43:32,816 - root - INFO - KG Training: Epoch 0001 Iter 3025 / 3136 | Time 0.1s | Iter Loss 0.0517 | Iter Mean Loss 0.1448\n",
      "2024-11-14 04:43:32,876 - root - INFO - KG Training: Epoch 0001 Iter 3026 / 3136 | Time 0.1s | Iter Loss 0.0592 | Iter Mean Loss 0.1448\n",
      "2024-11-14 04:43:32,938 - root - INFO - KG Training: Epoch 0001 Iter 3027 / 3136 | Time 0.1s | Iter Loss 0.0551 | Iter Mean Loss 0.1448\n",
      "2024-11-14 04:43:33,174 - root - INFO - KG Training: Epoch 0001 Iter 3028 / 3136 | Time 0.2s | Iter Loss 0.0631 | Iter Mean Loss 0.1447\n",
      "2024-11-14 04:43:33,290 - root - INFO - KG Training: Epoch 0001 Iter 3029 / 3136 | Time 0.1s | Iter Loss 0.0531 | Iter Mean Loss 0.1447\n",
      "2024-11-14 04:43:33,352 - root - INFO - KG Training: Epoch 0001 Iter 3030 / 3136 | Time 0.1s | Iter Loss 0.0701 | Iter Mean Loss 0.1447\n",
      "2024-11-14 04:43:33,416 - root - INFO - KG Training: Epoch 0001 Iter 3031 / 3136 | Time 0.1s | Iter Loss 0.0608 | Iter Mean Loss 0.1447\n",
      "2024-11-14 04:43:33,480 - root - INFO - KG Training: Epoch 0001 Iter 3032 / 3136 | Time 0.1s | Iter Loss 0.0649 | Iter Mean Loss 0.1446\n",
      "2024-11-14 04:43:33,539 - root - INFO - KG Training: Epoch 0001 Iter 3033 / 3136 | Time 0.1s | Iter Loss 0.0498 | Iter Mean Loss 0.1446\n",
      "2024-11-14 04:43:33,607 - root - INFO - KG Training: Epoch 0001 Iter 3034 / 3136 | Time 0.1s | Iter Loss 0.0540 | Iter Mean Loss 0.1446\n",
      "2024-11-14 04:43:33,667 - root - INFO - KG Training: Epoch 0001 Iter 3035 / 3136 | Time 0.1s | Iter Loss 0.0561 | Iter Mean Loss 0.1445\n",
      "2024-11-14 04:43:33,775 - root - INFO - KG Training: Epoch 0001 Iter 3036 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1445\n",
      "2024-11-14 04:43:33,837 - root - INFO - KG Training: Epoch 0001 Iter 3037 / 3136 | Time 0.1s | Iter Loss 0.0697 | Iter Mean Loss 0.1445\n",
      "2024-11-14 04:43:33,898 - root - INFO - KG Training: Epoch 0001 Iter 3038 / 3136 | Time 0.1s | Iter Loss 0.0599 | Iter Mean Loss 0.1445\n",
      "2024-11-14 04:43:33,958 - root - INFO - KG Training: Epoch 0001 Iter 3039 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1444\n",
      "2024-11-14 04:43:34,017 - root - INFO - KG Training: Epoch 0001 Iter 3040 / 3136 | Time 0.1s | Iter Loss 0.0555 | Iter Mean Loss 0.1444\n",
      "2024-11-14 04:43:34,076 - root - INFO - KG Training: Epoch 0001 Iter 3041 / 3136 | Time 0.1s | Iter Loss 0.0515 | Iter Mean Loss 0.1444\n",
      "2024-11-14 04:43:34,134 - root - INFO - KG Training: Epoch 0001 Iter 3042 / 3136 | Time 0.1s | Iter Loss 0.0562 | Iter Mean Loss 0.1443\n",
      "2024-11-14 04:43:34,194 - root - INFO - KG Training: Epoch 0001 Iter 3043 / 3136 | Time 0.1s | Iter Loss 0.0566 | Iter Mean Loss 0.1443\n",
      "2024-11-14 04:43:34,256 - root - INFO - KG Training: Epoch 0001 Iter 3044 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1443\n",
      "2024-11-14 04:43:34,319 - root - INFO - KG Training: Epoch 0001 Iter 3045 / 3136 | Time 0.1s | Iter Loss 0.0588 | Iter Mean Loss 0.1443\n",
      "2024-11-14 04:43:34,381 - root - INFO - KG Training: Epoch 0001 Iter 3046 / 3136 | Time 0.1s | Iter Loss 0.0690 | Iter Mean Loss 0.1442\n",
      "2024-11-14 04:43:34,442 - root - INFO - KG Training: Epoch 0001 Iter 3047 / 3136 | Time 0.1s | Iter Loss 0.0582 | Iter Mean Loss 0.1442\n",
      "2024-11-14 04:43:34,502 - root - INFO - KG Training: Epoch 0001 Iter 3048 / 3136 | Time 0.1s | Iter Loss 0.0584 | Iter Mean Loss 0.1442\n",
      "2024-11-14 04:43:34,966 - root - INFO - KG Training: Epoch 0001 Iter 3049 / 3136 | Time 0.5s | Iter Loss 0.0620 | Iter Mean Loss 0.1442\n",
      "2024-11-14 04:43:35,025 - root - INFO - KG Training: Epoch 0001 Iter 3050 / 3136 | Time 0.1s | Iter Loss 0.0542 | Iter Mean Loss 0.1441\n",
      "2024-11-14 04:43:35,089 - root - INFO - KG Training: Epoch 0001 Iter 3051 / 3136 | Time 0.1s | Iter Loss 0.0577 | Iter Mean Loss 0.1441\n",
      "2024-11-14 04:43:35,154 - root - INFO - KG Training: Epoch 0001 Iter 3052 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1441\n",
      "2024-11-14 04:43:35,213 - root - INFO - KG Training: Epoch 0001 Iter 3053 / 3136 | Time 0.1s | Iter Loss 0.0666 | Iter Mean Loss 0.1440\n",
      "2024-11-14 04:43:35,279 - root - INFO - KG Training: Epoch 0001 Iter 3054 / 3136 | Time 0.1s | Iter Loss 0.0620 | Iter Mean Loss 0.1440\n",
      "2024-11-14 04:43:35,338 - root - INFO - KG Training: Epoch 0001 Iter 3055 / 3136 | Time 0.1s | Iter Loss 0.0546 | Iter Mean Loss 0.1440\n",
      "2024-11-14 04:43:35,400 - root - INFO - KG Training: Epoch 0001 Iter 3056 / 3136 | Time 0.1s | Iter Loss 0.0600 | Iter Mean Loss 0.1440\n",
      "2024-11-14 04:43:35,462 - root - INFO - KG Training: Epoch 0001 Iter 3057 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1439\n",
      "2024-11-14 04:43:35,524 - root - INFO - KG Training: Epoch 0001 Iter 3058 / 3136 | Time 0.1s | Iter Loss 0.0506 | Iter Mean Loss 0.1439\n",
      "2024-11-14 04:43:35,584 - root - INFO - KG Training: Epoch 0001 Iter 3059 / 3136 | Time 0.1s | Iter Loss 0.0672 | Iter Mean Loss 0.1439\n",
      "2024-11-14 04:43:35,648 - root - INFO - KG Training: Epoch 0001 Iter 3060 / 3136 | Time 0.1s | Iter Loss 0.0481 | Iter Mean Loss 0.1438\n",
      "2024-11-14 04:43:35,708 - root - INFO - KG Training: Epoch 0001 Iter 3061 / 3136 | Time 0.1s | Iter Loss 0.0512 | Iter Mean Loss 0.1438\n",
      "2024-11-14 04:43:35,771 - root - INFO - KG Training: Epoch 0001 Iter 3062 / 3136 | Time 0.1s | Iter Loss 0.0537 | Iter Mean Loss 0.1438\n",
      "2024-11-14 04:43:35,832 - root - INFO - KG Training: Epoch 0001 Iter 3063 / 3136 | Time 0.1s | Iter Loss 0.0599 | Iter Mean Loss 0.1438\n",
      "2024-11-14 04:43:35,894 - root - INFO - KG Training: Epoch 0001 Iter 3064 / 3136 | Time 0.1s | Iter Loss 0.0636 | Iter Mean Loss 0.1437\n",
      "2024-11-14 04:43:35,956 - root - INFO - KG Training: Epoch 0001 Iter 3065 / 3136 | Time 0.1s | Iter Loss 0.0627 | Iter Mean Loss 0.1437\n",
      "2024-11-14 04:43:36,017 - root - INFO - KG Training: Epoch 0001 Iter 3066 / 3136 | Time 0.1s | Iter Loss 0.0604 | Iter Mean Loss 0.1437\n",
      "2024-11-14 04:43:36,077 - root - INFO - KG Training: Epoch 0001 Iter 3067 / 3136 | Time 0.1s | Iter Loss 0.0517 | Iter Mean Loss 0.1437\n",
      "2024-11-14 04:43:36,140 - root - INFO - KG Training: Epoch 0001 Iter 3068 / 3136 | Time 0.1s | Iter Loss 0.0588 | Iter Mean Loss 0.1436\n",
      "2024-11-14 04:43:36,200 - root - INFO - KG Training: Epoch 0001 Iter 3069 / 3136 | Time 0.1s | Iter Loss 0.0469 | Iter Mean Loss 0.1436\n",
      "2024-11-14 04:43:36,257 - root - INFO - KG Training: Epoch 0001 Iter 3070 / 3136 | Time 0.1s | Iter Loss 0.0594 | Iter Mean Loss 0.1436\n",
      "2024-11-14 04:43:36,321 - root - INFO - KG Training: Epoch 0001 Iter 3071 / 3136 | Time 0.1s | Iter Loss 0.0630 | Iter Mean Loss 0.1435\n",
      "2024-11-14 04:43:36,382 - root - INFO - KG Training: Epoch 0001 Iter 3072 / 3136 | Time 0.1s | Iter Loss 0.0568 | Iter Mean Loss 0.1435\n",
      "2024-11-14 04:43:36,441 - root - INFO - KG Training: Epoch 0001 Iter 3073 / 3136 | Time 0.1s | Iter Loss 0.0619 | Iter Mean Loss 0.1435\n",
      "2024-11-14 04:43:36,503 - root - INFO - KG Training: Epoch 0001 Iter 3074 / 3136 | Time 0.1s | Iter Loss 0.0543 | Iter Mean Loss 0.1435\n",
      "2024-11-14 04:43:36,576 - root - INFO - KG Training: Epoch 0001 Iter 3075 / 3136 | Time 0.1s | Iter Loss 0.0533 | Iter Mean Loss 0.1434\n",
      "2024-11-14 04:43:36,634 - root - INFO - KG Training: Epoch 0001 Iter 3076 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1434\n",
      "2024-11-14 04:43:36,708 - root - INFO - KG Training: Epoch 0001 Iter 3077 / 3136 | Time 0.1s | Iter Loss 0.0598 | Iter Mean Loss 0.1434\n",
      "2024-11-14 04:43:36,766 - root - INFO - KG Training: Epoch 0001 Iter 3078 / 3136 | Time 0.1s | Iter Loss 0.0623 | Iter Mean Loss 0.1433\n",
      "2024-11-14 04:43:36,840 - root - INFO - KG Training: Epoch 0001 Iter 3079 / 3136 | Time 0.1s | Iter Loss 0.0651 | Iter Mean Loss 0.1433\n",
      "2024-11-14 04:43:36,898 - root - INFO - KG Training: Epoch 0001 Iter 3080 / 3136 | Time 0.1s | Iter Loss 0.0669 | Iter Mean Loss 0.1433\n",
      "2024-11-14 04:43:36,973 - root - INFO - KG Training: Epoch 0001 Iter 3081 / 3136 | Time 0.1s | Iter Loss 0.0555 | Iter Mean Loss 0.1433\n",
      "2024-11-14 04:43:37,050 - root - INFO - KG Training: Epoch 0001 Iter 3082 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1432\n",
      "2024-11-14 04:43:37,112 - root - INFO - KG Training: Epoch 0001 Iter 3083 / 3136 | Time 0.1s | Iter Loss 0.0675 | Iter Mean Loss 0.1432\n",
      "2024-11-14 04:43:37,173 - root - INFO - KG Training: Epoch 0001 Iter 3084 / 3136 | Time 0.1s | Iter Loss 0.0519 | Iter Mean Loss 0.1432\n",
      "2024-11-14 04:43:37,233 - root - INFO - KG Training: Epoch 0001 Iter 3085 / 3136 | Time 0.1s | Iter Loss 0.0640 | Iter Mean Loss 0.1432\n",
      "2024-11-14 04:43:37,294 - root - INFO - KG Training: Epoch 0001 Iter 3086 / 3136 | Time 0.1s | Iter Loss 0.0609 | Iter Mean Loss 0.1431\n",
      "2024-11-14 04:43:37,359 - root - INFO - KG Training: Epoch 0001 Iter 3087 / 3136 | Time 0.1s | Iter Loss 0.0499 | Iter Mean Loss 0.1431\n",
      "2024-11-14 04:43:37,421 - root - INFO - KG Training: Epoch 0001 Iter 3088 / 3136 | Time 0.1s | Iter Loss 0.0576 | Iter Mean Loss 0.1431\n",
      "2024-11-14 04:43:37,482 - root - INFO - KG Training: Epoch 0001 Iter 3089 / 3136 | Time 0.1s | Iter Loss 0.0522 | Iter Mean Loss 0.1430\n",
      "2024-11-14 04:43:37,544 - root - INFO - KG Training: Epoch 0001 Iter 3090 / 3136 | Time 0.1s | Iter Loss 0.0516 | Iter Mean Loss 0.1430\n",
      "2024-11-14 04:43:37,604 - root - INFO - KG Training: Epoch 0001 Iter 3091 / 3136 | Time 0.1s | Iter Loss 0.0503 | Iter Mean Loss 0.1430\n",
      "2024-11-14 04:43:37,668 - root - INFO - KG Training: Epoch 0001 Iter 3092 / 3136 | Time 0.1s | Iter Loss 0.0710 | Iter Mean Loss 0.1430\n",
      "2024-11-14 04:43:37,783 - root - INFO - KG Training: Epoch 0001 Iter 3093 / 3136 | Time 0.1s | Iter Loss 0.0607 | Iter Mean Loss 0.1429\n",
      "2024-11-14 04:43:37,845 - root - INFO - KG Training: Epoch 0001 Iter 3094 / 3136 | Time 0.1s | Iter Loss 0.0488 | Iter Mean Loss 0.1429\n",
      "2024-11-14 04:43:37,908 - root - INFO - KG Training: Epoch 0001 Iter 3095 / 3136 | Time 0.1s | Iter Loss 0.0650 | Iter Mean Loss 0.1429\n",
      "2024-11-14 04:43:37,967 - root - INFO - KG Training: Epoch 0001 Iter 3096 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1429\n",
      "2024-11-14 04:43:38,030 - root - INFO - KG Training: Epoch 0001 Iter 3097 / 3136 | Time 0.1s | Iter Loss 0.0582 | Iter Mean Loss 0.1428\n",
      "2024-11-14 04:43:38,092 - root - INFO - KG Training: Epoch 0001 Iter 3098 / 3136 | Time 0.1s | Iter Loss 0.0532 | Iter Mean Loss 0.1428\n",
      "2024-11-14 04:43:38,157 - root - INFO - KG Training: Epoch 0001 Iter 3099 / 3136 | Time 0.1s | Iter Loss 0.0670 | Iter Mean Loss 0.1428\n",
      "2024-11-14 04:43:38,215 - root - INFO - KG Training: Epoch 0001 Iter 3100 / 3136 | Time 0.1s | Iter Loss 0.0523 | Iter Mean Loss 0.1427\n",
      "2024-11-14 04:43:38,276 - root - INFO - KG Training: Epoch 0001 Iter 3101 / 3136 | Time 0.1s | Iter Loss 0.0541 | Iter Mean Loss 0.1427\n",
      "2024-11-14 04:43:38,340 - root - INFO - KG Training: Epoch 0001 Iter 3102 / 3136 | Time 0.1s | Iter Loss 0.0566 | Iter Mean Loss 0.1427\n",
      "2024-11-14 04:43:38,403 - root - INFO - KG Training: Epoch 0001 Iter 3103 / 3136 | Time 0.1s | Iter Loss 0.0652 | Iter Mean Loss 0.1427\n",
      "2024-11-14 04:43:38,463 - root - INFO - KG Training: Epoch 0001 Iter 3104 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1426\n",
      "2024-11-14 04:43:38,522 - root - INFO - KG Training: Epoch 0001 Iter 3105 / 3136 | Time 0.1s | Iter Loss 0.0525 | Iter Mean Loss 0.1426\n",
      "2024-11-14 04:43:38,586 - root - INFO - KG Training: Epoch 0001 Iter 3106 / 3136 | Time 0.1s | Iter Loss 0.0596 | Iter Mean Loss 0.1426\n",
      "2024-11-14 04:43:38,650 - root - INFO - KG Training: Epoch 0001 Iter 3107 / 3136 | Time 0.1s | Iter Loss 0.0633 | Iter Mean Loss 0.1426\n",
      "2024-11-14 04:43:38,713 - root - INFO - KG Training: Epoch 0001 Iter 3108 / 3136 | Time 0.1s | Iter Loss 0.0518 | Iter Mean Loss 0.1425\n",
      "2024-11-14 04:43:38,778 - root - INFO - KG Training: Epoch 0001 Iter 3109 / 3136 | Time 0.1s | Iter Loss 0.0621 | Iter Mean Loss 0.1425\n",
      "2024-11-14 04:43:38,838 - root - INFO - KG Training: Epoch 0001 Iter 3110 / 3136 | Time 0.1s | Iter Loss 0.0727 | Iter Mean Loss 0.1425\n",
      "2024-11-14 04:43:38,905 - root - INFO - KG Training: Epoch 0001 Iter 3111 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1425\n",
      "2024-11-14 04:43:38,971 - root - INFO - KG Training: Epoch 0001 Iter 3112 / 3136 | Time 0.1s | Iter Loss 0.0664 | Iter Mean Loss 0.1424\n",
      "2024-11-14 04:43:39,036 - root - INFO - KG Training: Epoch 0001 Iter 3113 / 3136 | Time 0.1s | Iter Loss 0.0657 | Iter Mean Loss 0.1424\n",
      "2024-11-14 04:43:39,100 - root - INFO - KG Training: Epoch 0001 Iter 3114 / 3136 | Time 0.1s | Iter Loss 0.0511 | Iter Mean Loss 0.1424\n",
      "2024-11-14 04:43:39,162 - root - INFO - KG Training: Epoch 0001 Iter 3115 / 3136 | Time 0.1s | Iter Loss 0.0514 | Iter Mean Loss 0.1423\n",
      "2024-11-14 04:43:39,224 - root - INFO - KG Training: Epoch 0001 Iter 3116 / 3136 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1423\n",
      "2024-11-14 04:43:39,287 - root - INFO - KG Training: Epoch 0001 Iter 3117 / 3136 | Time 0.1s | Iter Loss 0.0498 | Iter Mean Loss 0.1423\n",
      "2024-11-14 04:43:39,384 - root - INFO - KG Training: Epoch 0001 Iter 3118 / 3136 | Time 0.1s | Iter Loss 0.0621 | Iter Mean Loss 0.1423\n",
      "2024-11-14 04:43:39,449 - root - INFO - KG Training: Epoch 0001 Iter 3119 / 3136 | Time 0.1s | Iter Loss 0.0527 | Iter Mean Loss 0.1422\n",
      "2024-11-14 04:43:39,509 - root - INFO - KG Training: Epoch 0001 Iter 3120 / 3136 | Time 0.1s | Iter Loss 0.0585 | Iter Mean Loss 0.1422\n",
      "2024-11-14 04:43:39,569 - root - INFO - KG Training: Epoch 0001 Iter 3121 / 3136 | Time 0.1s | Iter Loss 0.0510 | Iter Mean Loss 0.1422\n",
      "2024-11-14 04:43:39,631 - root - INFO - KG Training: Epoch 0001 Iter 3122 / 3136 | Time 0.1s | Iter Loss 0.0639 | Iter Mean Loss 0.1422\n",
      "2024-11-14 04:43:39,742 - root - INFO - KG Training: Epoch 0001 Iter 3123 / 3136 | Time 0.1s | Iter Loss 0.0545 | Iter Mean Loss 0.1421\n",
      "2024-11-14 04:43:39,804 - root - INFO - KG Training: Epoch 0001 Iter 3124 / 3136 | Time 0.1s | Iter Loss 0.0528 | Iter Mean Loss 0.1421\n",
      "2024-11-14 04:43:39,865 - root - INFO - KG Training: Epoch 0001 Iter 3125 / 3136 | Time 0.1s | Iter Loss 0.0565 | Iter Mean Loss 0.1421\n",
      "2024-11-14 04:43:39,932 - root - INFO - KG Training: Epoch 0001 Iter 3126 / 3136 | Time 0.1s | Iter Loss 0.0568 | Iter Mean Loss 0.1420\n",
      "2024-11-14 04:43:39,994 - root - INFO - KG Training: Epoch 0001 Iter 3127 / 3136 | Time 0.1s | Iter Loss 0.0564 | Iter Mean Loss 0.1420\n",
      "2024-11-14 04:43:40,055 - root - INFO - KG Training: Epoch 0001 Iter 3128 / 3136 | Time 0.1s | Iter Loss 0.0520 | Iter Mean Loss 0.1420\n",
      "2024-11-14 04:43:40,115 - root - INFO - KG Training: Epoch 0001 Iter 3129 / 3136 | Time 0.1s | Iter Loss 0.0581 | Iter Mean Loss 0.1420\n",
      "2024-11-14 04:43:40,175 - root - INFO - KG Training: Epoch 0001 Iter 3130 / 3136 | Time 0.1s | Iter Loss 0.0656 | Iter Mean Loss 0.1419\n",
      "2024-11-14 04:43:40,232 - root - INFO - KG Training: Epoch 0001 Iter 3131 / 3136 | Time 0.1s | Iter Loss 0.0611 | Iter Mean Loss 0.1419\n",
      "2024-11-14 04:43:40,291 - root - INFO - KG Training: Epoch 0001 Iter 3132 / 3136 | Time 0.1s | Iter Loss 0.0559 | Iter Mean Loss 0.1419\n",
      "2024-11-14 04:43:40,353 - root - INFO - KG Training: Epoch 0001 Iter 3133 / 3136 | Time 0.1s | Iter Loss 0.0605 | Iter Mean Loss 0.1419\n",
      "2024-11-14 04:43:40,415 - root - INFO - KG Training: Epoch 0001 Iter 3134 / 3136 | Time 0.1s | Iter Loss 0.0632 | Iter Mean Loss 0.1418\n",
      "2024-11-14 04:43:40,477 - root - INFO - KG Training: Epoch 0001 Iter 3135 / 3136 | Time 0.1s | Iter Loss 0.0645 | Iter Mean Loss 0.1418\n",
      "2024-11-14 04:43:40,539 - root - INFO - KG Training: Epoch 0001 Iter 3136 / 3136 | Time 0.1s | Iter Loss 0.0638 | Iter Mean Loss 0.1418\n",
      "2024-11-14 04:43:40,539 - root - INFO - KG Training: Epoch 0001 Total Iter 3136 | Total Time 215.6s | Iter Mean Loss 0.1418\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 35.81 MiB is free. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Process 305314 has 104.00 MiB memory in use. Of the allocated memory 7.40 GiB is allocated by PyTorch, and 244.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 234\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     args \u001b[38;5;241m=\u001b[39m parse_kgat_args()\n\u001b[0;32m--> 234\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# predict(args)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[183], line 165\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    163\u001b[0m r_list \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mr_list\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    164\u001b[0m relations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mlaplacian_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 165\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate_att\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate Attention: Epoch \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m | Total Time \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, time() \u001b[38;5;241m-\u001b[39m time5))\n\u001b[1;32m    168\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCF + KG Training: Epoch \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m | Total Time \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, time() \u001b[38;5;241m-\u001b[39m time0))\n",
      "File \u001b[0;32m~/anaconda3/envs/GT5/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GT5/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[161], line 250\u001b[0m, in \u001b[0;36mKGAT.forward\u001b[0;34m(self, mode, *input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_kg_loss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate_att\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_score(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "Cell \u001b[0;32mIn[161], line 212\u001b[0m, in \u001b[0;36mKGAT.update_attention\u001b[0;34m(self, h_list, t_list, r_list, relations)\u001b[0m\n\u001b[1;32m    209\u001b[0m batch_h_list \u001b[38;5;241m=\u001b[39m h_list[index_list]\n\u001b[1;32m    210\u001b[0m batch_t_list \u001b[38;5;241m=\u001b[39m t_list[index_list]\n\u001b[0;32m--> 212\u001b[0m batch_v_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_attention_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_h_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_t_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m rows\u001b[38;5;241m.\u001b[39mappend(batch_h_list)\n\u001b[1;32m    214\u001b[0m cols\u001b[38;5;241m.\u001b[39mappend(batch_t_list)\n",
      "Cell \u001b[0;32mIn[161], line 196\u001b[0m, in \u001b[0;36mKGAT.update_attention_batch\u001b[0;34m(self, h_list, t_list, r_idx)\u001b[0m\n\u001b[1;32m    194\u001b[0m r_mul_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(h_embed, W_r)\n\u001b[1;32m    195\u001b[0m r_mul_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(t_embed, W_r)\n\u001b[0;32m--> 196\u001b[0m v_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(r_mul_t \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mr_mul_h\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr_embed\u001b[49m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v_list\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 35.81 MiB is free. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Process 305314 has 104.00 MiB memory in use. Of the allocated memory 7.40 GiB is allocated by PyTorch, and 244.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    log_save_id = create_log_id(args.save_dir)\n",
    "    logging_config(folder=args.save_dir, name='log{:d}'.format(log_save_id), no_console=False)\n",
    "    logging.info(args)\n",
    "\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "    if args.use_pretrain == 1:\n",
    "        user_pre_embed = torch.tensor(data.user_pre_embed)\n",
    "        item_pre_embed = torch.tensor(data.item_pre_embed)\n",
    "    else:\n",
    "        user_pre_embed, item_pre_embed = None, None\n",
    "\n",
    "    # construct model & optimizer\n",
    "    # model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    # model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    if args.use_pretrain == 2:\n",
    "        model = load_model(model, args.pretrain_model_path)\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(model)\n",
    "\n",
    "    cf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kg_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # initialize metrics\n",
    "    best_epoch = -1\n",
    "    best_recall = 0\n",
    "\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    epoch_list = []\n",
    "    metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        time0 = time()\n",
    "        model.train()\n",
    "\n",
    "        # train cf\n",
    "        time1 = time()\n",
    "        cf_total_loss = 0\n",
    "        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_cf_batch + 1):\n",
    "            time2 = time()\n",
    "            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n",
    "            cf_batch_user = cf_batch_user.to(device)\n",
    "            cf_batch_pos_item = cf_batch_pos_item.to(device)\n",
    "            cf_batch_neg_item = cf_batch_neg_item.to(device)\n",
    "\n",
    "            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n",
    "\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "\n",
    "            if (iter % args.cf_print_every) == 0:\n",
    "                logging.info('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n",
    "        logging.info('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n",
    "\n",
    "        # train kg\n",
    "        time3 = time()\n",
    "        kg_total_loss = 0\n",
    "        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_kg_batch + 1):\n",
    "            time4 = time()\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n",
    "            kg_batch_head = kg_batch_head.to(device)\n",
    "            kg_batch_relation = kg_batch_relation.to(device)\n",
    "            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n",
    "            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n",
    "\n",
    "            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n",
    "\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "\n",
    "            if (iter % args.kg_print_every) == 0:\n",
    "                logging.info('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n",
    "        logging.info('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n",
    "\n",
    "        # update attention\n",
    "        time5 = time()\n",
    "        h_list = data.h_list.to(device)\n",
    "        t_list = data.t_list.to(device)\n",
    "        r_list = data.r_list.to(device)\n",
    "        relations = list(data.laplacian_dict.keys())\n",
    "        model(h_list, t_list, r_list, relations, mode='update_att')\n",
    "        logging.info('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n",
    "\n",
    "        logging.info('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n",
    "\n",
    "        # evaluate cf\n",
    "        if (epoch % args.evaluate_every) == 0 or epoch == args.n_epoch:\n",
    "            time6 = time()\n",
    "            _, metrics_dict = evaluate(model, data, Ks, device)\n",
    "            logging.info('CF Evaluation: Epoch {:04d} | Total Time {:.1f}s | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "                epoch, time() - time6, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            for k in Ks:\n",
    "                for m in ['precision', 'recall', 'ndcg']:\n",
    "                    metrics_list[k][m].append(metrics_dict[k][m])\n",
    "            best_recall, should_stop = early_stopping(metrics_list[k_min]['recall'], args.stopping_steps)\n",
    "\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            if metrics_list[k_min]['recall'].index(best_recall) == len(epoch_list) - 1:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                logging.info('Save model on epoch {:04d}!'.format(epoch))\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # save metrics\n",
    "    metrics_df = [epoch_list]\n",
    "    metrics_cols = ['epoch_idx']\n",
    "    for k in Ks:\n",
    "        for m in ['precision', 'recall', 'ndcg']:\n",
    "            metrics_df.append(metrics_list[k][m])\n",
    "            metrics_cols.append('{}@{}'.format(m, k))\n",
    "    metrics_df = pd.DataFrame(metrics_df).transpose()\n",
    "    metrics_df.columns = metrics_cols\n",
    "    metrics_df.to_csv(args.save_dir + '/metrics.tsv', sep='\\t', index=False)\n",
    "\n",
    "    # print best metrics\n",
    "    best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n",
    "    logging.info('Best CF Evaluation: Epoch {:04d} | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        int(best_metrics['epoch_idx']), best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n",
    "\n",
    "\n",
    "def predict(args):\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "    # load model\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    model = load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # predict\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    cf_scores, metrics_dict = evaluate(model, data, Ks, device)\n",
    "    np.save(args.save_dir + 'cf_scores.npy', cf_scores)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_kgat_args()\n",
    "    train(args)\n",
    "    # predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7333650",
   "metadata": {},
   "source": [
    "# 3. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66dd7d",
   "metadata": {},
   "source": [
    "## 3.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045ee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585aaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.4-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from kagglehub) (4.67.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Downloading kagglehub-0.3.4-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8b005",
   "metadata": {},
   "source": [
    "## 3.2. Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dbbfc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/GT5/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ngannkim/bxcsvdump?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24.6M/24.6M [00:02<00:00, 9.66MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/ngannkim/bxcsvdump/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ngannkim/bxcsvdump\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "743485bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the dataset directory: ['BX-Users.csv', 'BX-Book-Ratings.csv', 'BX-Books.csv']\n",
      "First 5 lines of BX-Books.csv:\n",
      "\"ISBN\";\"Book-Title\";\"Book-Author\";\"Year-Of-Publication\";\"Publisher\";\"Image-URL-S\";\"Image-URL-M\";\"Image-URL-L\"\n",
      "\"0195153448\";\"Classical Mythology\";\"Mark P. O. Morford\";\"2002\";\"Oxford University Press\";\"http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg\"\n",
      "\"0002005018\";\"Clara Callan\";\"Richard Bruce Wright\";\"2001\";\"HarperFlamingo Canada\";\"http://images.amazon.com/images/P/0002005018.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0002005018.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0002005018.01.LZZZZZZZ.jpg\"\n",
      "\"0060973129\";\"Decision in Normandy\";\"Carlo D'Este\";\"1991\";\"HarperPerennial\";\"http://images.amazon.com/images/P/0060973129.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0060973129.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0060973129.01.LZZZZZZZ.jpg\"\n",
      "\"0374157065\";\"Flu: The Story of the Great Influenza Pandemic of 1918 and the Search for the Virus That Caused It\";\"Gina Bari Kolata\";\"1999\";\"Farrar Straus Giroux\";\"http://images.amazon.com/images/P/0374157065.01.THUMBZZZ.jpg\";\"http://images.amazon.com/images/P/0374157065.01.MZZZZZZZ.jpg\";\"http://images.amazon.com/images/P/0374157065.01.LZZZZZZZ.jpg\"\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Path to the extracted dataset\n",
    "dataset_path = '/home/ubuntu/.cache/kagglehub/datasets/ngannkim/bxcsvdump/versions/1'\n",
    "\n",
    "# Check if the directory exists and list the files\n",
    "if os.path.exists(dataset_path):\n",
    "    # List files in the dataset path\n",
    "    files = os.listdir(dataset_path)\n",
    "    print(\"Files in the dataset directory:\", files)\n",
    "\n",
    "    # Open a specific file if needed, for example 'BX-Books.csv'\n",
    "    books_file = os.path.join(dataset_path, 'BX-Books.csv')\n",
    "    with open(books_file, 'r', encoding='latin-1') as f:\n",
    "        # Process the file (e.g., read data, print the first few lines)\n",
    "        print(\"First 5 lines of BX-Books.csv:\")\n",
    "        for i in range(5):\n",
    "            print(f.readline().strip())\n",
    "else:\n",
    "    print(\"Dataset directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c690162",
   "metadata": {},
   "source": [
    "## 3.3. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1532e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93580/128378676.py:15: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(books_file, sep=';', encoding='latin-1', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path to your dataset\n",
    "base_path = '/home/ubuntu/.cache/kagglehub/datasets/ngannkim/bxcsvdump/versions/1'\n",
    "\n",
    "# Define file paths\n",
    "ratings_file = os.path.join(base_path, 'BX-Book-Ratings.csv')\n",
    "users_file = os.path.join(base_path, 'BX-Users.csv')\n",
    "books_file = os.path.join(base_path, 'BX-Books.csv')\n",
    "\n",
    "# Load the datasets with appropriate options\n",
    "ratings = pd.read_csv(ratings_file, sep=';', encoding='latin-1')\n",
    "users = pd.read_csv(users_file, sep=';', encoding='latin-1')\n",
    "books = pd.read_csv(books_file, sep=';', encoding='latin-1', on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cc19176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating\n",
       "0   276725  034545104X            0\n",
       "1   276726  0155061224            5\n",
       "2   276727  0446520802            0\n",
       "3   276729  052165615X            3\n",
       "4   276729  0521795028            6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba4d5040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d5c175a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN                                         Book-Title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0    Mark P. O. Morford                2002     Oxford University Press   \n",
       "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2          Carlo D'Este                1991             HarperPerennial   \n",
       "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "\n",
       "                                         Image-URL-S  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-M  \\\n",
       "0  http://images.amazon.com/images/P/0195153448.0...   \n",
       "1  http://images.amazon.com/images/P/0002005018.0...   \n",
       "2  http://images.amazon.com/images/P/0060973129.0...   \n",
       "3  http://images.amazon.com/images/P/0374157065.0...   \n",
       "4  http://images.amazon.com/images/P/0393045218.0...   \n",
       "\n",
       "                                         Image-URL-L  \n",
       "0  http://images.amazon.com/images/P/0195153448.0...  \n",
       "1  http://images.amazon.com/images/P/0002005018.0...  \n",
       "2  http://images.amazon.com/images/P/0060973129.0...  \n",
       "3  http://images.amazon.com/images/P/0374157065.0...  \n",
       "4  http://images.amazon.com/images/P/0393045218.0...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "297c3c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Book-Rating', ylabel='count'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "sns.countplot(x=ratings['Book-Rating']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3370ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105283\n",
      "340556\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings['User-ID'].unique())) \n",
    "print(len(ratings['ISBN'].unique())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "239ac07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isbn_counts = ratings.groupby('ISBN').size()\n",
    "count_occurrences = isbn_counts.value_counts()\n",
    "count_occurrences[:15].plot(kind='bar')\n",
    "plt.xlabel(\"Number of occurrences of an ISBN number\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "784f5c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "userid_counts = ratings.groupby('User-ID').size() \n",
    "count_occurrences = userid_counts.value_counts() \n",
    "count_occurrences[:15].plot(kind='bar')\n",
    "plt.xlabel(\"Number of occurrences of a User-ID\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519d575",
   "metadata": {},
   "source": [
    "## 3.4. Preprocessing the Book-Crossing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcd406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn import LGConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2a2a3",
   "metadata": {},
   "source": [
    "### 3.4.1. Use 100k highest ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06b2fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "ratings = ratings.loc[ratings['ISBN'].isin(books['ISBN'].unique()) & ratings['User-ID'].isin(users['User-ID'].unique())]\n",
    "\n",
    "# Keep the 100k highest ratings\n",
    "ratings = ratings[ratings['Book-Rating'] >= 8].iloc[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8f5c7",
   "metadata": {},
   "source": [
    "### 3.4.2. Create mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "474ba939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "user_mapping = {userid: i for i, userid in enumerate(ratings['User-ID'].unique())}\n",
    "item_mapping = {isbn: i for i, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "\n",
    "# Count users and items\n",
    "num_users = len(user_mapping)\n",
    "num_items = len(item_mapping)\n",
    "num_total = num_users + num_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de513e0",
   "metadata": {},
   "source": [
    "### 3.4.3. Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2afc2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the adjacency matrix based on user ratings\n",
    "user_ids = torch.LongTensor([user_mapping[i] for i in ratings['User-ID']])\n",
    "item_ids = torch.LongTensor([item_mapping[i] for i in ratings['ISBN']])\n",
    "edge_index = torch.stack((user_ids, item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4d2a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation, and test adjacency matrices\n",
    "train_index, test_index = train_test_split(range(len(ratings)), test_size=0.2, random_state=0)\n",
    "val_index, test_index = train_test_split(test_index, test_size=0.5, random_state=0)\n",
    "\n",
    "train_edge_index = edge_index[:, train_index]\n",
    "val_edge_index = edge_index[:, val_index]\n",
    "test_edge_index = edge_index[:, test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ae4f6",
   "metadata": {},
   "source": [
    "## 3.5. Mini batch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3dc819e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mini_batch(edge_index):\n",
    "    # Generate BATCH_SIZE random indices\n",
    "    index = np.random.choice(range(edge_index.shape[1]), size=BATCH_SIZE)\n",
    "\n",
    "    # Generate negative sample indices\n",
    "    edge_index = structured_negative_sampling(edge_index)\n",
    "    edge_index = torch.stack(edge_index, dim=0)\n",
    "    \n",
    "    user_index = edge_index[0, index]\n",
    "    pos_item_index = edge_index[1, index]\n",
    "    neg_item_index = edge_index[2, index]\n",
    "    \n",
    "    return user_index, pos_item_index, neg_item_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298bdad",
   "metadata": {},
   "source": [
    "## 3.6. Implement LightGCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a47d3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_layers=4, dim_h=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_users = nn.Embedding(num_embeddings=self.num_users, embedding_dim=dim_h)\n",
    "        self.emb_items = nn.Embedding(num_embeddings=self.num_items, embedding_dim=dim_h)\n",
    "\n",
    "        self.convs = nn.ModuleList(LGConv() for _ in range(num_layers))\n",
    "\n",
    "        nn.init.normal_(self.emb_users.weight, std=0.01)\n",
    "        nn.init.normal_(self.emb_items.weight, std=0.01)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        emb = torch.cat([self.emb_users.weight, self.emb_items.weight])\n",
    "        embs = [emb]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            emb = conv(x=emb, edge_index=edge_index)\n",
    "            embs.append(emb)\n",
    "\n",
    "        emb_final = 1/(self.num_layers+1) * torch.mean(torch.stack(embs, dim=1), dim=1)\n",
    "\n",
    "        emb_users_final, emb_items_final = torch.split(emb_final, [self.num_users, self.num_items])\n",
    "\n",
    "        return emb_users_final, self.emb_users.weight, emb_items_final, self.emb_items.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c8a20",
   "metadata": {},
   "source": [
    "## 3.7. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "955934fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(emb_users_final, emb_users, emb_pos_items_final, emb_pos_items, emb_neg_items_final, emb_neg_items):\n",
    "    reg_loss = LAMBDA * (emb_users.norm().pow(2) +\n",
    "                        emb_pos_items.norm().pow(2) +\n",
    "                        emb_neg_items.norm().pow(2))\n",
    "\n",
    "    pos_ratings = torch.mul(emb_users_final, emb_pos_items_final).sum(dim=-1)\n",
    "    neg_ratings = torch.mul(emb_users_final, emb_neg_items_final).sum(dim=-1)\n",
    "\n",
    "    bpr_loss = torch.mean(torch.nn.functional.softplus(pos_ratings - neg_ratings))\n",
    "    # bpr_loss = torch.mean(torch.nn.functional.logsigmoid(pos_ratings - neg_ratings))\n",
    "\n",
    "    return -bpr_loss + reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a2b6e",
   "metadata": {},
   "source": [
    "## 3.8. Store user-item interaction in graph-based structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "791c1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_items(edge_index):\n",
    "    user_items = dict()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_items:\n",
    "            user_items[user] = []\n",
    "        user_items[user].append(item)\n",
    "    return user_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70384cf",
   "metadata": {},
   "source": [
    "## 3.9. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41689eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(items_ground_truth, items_predicted):\n",
    "    num_correct_pred = np.sum(items_predicted, axis=1)\n",
    "    num_total_pred = np.array([len(items_ground_truth[i]) for i in range(len(items_ground_truth))])\n",
    "\n",
    "    recall = np.mean(num_correct_pred / num_total_pred)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3392c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(items_ground_truth, items_predicted):\n",
    "    test_matrix = np.zeros((len(items_predicted), K))\n",
    "\n",
    "    for i, items in enumerate(items_ground_truth):\n",
    "        length = min(len(items), K)\n",
    "        test_matrix[i, :length] = 1\n",
    "    \n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1. / np.log2(np.arange(2, K + 2)), axis=1)\n",
    "    dcg = items_predicted * (1. / np.log2(np.arange(2, K + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg / idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    \n",
    "    return np.mean(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "978175dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to get evaluation metrics\n",
    "def get_metrics(model, edge_index, exclude_edge_indices):\n",
    "\n",
    "    ratings = torch.matmul(model.emb_users.weight, model.emb_items.weight.T)\n",
    "\n",
    "    for exclude_edge_index in exclude_edge_indices:\n",
    "        user_pos_items = get_user_items(exclude_edge_index)\n",
    "        exclude_users = []\n",
    "        exclude_items = []\n",
    "        for user, items in user_pos_items.items():\n",
    "            exclude_users.extend([user] * len(items))\n",
    "            exclude_items.extend(items)\n",
    "        ratings[exclude_users, exclude_items] = -1024\n",
    "\n",
    "    # get the top k recommended items for each user\n",
    "    _, top_K_items = torch.topk(ratings, k=K)\n",
    "\n",
    "    # get all unique users in evaluated split\n",
    "    users = edge_index[0].unique()\n",
    "\n",
    "    test_user_pos_items = get_user_items(edge_index)\n",
    "\n",
    "    # convert test user pos items dictionary into a list\n",
    "    test_user_pos_items_list = [test_user_pos_items[user.item()] for user in users]\n",
    "\n",
    "    # determine the correctness of topk predictions\n",
    "    items_predicted = []\n",
    "    for user in users:\n",
    "        ground_truth_items = test_user_pos_items[user.item()]\n",
    "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
    "        items_predicted.append(label)\n",
    "\n",
    "    recall = compute_recall_at_k(test_user_pos_items_list, items_predicted)\n",
    "    ndcg = compute_ndcg_at_k(test_user_pos_items_list, items_predicted)\n",
    "\n",
    "    return recall, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f35948",
   "metadata": {},
   "source": [
    "## 3.10. Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b70aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to evaluate model\n",
    "def test(model, edge_index, exclude_edge_indices):\n",
    "    emb_users_final, emb_users, emb_items_final, emb_items = model.forward(edge_index)\n",
    "    user_indices, pos_item_indices, neg_item_indices = structured_negative_sampling(edge_index, contains_neg_self_loops=False)\n",
    "\n",
    "    emb_users_final, emb_users = emb_users_final[user_indices], emb_users[user_indices]\n",
    "\n",
    "    emb_pos_items_final, emb_pos_items = emb_items_final[pos_item_indices], emb_items[pos_item_indices]\n",
    "    emb_neg_items_final, emb_neg_items = emb_items_final[neg_item_indices], emb_items[neg_item_indices]\n",
    "\n",
    "    loss = bpr_loss(emb_users_final, emb_users, emb_pos_items_final, emb_pos_items, emb_neg_items_final, emb_neg_items).item()\n",
    "\n",
    "    recall, ndcg = get_metrics(model, edge_index, exclude_edge_indices)\n",
    "\n",
    "    return loss, recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9af3d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "LAMBDA = 1e-6\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6ca5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LightGCN(num_users, num_items)\n",
    "model = model.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "train_edge_index = train_edge_index.to(device)\n",
    "val_edge_index = val_edge_index.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8018fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss: -0.69342 | Val loss: -0.69256 | Val recall@20: 0.01110 | Val ndcg@20: 0.00607\n",
      "Epoch 5 | Train loss: -0.73106 | Val loss: -0.63753 | Val recall@20: 0.01841 | Val ndcg@20: 0.00861\n",
      "Epoch 10 | Train loss: -0.91499 | Val loss: -0.42976 | Val recall@20: 0.01815 | Val ndcg@20: 0.00871\n",
      "Epoch 15 | Train loss: -1.34274 | Val loss: -0.03908 | Val recall@20: 0.01786 | Val ndcg@20: 0.00867\n",
      "Epoch 20 | Train loss: -2.16877 | Val loss: 0.49820 | Val recall@20: 0.01783 | Val ndcg@20: 0.00866\n",
      "Epoch 25 | Train loss: -2.99264 | Val loss: 1.15565 | Val recall@20: 0.01794 | Val ndcg@20: 0.00867\n",
      "Epoch 30 | Train loss: -4.16030 | Val loss: 1.92545 | Val recall@20: 0.01818 | Val ndcg@20: 0.00876\n"
     ]
    }
   ],
   "source": [
    "n_batch = int(len(train_index)/BATCH_SIZE)\n",
    "\n",
    "for epoch in range(31):\n",
    "    model.train()\n",
    "\n",
    "    for _ in range(n_batch):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb_users_final, emb_users, emb_items_final, emb_items = model.forward(train_edge_index)\n",
    "\n",
    "        user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(train_edge_index)\n",
    "        \n",
    "        emb_users_final, emb_users = emb_users_final[user_indices], emb_users[user_indices]\n",
    "        emb_pos_items_final, emb_pos_items = emb_items_final[pos_item_indices], emb_items[pos_item_indices]\n",
    "        emb_neg_items_final, emb_neg_items = emb_items_final[neg_item_indices], emb_items[neg_item_indices]\n",
    "\n",
    "        train_loss = bpr_loss(emb_users_final, emb_users, emb_pos_items_final, emb_pos_items, emb_neg_items_final, emb_neg_items)\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss, recall, ndcg = test(model, val_edge_index, [train_edge_index])\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss.item():.5f} | Val loss: {val_loss:.5f} | Val recall@{K}: {recall:.5f} | Val ndcg@{K}: {ndcg:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03aa38f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.89800 | Test recall@20: 0.01725 | Test ndcg@20: 0.00890\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_recall, test_ndcg = test(model, test_edge_index.to(device), [train_edge_index, val_edge_index])\n",
    "\n",
    "print(f\"Test loss: {test_loss:.5f} | Test recall@{K}: {test_recall:.5f} | Test ndcg@{K}: {test_ndcg:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb19b30",
   "metadata": {},
   "source": [
    "## 3.11. Recommend books for a particular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0840f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookid_title = pd.Series(books['Book-Title'].values, index=books.ISBN).to_dict()\n",
    "bookid_author = pd.Series(books['Book-Author'].values, index=books.ISBN).to_dict()\n",
    "user_pos_items = get_user_items(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9929b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def recommend(user_id, num_recs):\n",
    "    user = user_mapping[user_id]\n",
    "    emb_user = model.emb_users.weight[user]\n",
    "    ratings = model.emb_items.weight @ emb_user\n",
    "\n",
    "    values, indices = torch.topk(ratings, k=100)\n",
    "\n",
    "    # Fetching favorite books\n",
    "    ids = [index.cpu().item() for index in indices if index in user_pos_items[user]][:num_recs]\n",
    "    item_isbns = [list(item_mapping.keys())[list(item_mapping.values()).index(book)] for book in ids]\n",
    "    titles = [bookid_title[id] for id in item_isbns]\n",
    "    authors = [bookid_author[id] for id in item_isbns]\n",
    "\n",
    "    print(f'Favorite books from user n°{user_id}:')\n",
    "    for i in range(len(item_isbns)):\n",
    "        print(f'- {titles[i]}, by {authors[i]}')\n",
    "\n",
    "    # Fetching recommended books\n",
    "    ids = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:num_recs]\n",
    "    item_isbns = [list(item_mapping.keys())[list(item_mapping.values()).index(book)] for book in ids]\n",
    "    titles = [bookid_title[id] for id in item_isbns]\n",
    "    authors = [bookid_author[id] for id in item_isbns]\n",
    "\n",
    "    print(f'\\nRecommended books for user n°{user_id}')\n",
    "    for i in range(num_recs):\n",
    "        print(f'- {titles[i]}, by {authors[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a157fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Favorite books from user n°277427:\n",
      "- The Da Vinci Code, by Dan Brown\n",
      "- Into the Wild, by Jon Krakauer\n",
      "- One for the Money (Stephanie Plum Novels (Paperback)), by Janet Evanovich\n",
      "\n",
      "Recommended books for user n°277427\n",
      "- The Lovely Bones: A Novel, by Alice Sebold\n",
      "- The Red Tent (Bestselling Backlist), by Anita Diamant\n",
      "- Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback)), by J. K. Rowling\n",
      "- To Kill a Mockingbird, by Harper Lee\n",
      "- Angels &amp; Demons, by Dan Brown\n"
     ]
    }
   ],
   "source": [
    "recommend(277427, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GT5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
