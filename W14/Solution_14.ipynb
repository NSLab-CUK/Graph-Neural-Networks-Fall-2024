{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3dcbfb-8bae-48de-94b2-288f1c0b132d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Assignment 1. Do regression task on MoleculeNet/ESOL with GCN, GraphSAGE, GIN, GAT, GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf308bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES String: Cc1occc1C(=O)Nc2ccccc2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/GNN2/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "import matplotlib.pyplot as plt\n",
    "import py3Dmol\n",
    "\n",
    "\n",
    "# Load MoleculeNet ESOL dataset\n",
    "dataset = MoleculeNet(root=\"data/MoleculeNet\", name=\"ESOL\")\n",
    "\n",
    "# Extract the SMILES strings from the dataset (Tox21 contains SMILES in `smiles` attribute)\n",
    "smiles_list = dataset.data.smiles  # SMILES strings are typically in this field\n",
    "\n",
    "# Process the first molecule\n",
    "example_index = 1\n",
    "smiles = smiles_list[example_index]\n",
    "print(f\"SMILES String: {smiles}\")\n",
    "\n",
    "# Convert SMILES to RDKit molecule\n",
    "mol = Chem.MolFromSmiles(smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfdcfdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "\n",
    "dataset = MoleculeNet(root='/tmp/MoleculeNet', name='ESOL')\n",
    "\n",
    "train_dataset = dataset[:int(len(dataset) * 0.8)]\n",
    "val_dataset = dataset[int(len(dataset) * 0.8):int(len(dataset) * 0.9)]\n",
    "test_dataset = dataset[int(len(dataset) * 0.9):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9779e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GINConv, GATConv, global_mean_pool, TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GraphSAGE\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GIN \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GIN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GINConv(nn=nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINConv(nn=nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels, hidden_channels)\n",
    "            )))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GAT \n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GT (Graph Transformer) \n",
    "class GT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GT, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Use TransformerConv for GT\n",
    "        self.convs.append(TransformerConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(TransformerConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94016ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4f019c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GCN(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,  \n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        data.y = data.y.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1))  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            data.y = data.y.float()\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(out.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "\n",
    "    mse = mean_squared_error(y_true.numpy(), y_pred.numpy())\n",
    "    rmse = torch.sqrt(torch.tensor(mse))  \n",
    "    mae = mean_absolute_error(y_true.numpy(), y_pred.numpy())\n",
    "\n",
    "    return total_loss / len(loader), rmse.item(), mae  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25d58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 12.9826, Val Loss: 11.7250, Val RMSE: 3.4540, Val MAE: 2.8551\n",
      "Epoch: 02, Train Loss: 9.6557, Val Loss: 8.1820, Val RMSE: 2.8917, Val MAE: 2.3359\n",
      "Epoch: 03, Train Loss: 6.8217, Val Loss: 6.2527, Val RMSE: 2.5288, Val MAE: 2.0552\n",
      "Epoch: 04, Train Loss: 5.1686, Val Loss: 4.6560, Val RMSE: 2.1800, Val MAE: 1.7869\n",
      "Epoch: 05, Train Loss: 4.1268, Val Loss: 3.6647, Val RMSE: 1.9289, Val MAE: 1.5969\n",
      "Epoch: 06, Train Loss: 3.8822, Val Loss: 3.3258, Val RMSE: 1.8311, Val MAE: 1.5233\n",
      "Epoch: 07, Train Loss: 3.2430, Val Loss: 2.8203, Val RMSE: 1.6822, Val MAE: 1.4011\n",
      "Epoch: 08, Train Loss: 3.1145, Val Loss: 6.3690, Val RMSE: 2.5333, Val MAE: 2.0701\n",
      "Epoch: 09, Train Loss: 2.7074, Val Loss: 2.8670, Val RMSE: 1.6976, Val MAE: 1.4134\n",
      "Epoch: 10, Train Loss: 2.4434, Val Loss: 1.9120, Val RMSE: 1.3758, Val MAE: 1.0661\n",
      "Epoch: 11, Train Loss: 2.4106, Val Loss: 1.5712, Val RMSE: 1.2720, Val MAE: 1.0082\n",
      "Epoch: 12, Train Loss: 2.2700, Val Loss: 1.5378, Val RMSE: 1.2470, Val MAE: 0.9937\n",
      "Epoch: 13, Train Loss: 2.2539, Val Loss: 2.3190, Val RMSE: 1.5326, Val MAE: 1.2346\n",
      "Epoch: 14, Train Loss: 2.1559, Val Loss: 3.9339, Val RMSE: 2.0027, Val MAE: 1.6359\n",
      "Epoch: 15, Train Loss: 2.0566, Val Loss: 2.6109, Val RMSE: 1.6170, Val MAE: 1.2791\n",
      "Epoch: 16, Train Loss: 2.2362, Val Loss: 1.7135, Val RMSE: 1.3181, Val MAE: 1.0202\n",
      "Epoch: 17, Train Loss: 2.0278, Val Loss: 2.9884, Val RMSE: 1.7393, Val MAE: 1.3985\n",
      "Epoch: 18, Train Loss: 1.9250, Val Loss: 1.2265, Val RMSE: 1.1226, Val MAE: 0.8902\n",
      "Epoch: 19, Train Loss: 1.9488, Val Loss: 2.1968, Val RMSE: 1.5075, Val MAE: 1.2215\n",
      "Epoch: 20, Train Loss: 1.9186, Val Loss: 1.3894, Val RMSE: 1.1820, Val MAE: 0.8746\n",
      "Epoch: 21, Train Loss: 1.8601, Val Loss: 1.3494, Val RMSE: 1.1796, Val MAE: 0.9048\n",
      "Epoch: 22, Train Loss: 1.6771, Val Loss: 1.2194, Val RMSE: 1.1178, Val MAE: 0.8606\n",
      "Epoch: 23, Train Loss: 1.9806, Val Loss: 1.1762, Val RMSE: 1.0944, Val MAE: 0.8797\n",
      "Epoch: 24, Train Loss: 1.8224, Val Loss: 1.2956, Val RMSE: 1.1517, Val MAE: 0.8945\n",
      "Epoch: 25, Train Loss: 1.8434, Val Loss: 1.0875, Val RMSE: 1.0406, Val MAE: 0.8112\n",
      "Epoch: 26, Train Loss: 1.6852, Val Loss: 1.4203, Val RMSE: 1.2202, Val MAE: 0.9723\n",
      "Epoch: 27, Train Loss: 1.6363, Val Loss: 1.4915, Val RMSE: 1.2494, Val MAE: 0.9713\n",
      "Epoch: 28, Train Loss: 1.8092, Val Loss: 1.3195, Val RMSE: 1.1516, Val MAE: 0.8567\n",
      "Epoch: 29, Train Loss: 1.8354, Val Loss: 1.0920, Val RMSE: 1.0503, Val MAE: 0.7843\n",
      "Epoch: 30, Train Loss: 1.6141, Val Loss: 1.5106, Val RMSE: 1.2463, Val MAE: 0.9813\n",
      "Test Loss: 1.6615, Test RMSE: 1.2778, Test MAE: 1.0615\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 31):\n",
    "    train_loss = train()\n",
    "    val_loss, val_rmse, val_mae = evaluate(val_loader)\n",
    "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "test_loss, test_rmse, test_mae = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c83468c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GraphSAGE(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,  \n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        data.y = data.y.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1))  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            data.y = data.y.float()\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(out.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "\n",
    "    mse = mean_squared_error(y_true.numpy(), y_pred.numpy())\n",
    "    rmse = torch.sqrt(torch.tensor(mse))  \n",
    "    mae = mean_absolute_error(y_true.numpy(), y_pred.numpy())\n",
    "\n",
    "    return total_loss / len(loader), rmse.item(), mae  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8feac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 12.8582, Val Loss: 11.0460, Val RMSE: 3.3555, Val MAE: 2.7546\n",
      "Epoch: 02, Train Loss: 8.6751, Val Loss: 7.6539, Val RMSE: 2.7967, Val MAE: 2.2386\n",
      "Epoch: 03, Train Loss: 6.2893, Val Loss: 5.5190, Val RMSE: 2.3756, Val MAE: 1.9046\n",
      "Epoch: 04, Train Loss: 4.7920, Val Loss: 3.9615, Val RMSE: 2.0134, Val MAE: 1.6187\n",
      "Epoch: 05, Train Loss: 4.1408, Val Loss: 3.3604, Val RMSE: 1.8535, Val MAE: 1.4846\n",
      "Epoch: 06, Train Loss: 3.5440, Val Loss: 2.8584, Val RMSE: 1.7147, Val MAE: 1.3518\n",
      "Epoch: 07, Train Loss: 3.2259, Val Loss: 2.5786, Val RMSE: 1.6344, Val MAE: 1.2994\n",
      "Epoch: 08, Train Loss: 3.0202, Val Loss: 2.6459, Val RMSE: 1.6516, Val MAE: 1.3360\n",
      "Epoch: 09, Train Loss: 2.8097, Val Loss: 2.0016, Val RMSE: 1.4503, Val MAE: 1.1116\n",
      "Epoch: 10, Train Loss: 2.6614, Val Loss: 2.3789, Val RMSE: 1.5735, Val MAE: 1.2340\n",
      "Epoch: 11, Train Loss: 2.3906, Val Loss: 2.7865, Val RMSE: 1.7058, Val MAE: 1.3733\n",
      "Epoch: 12, Train Loss: 2.4222, Val Loss: 1.9366, Val RMSE: 1.4144, Val MAE: 1.0968\n",
      "Epoch: 13, Train Loss: 2.5114, Val Loss: 1.8219, Val RMSE: 1.3697, Val MAE: 1.0590\n",
      "Epoch: 14, Train Loss: 2.2579, Val Loss: 1.6236, Val RMSE: 1.2921, Val MAE: 0.9756\n",
      "Epoch: 15, Train Loss: 2.1097, Val Loss: 1.5670, Val RMSE: 1.2702, Val MAE: 0.9609\n",
      "Epoch: 16, Train Loss: 2.1076, Val Loss: 1.5131, Val RMSE: 1.2526, Val MAE: 0.9544\n",
      "Epoch: 17, Train Loss: 2.0592, Val Loss: 2.3183, Val RMSE: 1.5501, Val MAE: 1.2335\n",
      "Epoch: 18, Train Loss: 2.1884, Val Loss: 1.5810, Val RMSE: 1.2783, Val MAE: 0.9635\n",
      "Epoch: 19, Train Loss: 2.0375, Val Loss: 1.7845, Val RMSE: 1.3564, Val MAE: 1.0600\n",
      "Epoch: 20, Train Loss: 1.9793, Val Loss: 1.3563, Val RMSE: 1.1844, Val MAE: 0.9008\n",
      "Epoch: 21, Train Loss: 2.0089, Val Loss: 1.3053, Val RMSE: 1.1610, Val MAE: 0.8854\n",
      "Epoch: 22, Train Loss: 2.0326, Val Loss: 1.4938, Val RMSE: 1.2366, Val MAE: 0.9294\n",
      "Epoch: 23, Train Loss: 1.9526, Val Loss: 1.3185, Val RMSE: 1.1693, Val MAE: 0.8788\n",
      "Epoch: 24, Train Loss: 1.8808, Val Loss: 1.3861, Val RMSE: 1.1953, Val MAE: 0.9032\n",
      "Epoch: 25, Train Loss: 1.9421, Val Loss: 1.4809, Val RMSE: 1.2324, Val MAE: 0.9084\n",
      "Epoch: 26, Train Loss: 1.9170, Val Loss: 2.5534, Val RMSE: 1.6207, Val MAE: 1.2986\n",
      "Epoch: 27, Train Loss: 1.9747, Val Loss: 1.3211, Val RMSE: 1.1560, Val MAE: 0.8764\n",
      "Epoch: 28, Train Loss: 1.7797, Val Loss: 1.2630, Val RMSE: 1.1274, Val MAE: 0.8521\n",
      "Epoch: 29, Train Loss: 1.8260, Val Loss: 1.2115, Val RMSE: 1.1124, Val MAE: 0.8477\n",
      "Epoch: 30, Train Loss: 1.7895, Val Loss: 1.5346, Val RMSE: 1.2497, Val MAE: 0.9117\n",
      "Test Loss: 1.7556, Test RMSE: 1.3329, Test MAE: 1.0732\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 31):\n",
    "    train_loss = train()\n",
    "    val_loss, val_rmse, val_mae = evaluate(val_loader)\n",
    "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "test_loss, test_rmse, test_mae = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19202d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 11.9896, Val Loss: 8.2978, Val RMSE: 2.9092, Val MAE: 2.3301\n",
      "Epoch: 02, Train Loss: 7.3944, Val Loss: 7.7660, Val RMSE: 2.8261, Val MAE: 2.3144\n",
      "Epoch: 03, Train Loss: 4.7441, Val Loss: 11.6998, Val RMSE: 3.4459, Val MAE: 2.8806\n",
      "Epoch: 04, Train Loss: 3.0667, Val Loss: 2.4060, Val RMSE: 1.5956, Val MAE: 1.1932\n",
      "Epoch: 05, Train Loss: 2.1827, Val Loss: 1.8995, Val RMSE: 1.4234, Val MAE: 1.0664\n",
      "Epoch: 06, Train Loss: 2.0153, Val Loss: 1.9209, Val RMSE: 1.4058, Val MAE: 1.0537\n",
      "Epoch: 07, Train Loss: 1.8907, Val Loss: 1.3925, Val RMSE: 1.2154, Val MAE: 0.9413\n",
      "Epoch: 08, Train Loss: 1.9068, Val Loss: 1.4911, Val RMSE: 1.2554, Val MAE: 0.9370\n",
      "Epoch: 09, Train Loss: 1.7552, Val Loss: 3.1630, Val RMSE: 1.8150, Val MAE: 1.4639\n",
      "Epoch: 10, Train Loss: 1.7963, Val Loss: 9.8948, Val RMSE: 3.1743, Val MAE: 2.6408\n",
      "Epoch: 11, Train Loss: 1.6700, Val Loss: 6.2972, Val RMSE: 2.5719, Val MAE: 2.1460\n",
      "Epoch: 12, Train Loss: 1.7152, Val Loss: 3.4048, Val RMSE: 1.8551, Val MAE: 1.5067\n",
      "Epoch: 13, Train Loss: 1.5496, Val Loss: 1.3988, Val RMSE: 1.1916, Val MAE: 0.9419\n",
      "Epoch: 14, Train Loss: 1.5969, Val Loss: 1.2781, Val RMSE: 1.1501, Val MAE: 0.8625\n",
      "Epoch: 15, Train Loss: 1.4849, Val Loss: 1.8850, Val RMSE: 1.3953, Val MAE: 1.1034\n",
      "Epoch: 16, Train Loss: 1.4862, Val Loss: 1.2263, Val RMSE: 1.1404, Val MAE: 0.8775\n",
      "Epoch: 17, Train Loss: 1.4169, Val Loss: 1.1248, Val RMSE: 1.0822, Val MAE: 0.8268\n",
      "Epoch: 18, Train Loss: 1.5090, Val Loss: 1.4193, Val RMSE: 1.2105, Val MAE: 0.9204\n",
      "Epoch: 19, Train Loss: 1.4640, Val Loss: 1.1177, Val RMSE: 1.0758, Val MAE: 0.8581\n",
      "Epoch: 20, Train Loss: 1.3700, Val Loss: 6.0628, Val RMSE: 2.4991, Val MAE: 2.0631\n",
      "Epoch: 21, Train Loss: 1.3286, Val Loss: 3.1744, Val RMSE: 1.8027, Val MAE: 1.4488\n",
      "Epoch: 22, Train Loss: 1.3498, Val Loss: 1.2736, Val RMSE: 1.1432, Val MAE: 0.8574\n",
      "Epoch: 23, Train Loss: 1.4994, Val Loss: 2.1814, Val RMSE: 1.4986, Val MAE: 1.1509\n",
      "Epoch: 24, Train Loss: 1.4735, Val Loss: 1.7185, Val RMSE: 1.3341, Val MAE: 1.0388\n",
      "Epoch: 25, Train Loss: 1.3778, Val Loss: 1.3581, Val RMSE: 1.1816, Val MAE: 0.9089\n",
      "Epoch: 26, Train Loss: 1.3211, Val Loss: 1.2469, Val RMSE: 1.1559, Val MAE: 0.8712\n",
      "Epoch: 27, Train Loss: 1.3138, Val Loss: 1.9159, Val RMSE: 1.4080, Val MAE: 1.1068\n",
      "Epoch: 28, Train Loss: 1.2807, Val Loss: 1.5078, Val RMSE: 1.2321, Val MAE: 0.9645\n",
      "Epoch: 29, Train Loss: 1.3720, Val Loss: 1.6114, Val RMSE: 1.2908, Val MAE: 0.9826\n",
      "Epoch: 30, Train Loss: 1.3647, Val Loss: 1.0677, Val RMSE: 1.0373, Val MAE: 0.7436\n",
      "Test Loss: 0.9347, Test RMSE: 0.9575, Test MAE: 0.7449\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GIN(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,  \n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        data.y = data.y.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1))  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            data.y = data.y.float()\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(out.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "\n",
    "    mse = mean_squared_error(y_true.numpy(), y_pred.numpy())\n",
    "    rmse = torch.sqrt(torch.tensor(mse))  \n",
    "    mae = mean_absolute_error(y_true.numpy(), y_pred.numpy())\n",
    "\n",
    "    return total_loss / len(loader), rmse.item(), mae  \n",
    "\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    train_loss = train()\n",
    "    val_loss, val_rmse, val_mae = evaluate(val_loader)\n",
    "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "test_loss, test_rmse, test_mae = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db79f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 13.0838, Val Loss: 11.1945, Val RMSE: 3.3744, Val MAE: 2.7792\n",
      "Epoch: 02, Train Loss: 9.3297, Val Loss: 8.1729, Val RMSE: 2.8852, Val MAE: 2.3547\n",
      "Epoch: 03, Train Loss: 6.9287, Val Loss: 5.7426, Val RMSE: 2.4191, Val MAE: 1.9669\n",
      "Epoch: 04, Train Loss: 5.2070, Val Loss: 4.5182, Val RMSE: 2.1396, Val MAE: 1.7525\n",
      "Epoch: 05, Train Loss: 4.6013, Val Loss: 3.8227, Val RMSE: 1.9613, Val MAE: 1.6191\n",
      "Epoch: 06, Train Loss: 4.2263, Val Loss: 3.3533, Val RMSE: 1.8346, Val MAE: 1.5235\n",
      "Epoch: 07, Train Loss: 3.9601, Val Loss: 3.1117, Val RMSE: 1.7663, Val MAE: 1.4714\n",
      "Epoch: 08, Train Loss: 3.7599, Val Loss: 2.8941, Val RMSE: 1.6941, Val MAE: 1.4101\n",
      "Epoch: 09, Train Loss: 3.3453, Val Loss: 2.6734, Val RMSE: 1.6351, Val MAE: 1.3650\n",
      "Epoch: 10, Train Loss: 3.2113, Val Loss: 2.4482, Val RMSE: 1.5691, Val MAE: 1.2844\n",
      "Epoch: 11, Train Loss: 3.0806, Val Loss: 2.6269, Val RMSE: 1.6259, Val MAE: 1.3311\n",
      "Epoch: 12, Train Loss: 3.0031, Val Loss: 2.1242, Val RMSE: 1.4627, Val MAE: 1.1810\n",
      "Epoch: 13, Train Loss: 2.6858, Val Loss: 2.2335, Val RMSE: 1.5017, Val MAE: 1.2075\n",
      "Epoch: 14, Train Loss: 2.5733, Val Loss: 2.3179, Val RMSE: 1.5331, Val MAE: 1.2178\n",
      "Epoch: 15, Train Loss: 2.4460, Val Loss: 2.1523, Val RMSE: 1.4864, Val MAE: 1.1707\n",
      "Epoch: 16, Train Loss: 2.4034, Val Loss: 1.7271, Val RMSE: 1.3334, Val MAE: 1.0509\n",
      "Epoch: 17, Train Loss: 2.9091, Val Loss: 2.1027, Val RMSE: 1.4726, Val MAE: 1.1412\n",
      "Epoch: 18, Train Loss: 2.3957, Val Loss: 1.6210, Val RMSE: 1.2826, Val MAE: 1.0035\n",
      "Epoch: 19, Train Loss: 2.2977, Val Loss: 1.7322, Val RMSE: 1.3219, Val MAE: 1.0119\n",
      "Epoch: 20, Train Loss: 1.9903, Val Loss: 1.4720, Val RMSE: 1.2262, Val MAE: 0.9234\n",
      "Epoch: 21, Train Loss: 2.2544, Val Loss: 1.3981, Val RMSE: 1.1979, Val MAE: 0.9186\n",
      "Epoch: 22, Train Loss: 2.0574, Val Loss: 1.3793, Val RMSE: 1.1871, Val MAE: 0.8926\n",
      "Epoch: 23, Train Loss: 2.0245, Val Loss: 1.6397, Val RMSE: 1.2974, Val MAE: 0.9467\n",
      "Epoch: 24, Train Loss: 2.0582, Val Loss: 1.4593, Val RMSE: 1.2253, Val MAE: 0.9600\n",
      "Epoch: 25, Train Loss: 1.9934, Val Loss: 1.3419, Val RMSE: 1.1697, Val MAE: 0.8780\n",
      "Epoch: 26, Train Loss: 2.0464, Val Loss: 1.5356, Val RMSE: 1.2544, Val MAE: 0.9284\n",
      "Epoch: 27, Train Loss: 1.8935, Val Loss: 1.3114, Val RMSE: 1.1632, Val MAE: 0.8745\n",
      "Epoch: 28, Train Loss: 1.8929, Val Loss: 1.3800, Val RMSE: 1.1895, Val MAE: 0.8895\n",
      "Epoch: 29, Train Loss: 1.8144, Val Loss: 1.7482, Val RMSE: 1.3571, Val MAE: 1.0738\n",
      "Epoch: 30, Train Loss: 1.7467, Val Loss: 1.3012, Val RMSE: 1.1628, Val MAE: 0.8514\n",
      "Test Loss: 1.2634, Test RMSE: 1.1248, Test MAE: 0.8694\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GAT(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,  \n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        data.y = data.y.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1))  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            data.y = data.y.float()\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(out.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "\n",
    "    mse = mean_squared_error(y_true.numpy(), y_pred.numpy())\n",
    "    rmse = torch.sqrt(torch.tensor(mse))  \n",
    "    mae = mean_absolute_error(y_true.numpy(), y_pred.numpy())\n",
    "\n",
    "    return total_loss / len(loader), rmse.item(), mae  \n",
    "\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    train_loss = train()\n",
    "    val_loss, val_rmse, val_mae = evaluate(val_loader)\n",
    "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "test_loss, test_rmse, test_mae = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e930b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 10.9462, Val Loss: 10.6572, Val RMSE: 3.2939, Val MAE: 2.7016\n",
      "Epoch: 02, Train Loss: 7.1088, Val Loss: 6.2751, Val RMSE: 2.5252, Val MAE: 2.0326\n",
      "Epoch: 03, Train Loss: 5.0210, Val Loss: 4.0133, Val RMSE: 2.0204, Val MAE: 1.6379\n",
      "Epoch: 04, Train Loss: 3.4643, Val Loss: 3.2341, Val RMSE: 1.8198, Val MAE: 1.4383\n",
      "Epoch: 05, Train Loss: 2.9508, Val Loss: 2.7045, Val RMSE: 1.6633, Val MAE: 1.3030\n",
      "Epoch: 06, Train Loss: 2.6867, Val Loss: 2.6436, Val RMSE: 1.6423, Val MAE: 1.3032\n",
      "Epoch: 07, Train Loss: 2.5860, Val Loss: 2.7761, Val RMSE: 1.6863, Val MAE: 1.3469\n",
      "Epoch: 08, Train Loss: 2.3697, Val Loss: 2.2703, Val RMSE: 1.5194, Val MAE: 1.1966\n",
      "Epoch: 09, Train Loss: 2.4803, Val Loss: 2.0497, Val RMSE: 1.4497, Val MAE: 1.1459\n",
      "Epoch: 10, Train Loss: 2.1482, Val Loss: 1.8838, Val RMSE: 1.3932, Val MAE: 1.0740\n",
      "Epoch: 11, Train Loss: 2.1176, Val Loss: 2.2881, Val RMSE: 1.5396, Val MAE: 1.1928\n",
      "Epoch: 12, Train Loss: 2.2578, Val Loss: 2.0981, Val RMSE: 1.4709, Val MAE: 1.1400\n",
      "Epoch: 13, Train Loss: 1.9660, Val Loss: 1.6738, Val RMSE: 1.3164, Val MAE: 1.0122\n",
      "Epoch: 14, Train Loss: 1.8846, Val Loss: 2.5247, Val RMSE: 1.6201, Val MAE: 1.2528\n",
      "Epoch: 15, Train Loss: 1.8624, Val Loss: 1.4720, Val RMSE: 1.2270, Val MAE: 0.9461\n",
      "Epoch: 16, Train Loss: 2.0571, Val Loss: 1.4692, Val RMSE: 1.2270, Val MAE: 0.9481\n",
      "Epoch: 17, Train Loss: 1.8570, Val Loss: 1.4980, Val RMSE: 1.2479, Val MAE: 0.9241\n",
      "Epoch: 18, Train Loss: 1.9515, Val Loss: 1.4877, Val RMSE: 1.2307, Val MAE: 0.9726\n",
      "Epoch: 19, Train Loss: 1.6943, Val Loss: 1.5057, Val RMSE: 1.2473, Val MAE: 0.9382\n",
      "Epoch: 20, Train Loss: 1.7217, Val Loss: 2.3817, Val RMSE: 1.5713, Val MAE: 1.1826\n",
      "Epoch: 21, Train Loss: 1.8999, Val Loss: 1.4222, Val RMSE: 1.2195, Val MAE: 0.9156\n",
      "Epoch: 22, Train Loss: 1.7703, Val Loss: 1.4535, Val RMSE: 1.2227, Val MAE: 0.9161\n",
      "Epoch: 23, Train Loss: 1.6866, Val Loss: 1.3981, Val RMSE: 1.2021, Val MAE: 0.9176\n",
      "Epoch: 24, Train Loss: 1.6008, Val Loss: 1.4656, Val RMSE: 1.2301, Val MAE: 0.9112\n",
      "Epoch: 25, Train Loss: 1.5982, Val Loss: 1.3837, Val RMSE: 1.1936, Val MAE: 0.8767\n",
      "Epoch: 26, Train Loss: 1.6865, Val Loss: 1.6242, Val RMSE: 1.2892, Val MAE: 0.9408\n",
      "Epoch: 27, Train Loss: 1.4798, Val Loss: 1.2374, Val RMSE: 1.1276, Val MAE: 0.8304\n",
      "Epoch: 28, Train Loss: 1.6481, Val Loss: 1.2736, Val RMSE: 1.1559, Val MAE: 0.8624\n",
      "Epoch: 29, Train Loss: 1.5879, Val Loss: 1.6963, Val RMSE: 1.3155, Val MAE: 0.9595\n",
      "Epoch: 30, Train Loss: 1.5425, Val Loss: 1.2331, Val RMSE: 1.1237, Val MAE: 0.8025\n",
      "Test Loss: 1.0120, Test RMSE: 1.0034, Test MAE: 0.7869\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GT(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,  \n",
    "    num_layers=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        data.y = data.y.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.view(-1, 1))  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()\n",
    "            data.y = data.y.float()\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.append(data.y.cpu())\n",
    "            y_pred.append(out.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "\n",
    "    mse = mean_squared_error(y_true.numpy(), y_pred.numpy())\n",
    "    rmse = torch.sqrt(torch.tensor(mse))  \n",
    "    mae = mean_absolute_error(y_true.numpy(), y_pred.numpy())\n",
    "\n",
    "    return total_loss / len(loader), rmse.item(), mae  \n",
    "\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    train_loss = train()\n",
    "    val_loss, val_rmse, val_mae = evaluate(val_loader)\n",
    "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "test_loss, test_rmse, test_mae = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
