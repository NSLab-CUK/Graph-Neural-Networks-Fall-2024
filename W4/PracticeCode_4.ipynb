{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e75cd95-357a-4830-8cd9-43d6c251516e",
   "metadata": {},
   "source": [
    "# GraphSage with Cora dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66343a2a-1d6c-45a1-b35a-2871df8cee69",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1d0d45-609e-4036-8cb2-de74716fe23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch_geometric.datasets import Planetoid, Reddit\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be1802-1156-42f1-be42-ea8596517c90",
   "metadata": {},
   "source": [
    "## Store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc99f521-a195-4e5a-9ac3-08923dc366e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntucat/anaconda3/envs/W4_test/lib/python3.9/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('data', 'cora')\n",
    "#dataset = Reddit(path)\n",
    "dataset = Planetoid(path, name = 'cora')\n",
    "\n",
    "# Already send node features/labels to GPU for faster access during sampling\n",
    "data = dataset[0].to(device, 'x', 'y')\n",
    "\n",
    "# Define parameters for the NeighborLoader\n",
    "kwargs = {'batch_size': 1024, 'num_workers': 6, 'persistent_workers': True}\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask,\n",
    "                              num_neighbors=[25, 10], shuffle=True, **kwargs)\n",
    "\n",
    "subgraph_loader = NeighborLoader(copy.copy(data), input_nodes=None,\n",
    "                                 num_neighbors=[-1], shuffle=False, **kwargs)\n",
    "\n",
    "# No need to maintain these features during evaluation:\n",
    "del subgraph_loader.data.x, subgraph_loader.data.y\n",
    "# Add global node index information.\n",
    "subgraph_loader.data.num_nodes = data.num_nodes\n",
    "subgraph_loader.data.n_id = torch.arange(data.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f92e9-6aaf-467c-93bc-9e98bf7db7be",
   "metadata": {},
   "source": [
    "## Graph statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "661c236c-36fe-416a-894b-79f33bf631f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 2708\n",
      "Number of edges in the graph: 10556\n",
      "Node feature matrix with shape: torch.Size([2708, 1433])\n",
      "Graph connectivity in COO format with shape: torch.Size([2, 10556])\n",
      "Target to train against : torch.Size([2708])\n",
      "Node feature length 1433\n"
     ]
    }
   ],
   "source": [
    "# Check some graph statistics of Cora graph\n",
    "print(\"Number of nodes in the graph:\", data.num_nodes)\n",
    "print(\"Number of edges in the graph:\", data.num_edges)\n",
    "print(\"Node feature matrix with shape:\", data.x.shape) # [num_nodes, num_node_features]\n",
    "print(\"Graph connectivity in COO format with shape:\", data.edge_index.shape) # [2, num_edges]\n",
    "print(\"Target to train against :\", data.y.shape) \n",
    "print(\"Node feature length\", dataset.num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754fb6c-dca2-4105-8edc-0702a21621ef",
   "metadata": {},
   "source": [
    "## Check the number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3284bd2d-212c-4fdf-8812-6f56fd8dbb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a48c716e-15a3-4548-987f-7281d67d3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "    #Compute representations of nodes layer by layer\n",
    "    #This leads to faster computation in contrast to immediately computing the final representations of each batch\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, subgraph_loader):\n",
    "        # Initialize a progress bar\n",
    "        pbar = tqdm(total=len(subgraph_loader.dataset) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Forward pass through each SAGEConv layer\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                # Get the node features for the current batch\n",
    "                x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
    "                # Apply the current SAGEConv layer\n",
    "                x = conv(x, batch.edge_index.to(device))\n",
    "                if i < len(self.convs) - 1:\n",
    "                    x = x.relu_()\n",
    "                xs.append(x[:batch.batch_size].cpu())\n",
    "                pbar.update(batch.batch_size)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        pbar.close()\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6708dc-9765-4603-8a56-774fb6489cbb",
   "metadata": {},
   "source": [
    "## Construct SAGE model with (in_channels, hidden_channels, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6c03d33-aa0c-42ef-91fb-1bb1f3484653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(dataset.num_features, 256, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "508d3fc1-178b-4136-a3bc-0dab784f452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=int(len(train_loader.dataset)))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = total_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        y_hat = model(batch.x, batch.edge_index.to(device))[:batch.batch_size]\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * batch.batch_size\n",
    "        total_correct += int((y_hat.argmax(dim=-1) == y).sum())\n",
    "        total_examples += batch.batch_size\n",
    "        pbar.update(batch.batch_size)\n",
    "    pbar.close()\n",
    "\n",
    "    return total_loss / total_examples, total_correct / total_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b128352a-b38e-4b59-88c9-ada900a9d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    y_hat = model.inference(data.x, subgraph_loader).argmax(dim=-1)\n",
    "    y = data.y.to(y_hat.device)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((y_hat[mask] == y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb9db82e-3a9c-41aa-8577-3de4bfe65017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 140/140 [00:00<00:00, 1745.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 1.9509, Approx. Train: 0.1929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 54798.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train: 1.0000, Val: 0.7440, Test: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 140/140 [00:00<00:00, 6126.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02, Loss: 1.3975, Approx. Train: 0.9929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 176343.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train: 0.9929, Val: 0.7680, Test: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 140/140 [00:00<00:00, 8311.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, Loss: 0.6457, Approx. Train: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 158475.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train: 1.0000, Val: 0.7640, Test: 0.7790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 140/140 [00:00<00:00, 9729.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04, Loss: 0.1990, Approx. Train: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 181090.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train: 1.0000, Val: 0.7660, Test: 0.7900\n",
      "Median time per epoch: 0.0537s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for epoch in range(1, 5):\n",
    "    start = time.time()\n",
    "    loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, '\n",
    "          f'Test: {test_acc:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba88a47-658e-4240-ad91-96922364c2f4",
   "metadata": {},
   "source": [
    "## Save the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57792588-7916-40cc-a994-f40b3260fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'Graphsage_model.pt'\n",
    "torch.save(model, './model.pt')\n",
    "torch.save(model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a35bad-bc8a-4067-b13c-092a15a1d0fa",
   "metadata": {},
   "source": [
    "# GraphSAINT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70297f3c-6bc1-4e0f-8650-89a9787ca72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Flickr\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.typing import WITH_TORCH_SPARSE\n",
    "from torch_geometric.utils import degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457e91cd-170a-41bb-856c-1d784e210a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid, Reddit, Flickr\n",
    "if not WITH_TORCH_SPARSE:\n",
    "    quit(\"This example requires 'torch-sparse'\")\n",
    "    \n",
    "path = osp.join('data', 'cora')\n",
    "dataset = Planetoid(path, name=  'cora')\n",
    "data = dataset[0]\n",
    "row, col = data.edge_index\n",
    "data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c4d063-cb0c-4d7c-b09c-f1a5b9469bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 2708\n",
      "Number of edges in the graph: 10556\n",
      "Node feature matrix with shape: torch.Size([2708, 1433])\n",
      "Graph connectivity in COO format with shape: torch.Size([2, 10556])\n",
      "Target to train against : torch.Size([2708])\n",
      "Node feature length 1433\n"
     ]
    }
   ],
   "source": [
    "# Check some graph statistics of Cora graph\n",
    "print(\"Number of nodes in the graph:\", data.num_nodes)\n",
    "print(\"Number of edges in the graph:\", data.num_edges)\n",
    "print(\"Node feature matrix with shape:\", data.x.shape) # [num_nodes, num_node_features]\n",
    "print(\"Graph connectivity in COO format with shape:\", data.edge_index.shape) # [2, num_edges]\n",
    "print(\"Target to train against :\", data.y.shape) \n",
    "print(\"Node feature length\", dataset.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ad5c30-d295-42cf-acc7-6c9f3b9bca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization:   0%|          | 0/27080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization: : 27094it [00:00, 527343.77it/s]         \n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Define the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_normalization', required=False, action='store_true')\n",
    "parser.add_argument(\"-f\", required=False)  # This is to handle the Jupyter Notebook's additional argument\n",
    "\n",
    "# Parse known arguments to avoid errors in Jupyter Notebook\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Define the model\n",
    "loader = GraphSAINTRandomWalkSampler(data, batch_size=16, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=dataset.processed_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8733f3ed-0f34-4457-aca3-35ab83795b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # Get the number of input features and output classes from the dataset\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        # Define a linear layer to combine the outputs of the three convolution layers\n",
    "        self.lin = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "\n",
    "    def set_aggr(self, aggr):\n",
    "        # Set the aggregation method for each convolution layer\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "        self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        # Apply the first GraphConv layer\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "        x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        # Concatenate the outputs of the three convolution layers\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        # Apply log softmax to the output\n",
    "        return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39e6b86-0f58-42a0-bdb2-252924949fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(hidden_channels=256).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea26d82-51ec-4ea4-921f-fdea02c7c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Set the aggregation method based on the use_normalization flag\n",
    "    model.set_aggr('add' if args.use_normalization else 'mean')\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    # Iterate over the data loader\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if args.use_normalization:\n",
    "            # Compute edge weights if normalization is used\n",
    "            edge_weight = data.edge_norm * data.edge_weight\n",
    "            # Forward pass through the model with edge weights\n",
    "            out = model(data.x, data.edge_index, edge_weight)\n",
    "            # Compute the loss with no reduction\n",
    "            loss = F.nll_loss(out, data.y, reduction='none')\n",
    "            # Apply node normalization and sum the loss for the training mask\n",
    "            loss = (loss * data.node_norm)[data.train_mask].sum()\n",
    "        else:\n",
    "            # Forward pass through the model without edge weights\n",
    "            out = model(data.x, data.edge_index)\n",
    "            # Compute the loss for the training mask\n",
    "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        # Backward pass to compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate the total loss and the number of examples\n",
    "        total_loss += loss.item() * data.num_nodes\n",
    "        total_examples += data.num_nodes\n",
    "        \n",
    "    return total_loss / total_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d2891e-aa0d-47cb-a6bf-e61fe0b34ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Set the aggregation method to 'mean' for evaluation\n",
    "    model.set_aggr('mean')\n",
    "    # Perform a forward pass through the model\n",
    "    out = model(data.x.to(device), data.edge_index.to(device))\n",
    "    # Get the predicted class by taking the argmax of the output\n",
    "    pred = out.argmax(dim=-1)\n",
    "    # Check which predictions are correct\n",
    "    correct = pred.eq(data.y.to(device))\n",
    "\n",
    "    accs = []\n",
    "    # Calculate accuracy for train, validation, and test masks\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        accs.append(correct[mask].sum().item() / mask.sum().item())\n",
    "        \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ba7b91-51e5-4de2-b1f5-be433d7951ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.9535, Train: 0.1643, Val: 0.0880, Test: 0.1010\n",
      "Epoch: 02, Loss: 1.8895, Train: 0.1500, Val: 0.0720, Test: 0.0910\n",
      "Epoch: 03, Loss: 1.9626, Train: 0.1571, Val: 0.0800, Test: 0.0940\n",
      "Epoch: 04, Loss: 1.8472, Train: 0.2786, Val: 0.1380, Test: 0.1460\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, '\n",
    "          f'Val: {accs[1]:.4f}, Test: {accs[2]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca30209-862f-417c-9ae7-e7cda9689e2a",
   "metadata": {},
   "source": [
    "# Cluster_GCN with Reddit graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfff25c-a6e7-4610-8ee3-8a66bc1bd326",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0004643c-7ba2-4678-9903-4d0260281831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import Planetoid, Reddit\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da4d53-64b1-45a4-90bb-9de7e70ac6e1",
   "metadata": {},
   "source": [
    "## Import graph datasets: cora/citeseer/pubmed/ Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364d0d4a-94e1-434a-9edc-3901cd16d7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "/home/ubuntucat/anaconda3/envs/W4_test/lib/python3.9/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('data', 'cora')\n",
    "dataset = Planetoid(path,name = 'cora')\n",
    "data = dataset[0]\n",
    "\n",
    "cluster_data = ClusterData(data, num_parts=10, recursive=False, save_dir=dataset.processed_dir)\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=20, shuffle=True,\n",
    "                             num_workers=12)\n",
    "\n",
    "subgraph_loader = NeighborLoader(data, num_neighbors=[-1], batch_size=1024,\n",
    "                                 shuffle=False, num_workers=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02afceac-ff51-49e3-8cf5-791b379adb69",
   "metadata": {},
   "source": [
    "## Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b782da-c340-445a-8680-add78c9543c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Initialize a list of SAGEConv layers\n",
    "        self.convs = ModuleList(\n",
    "            [SAGEConv(in_channels, 128), # First SAGEConv layer\n",
    "             SAGEConv(128, out_channels)]) # Second SAGEConv layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Forward pass through each SAGEConv layer\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        # Initialize a progress bar\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                edge_index = batch.edge_index.to(device)\n",
    "                x = x_all[batch.n_id].to(device)\n",
    "                # Get the target node features for the current batch\n",
    "                x_target = x[:batch.batch_size]\n",
    "                # Apply the current SAGEConv layer\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    # Apply ReLU activation after each layer except the last one\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch.batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f8741-19c6-4d6b-ae14-f4000474a375",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e624fe9-9e87-406a-ba5e-3a3fd9eb1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ef14f-e883-41bf-8696-74bf4831781c",
   "metadata": {},
   "source": [
    "## Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf802c0-75c7-41f1-9cc1-444125883c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "\n",
    "    return total_loss / total_nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654091e-d49a-42ca-9a39-6294f9e70253",
   "metadata": {},
   "source": [
    "## Define the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc16b35-6cbd-4cfe-84e8-68b8247086e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test():  # Inference should be performed on the full graph.\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(data.x)\n",
    "    y_pred = out.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ffab4-8339-4d56-9717-c9e0be0402aa",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7573960a-bcbe-4326-b66c-326467d39bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.9491\n",
      "Epoch: 02, Loss: 1.7128\n",
      "Epoch: 03, Loss: 1.4123\n",
      "Epoch: 04, Loss: 1.0836\n",
      "Median time per epoch: 0.1457s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for epoch in range(1, 5):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "              f'Val: {val_acc:.4f}, test: {test_acc:.4f}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
